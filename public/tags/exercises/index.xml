<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>exercises | CSU MSMB Group Study</title>
    <link>/tags/exercises/</link>
      <atom:link href="/tags/exercises/index.xml" rel="self" type="application/rss+xml" />
    <description>exercises</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 30 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>exercises</title>
      <link>/tags/exercises/</link>
    </image>
    
    <item>
      <title>Exercise solution for Chapter 8, Part 1</title>
      <link>/post/ex-8-1/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/ex-8-1/</guid>
      <description>


&lt;div id=&#34;exercise-8.1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise 8.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Do the analyses of Section 8.5 with the &lt;code&gt;edgeR&lt;/code&gt; package and compare the results: make a scatterplot of the
log 10 p-values, pick some genes where there are large differences,
and visualize the raw data to see what is going on. Based on this can you explain the differences?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most of the following code is taken straight from the book in section 8.5 for data cleaning/wrangling and the &lt;code&gt;DESeq2&lt;/code&gt; analysis. The steps performed in &lt;code&gt;edgeR&lt;/code&gt; are quite similar but we do see some differences that we will get to towards the end.&lt;/p&gt;
&lt;p&gt;First, we load our libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pasilla)
library(edgeR)
library(dplyr)
library(DESeq2)
library(ggplot2)
library(pheatmap)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the same data used in the &lt;code&gt;DESeq2&lt;/code&gt; examples from the section 8.5.&lt;/p&gt;
&lt;p&gt;Load the example data using the &lt;code&gt;system.file()&lt;/code&gt; call for R data stored as part of a R package.
We then convert our data to a matrix object called &lt;code&gt;counts&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fn = system.file(&amp;quot;extdata&amp;quot;, &amp;quot;pasilla_gene_counts.tsv&amp;quot;,
                  package = &amp;quot;pasilla&amp;quot;, mustWork = TRUE)
counts = as.matrix(read.csv(fn, sep = &amp;quot;\t&amp;quot;, row.names = &amp;quot;gene_id&amp;quot;))

head(counts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
## FBgn0000018        583        761        245        310      722      299
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## FBgn0000018      308&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The matrix tallies the number of reads seen for each gene in each sample. We call it the count table. It has 14599 rows, corresponding to the genes, and 7 columns, corresponding to the samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(counts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14599     7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;edger&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;edgeR&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now begins the analysis with the &lt;code&gt;edgeR&lt;/code&gt; package. To do this, we follow the vignette for the package that is a downloadable .pdf file that you can get in your Rstudio session &lt;code&gt;vignette(&#34;edgeR&#34;)&lt;/code&gt; or online with &lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&#34; class=&#34;uri&#34;&gt;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we make the &lt;code&gt;group&lt;/code&gt; object which is a vector with values representing the treatment status of each of the 7 samples, where 1 refers to the untreated group, and 2 refers to the treated group. With this &lt;code&gt;group&lt;/code&gt; object we can make a &lt;code&gt;DGEList()&lt;/code&gt;, from the &lt;code&gt;edgeR&lt;/code&gt; package, with our count data and the groups we just made. A &lt;code&gt;DGEList()&lt;/code&gt; object is very similar to any traditional R list object and can be manipulated like any other list in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group &amp;lt;- factor(c(1,1,1,1,2,2,2))
x &amp;lt;- DGEList(counts=counts, group=group)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RNA-seq provides a measure of the relative abundance of each gene in each RNA
sample, but does not provide any measure of the total RNA output on a per-cell basis. In other words, RNA-seq measures relative expression rather than absolute expression. This can become an issue in RNA-seq when a small number of highly expressed genes consume a substantial proportion of the total library for a sample causing under sampling of the other expressed genes.&lt;br /&gt;
To help combat this we turn to normalization. &lt;code&gt;calcNormFactors&lt;/code&gt; normalizes by finding a set of scaling factors for the library sizes that minimizes the log-fold changes between the samples for most genes. Using a trimmed mean of M-values (TMM) between each pair of samples. If we receive a &lt;code&gt;norm.factors&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\lt\)&lt;/span&gt; 1 that means a small number of high count genes are monopolizing the sequencing reducing the counts of other genes. Conversely, a &lt;code&gt;norm.factors&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gt\)&lt;/span&gt; 1 scales up the library size, analogous to downscaling the counts. This normalization can help account for things like varying sequencing depth, length of genes, and RNA composition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normalization &amp;lt;- calcNormFactors(x)
normalization&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## An object of class &amp;quot;DGEList&amp;quot;
## $counts
##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## 14594 more rows ...
## 
## $samples
##            group lib.size norm.factors
## untreated1     1 13972512    0.9995731
## untreated2     1 21911438    1.0081519
## untreated3     1  8358426    0.9843974
## untreated4     1  9841335    0.9525077
## treated1       2 18670279    1.0651817
## treated2       2  9571826    0.9957012
## treated3       2 10343856    0.9978557&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;model.matrix()&lt;/code&gt; function creates a design matrix which is a matrix of values of &lt;strong&gt;explanatory variables&lt;/strong&gt; of a set of objects. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The purpose of this conversion is to prepare the data in a manner that facilitates regression-like modelling (ex. &lt;code&gt;glm&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;design &amp;lt;- model.matrix(~group)
head(design)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      0
## 4           1      0
## 5           1      1
## 6           1      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;estimateDisp()&lt;/code&gt; function, “Maximizes the negative binomial likelihood to give the estimate of the common, trended and tagwise dispersions across all tags.” We have to use this negative binomial (aka gamma-Poisson) model since our experiments vary from replicate to replicate more than the traditional Poisson can account for. This variance can be due to seemingly miniscule experimental conditions such as, temperature of cell culture, pipettor calibration, etc. In the case of the gamma-Poisson we have two inputs for variance and mean instead of just having &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for both variance and mean in the normal Poisson.
An important consideration that the &lt;code&gt;edgeR&lt;/code&gt; package has taken into account is the fact that RNA-seq and other Next Generation Sequencing projects are extremely expensive and generally have few samples. Accounting for dispersion with a small number of samples can be challenging and the &lt;code&gt;edgeR&lt;/code&gt; package tackles this conundrum using a qCML method. The qCML method calculates the likelihood by conditioning on the total counts for each tag, and uses pseudo counts after adjusting for library sizes. Given a table of counts or a &lt;code&gt;DGEList&lt;/code&gt; object, the qCML common dispersion and tagwise dispersions can be estimated using the &lt;code&gt;estimateDisp()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;design_matrix &amp;lt;- estimateDisp(normalization, design)
design_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## An object of class &amp;quot;DGEList&amp;quot;
## $counts
##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## 14594 more rows ...
## 
## $samples
##            group lib.size norm.factors
## untreated1     1 13972512    0.9995731
## untreated2     1 21911438    1.0081519
## untreated3     1  8358426    0.9843974
## untreated4     1  9841335    0.9525077
## treated1       2 18670279    1.0651817
## treated2       2  9571826    0.9957012
## treated3       2 10343856    0.9978557
## 
## $design
##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      0
## 4           1      0
## 5           1      1
## 6           1      1
## 7           1      1
## attr(,&amp;quot;assign&amp;quot;)
## [1] 0 1
## attr(,&amp;quot;contrasts&amp;quot;)
## attr(,&amp;quot;contrasts&amp;quot;)$group
## [1] &amp;quot;contr.treatment&amp;quot;
## 
## 
## $common.dispersion
## [1] 0.0228685
## 
## $trended.dispersion
## [1] 0.12060195 0.04196786 0.11986264 0.12040360 0.01632837
## 14594 more elements ...
## 
## $tagwise.dispersion
## [1] 0.12060195 0.02742256 0.70902130 0.09430132 0.01321566
## 14594 more elements ...
## 
## $AveLogCPM
## [1] -2.636763  2.953356 -1.966526 -2.223010  8.454625
## 14594 more elements ...
## 
## $trend.method
## [1] &amp;quot;locfit&amp;quot;
## 
## $prior.df
## [1] 5.886884
## 
## $prior.n
## [1] 1.177377
## 
## $span
## [1] 0.3013916&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our &lt;code&gt;design_matrix&lt;/code&gt; we can begin fitting modified linear models to it. Here we use a quasi-likelihood negative binomial generalized log-linear model, which is a mouth full. “Quasi-likelihood estimation is one way of allowing for overdispersion, that is, greater variability in the data than would be expected from the statistical model used.” Since we have already stated that we will have variation in our experiments, possibly due to the most minute factors, this issue of overdispersion is apparent. Instead of using traditional probability functions, a variance function is used (variance as a function of the mean) and allows for an overdispersion parameter input which is used to “fix” the variance function to resemble that of an existing probability function (ex. Poisson).&lt;/p&gt;
&lt;p&gt;The goal of this fit is to identify genes where the intensity level (gene expression level) is notably different between our treated and untreated groups.
Running the &lt;code&gt;glmQLF...()&lt;/code&gt; functions gives the null model against which the full model is compared. Tags can then be ranked in order of evidence for differential expression, based on the p-value computed for each tag.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glmQLFit(design_matrix, design)
qlf &amp;lt;- glmQLFTest(fit, coef=2)
edgeRoutput &amp;lt;- topTags(qlf)

edgeRoutput&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coefficient:  group2 
##                 logFC   logCPM        F       PValue          FDR
## FBgn0039155 -4.601317 5.882317 953.1722 1.646470e-14 2.403682e-10
## FBgn0025111  2.905756 6.923428 714.2877 1.257310e-13 9.177735e-10
## FBgn0035085 -2.548257 5.684922 452.3866 3.068717e-12 1.311741e-08
## FBgn0003360 -3.173036 8.452776 442.2207 3.594058e-12 1.311741e-08
## FBgn0029167 -2.188103 8.221274 412.0926 5.866109e-12 1.404698e-08
## FBgn0039827 -4.142255 4.397963 408.8548 6.195926e-12 1.404698e-08
## FBgn0034736 -3.492036 4.186934 403.9614 6.735313e-12 1.404698e-08
## FBgn0029896 -2.434452 5.305827 336.3777 2.386477e-11 4.355023e-08
## FBgn0000071  2.685868 4.795202 288.1793 6.900283e-11 1.119303e-07
## FBgn0034434 -3.624878 3.214994 282.6144 7.884818e-11 1.151105e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(as.data.frame(qlf))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   logFC    logCPM          F     PValue
## FBgn0000003  1.90105753 -2.636763 1.40991069 0.25938432
## FBgn0000008  0.01020453  2.953356 0.00282599 0.95833875
## FBgn0000014 -0.21077864 -1.966526 0.03020978 0.86444776
## FBgn0000015 -1.61118380 -2.223010 1.65428857 0.21877991
## FBgn0000017 -0.23044399  8.454625 3.99686462 0.06492692
## FBgn0000018 -0.09673451  5.088412 0.53070377 0.47805393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our outputs can be summarized by looking at our &lt;code&gt;logFC&lt;/code&gt; column (log fold change) where the higher the number the higher the count of that particular gene was read during the sequencing run. Secondly our &lt;code&gt;PValue&lt;/code&gt;, if it meets threshold (typically pvalue &amp;lt;= 0.05) allows the rejection of our null hypothesis, which is, there is equal expression regardless of what gene you look at.&lt;/p&gt;
&lt;p&gt;Now we want to visualize the data points after their regression fits. We must tidy up the data sets a bit to apply some &lt;code&gt;tidyverse&lt;/code&gt; magic. First the data is converted to a data frame, using &lt;code&gt;as.data.frame()&lt;/code&gt;, then we use the &lt;code&gt;rownames_to_column()&lt;/code&gt; function which sounds like its name, and pulls the row names, in this case &lt;code&gt;gene_id&lt;/code&gt;, into a new column of the dataframe. Lastly, we subset the data using the &lt;code&gt;select()&lt;/code&gt; function for only the columns we want, and order the data using the &lt;code&gt;arrange()&lt;/code&gt; function to start with the largest values with respect to the &lt;code&gt;padj&lt;/code&gt; via the &lt;code&gt;desc()&lt;/code&gt; argument inside &lt;code&gt;arrange()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_pasilla &amp;lt;- pasilla %&amp;gt;% 
  results() %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  rownames_to_column(var = &amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  select(gene_id, pvalue, padj) %&amp;gt;% 
  arrange(desc(padj))

tidy_edgeR &amp;lt;- qlf %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  rownames_to_column(var = &amp;quot;gene_id&amp;quot;) %&amp;gt;%
  select(gene_id, PValue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;full_join()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; package we are able to subset these two data frames based on the &lt;code&gt;gene_id&lt;/code&gt; column and keeping all other matching columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_join(tidy_pasilla, tidy_edgeR, 
          &amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = pvalue, y = PValue)) +
  labs(x = &amp;quot;DESeq2 pvalue&amp;quot;, y = &amp;quot;edgeR PValue&amp;quot;) +
  scale_x_log10() +
  scale_y_log10() +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = &amp;quot;blue&amp;quot;) +
  geom_vline(xintercept = 1e-25, color = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;DESeq2 vs EdgeR&amp;quot;, subtitle = &amp;quot;Each point represents a single gene, p-value is for whether 
          the gene has differential expression between groups&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-30-exercise-solution-for-chapter-8_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reference line drawn in blue here using &lt;code&gt;geom_abline()&lt;/code&gt; is to show what the data would look like if the two methods were identical.
Looking at the graph above, we see that most of the data are falling relatively close to one another when our pvalue &lt;span class=&#34;math inline&#34;&gt;\(\gt\)&lt;/span&gt; 1e-25. We know this because the alpha of the &lt;code&gt;ggplot&lt;/code&gt; object is set to 0.5, so if we see a black dot, it means there are two points, one on top of each other. This is what we would expect considering these packages &lt;code&gt;DESeq2&lt;/code&gt; and &lt;code&gt;edgeR&lt;/code&gt; have the same purpose in mind and is why we are comparing their outputs in this exercise!&lt;/p&gt;
&lt;p&gt;Below we subset the data again, this time selecting those points with pvalues &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; 1e-25 (log10 transform). When plotting these we don’t see much overlapping, supporting the variation between the &lt;code&gt;edgeR&lt;/code&gt; and &lt;code&gt;DESeq2&lt;/code&gt; modes of analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_join(tidy_pasilla, tidy_edgeR,&amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  filter(pvalue &amp;lt;= 1e-25) %&amp;gt;%
  ggplot(aes(x = pvalue, y = PValue)) +
  labs(x = &amp;quot;DESeq2 pvalue&amp;quot;, y = &amp;quot;edgeR PValue&amp;quot;) +
  geom_point(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle(&amp;quot;Genes with DESeq pvalue &amp;lt;= -25&amp;quot;, subtitle = &amp;quot;Each point represents a single gene, p-value is for whether 
          the gene has differential expression between groups&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-30-exercise-solution-for-chapter-8_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&#34; class=&#34;uri&#34;&gt;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Quasi-likelihood&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Quasi-likelihood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/Chap-CountData.html&#34; class=&#34;uri&#34;&gt;http://web.stanford.edu/class/bios221/book/Chap-CountData.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&#34; class=&#34;uri&#34;&gt;https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://geanders.github.io/RProgrammingForResearch/&#34; class=&#34;uri&#34;&gt;https://geanders.github.io/RProgrammingForResearch/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 9</title>
      <link>/post/exercise-solution-for-chapter-9/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-9/</guid>
      <description>


&lt;div id=&#34;exercise-9.2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 9.2&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;“Correspondence Analysis on color association tables:
Here is an example of data collected by looking at the number of Google hits resulting from queries of pairs of words. The numbers in Table 9.4 &lt;em&gt;[not reproduced]&lt;/em&gt; are to be multiplied by 1000. For instance, the combination of the words “quiet” and “blue” returned 2,150,000 hits. Perform a correspondence analysis of these data. What do you notice when you look at the two-dimensional biplot?&#34;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this exercise, we will essentially be repeating the correspondence analysis process seen in section 9.4.2, this time using associations between color and sentiment terms in search engine queries, rather than hair and eye color. Rather than testing whether certain hair/eye combinations are more likely to co-occur, we will be exploring whether a given color is more or less likely to be associated with certain sentiments (e.g. would we expect “orange” and “happy” to co-occur more frequently than would be expected by chance?). The same general steps can be repeated without many changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-loading-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 1: Loading libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ade4)
library(sva)
library(tidyverse)
library(factoextra)
library(janitor) #optional; function `clean_names()` makes column names easier to work with
library(ggplot2)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-two-ways-to-load-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 2: Two ways to load data:&lt;/h1&gt;
&lt;p&gt;At least working from the online version of the text, there are two ways to obtain (roughly) equal data for this exercise. One option is to copy table 9.4 directly from the book into Excel, shift the header one cell to the right to align columns with their proper names, export a .csv (&lt;code&gt;ex_9.2_color_table.csv&lt;/code&gt; in this example), and load it into R using a command like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix &amp;lt;- read_csv(&amp;quot;example_datasets/ex_9_2_color_table.csv&amp;quot;, col_names = TRUE) %&amp;gt;%
  column_to_rownames(var = &amp;#39;X1&amp;#39;) %&amp;gt;%
  as.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something to note is that this data is rounded to units of thousands (e.g. “19” is ~19,000), while the course data in the downloadable &lt;code&gt;data&lt;/code&gt; file gives more-precise values. I don’t know that this would disrupt any major correlations, but it could cause some minor discrepancies on comparison.&lt;/p&gt;
&lt;p&gt;Alternatively, the file is included in the course data as &lt;code&gt;colorsentiment.csv&lt;/code&gt;, although in a different, three-column format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_csv(&amp;quot;example_datasets/colorsentiment.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   `&amp;lt;X&amp;gt;` = col_character(),
##   `&amp;lt;Y&amp;gt;` = col_character(),
##   Results = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   `&amp;lt;X&amp;gt;` `&amp;lt;Y&amp;gt;`     Results
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 blue  happy     8310000
## 2 blue  depressed  957000
## 3 blue  lively    1250000
## 4 blue  clever    1270000
## 5 blue  perplexed   71300
## 6 blue  virtuous    80200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be converted to match our correspondence table format using the &lt;code&gt;pivot_wider&lt;/code&gt; function and other &lt;code&gt;tidyverse&lt;/code&gt; formatting tools:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix &amp;lt;- read_csv(&amp;quot;example_datasets/colorsentiment.csv&amp;quot;) %&amp;gt;%
  janitor::clean_names() %&amp;gt;% #standardizes column name format
  arrange(desc(results)) %&amp;gt;% # Rearranging to match row/col order in table 9.4
  pivot_wider(names_from = x, values_from = results) %&amp;gt;% # converts colors from column entries (`x`) to column names
  column_to_rownames(var = &amp;#39;y&amp;#39;) 

color_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              black   white   green    blue  orange  purple    grey
## happy     19300000 9150000 8730000 8310000 4220000 2610000 1920000
## angry      2970000 1730000 1740000 1530000 1040000  710000  752000
## quiet      2770000 2510000 2140000 2150000 1220000  821000  875000
## lively     1840000 1480000 1350000 1250000  621000  488000  659000
## clever     1650000 1420000 1320000 1270000  693000  416000  495000
## depressed  1480000 1270000  983000  957000  330000  102000  147000
## virtuous    179000  165000  102000   80200   24700   17300   20000
## perplexed   110000  109000   80100   71300   23300   15200   18900&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correspondence-analysis-following-section-9.4.2-as-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correspondence Analysis (following section 9.4.2 as a model)&lt;/h1&gt;
&lt;p&gt;Setting up the correspondence analysis object using the correspondence analysis &lt;code&gt;dudi&lt;/code&gt; function from the &lt;code&gt;ade4&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix_ca &amp;lt;- dudi.coa(color_matrix, n = 2, scannf = FALSE) # scannf = FALSE stops automatic printing of eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a special “&lt;em&gt;Du&lt;/em&gt;ality &lt;em&gt;Di&lt;/em&gt;agram” list object (used by the &lt;code&gt;ade4&lt;/code&gt; package for correspondence analysis, but also principal component analysis and other methods) containing a variety of data generated by the analysis; the call &lt;code&gt;?dudi()&lt;/code&gt; will give a list of what each component contains (axis weights, point coordinates, etc.).Stored features include as the base data table (&lt;code&gt;tab&lt;/code&gt;), a vector of eigenvalues (&lt;code&gt;eig&lt;/code&gt;), and a variety of information on row and column data (e.g. weights, coordinates, and principal components).&lt;/p&gt;
&lt;p&gt;Question 9.2 specifies that we use two dimensions, but visualizing the eigenvalues with the &lt;code&gt;factoextra&lt;/code&gt; package confirms that this is a good representation of the system, with almost all variance being explained by the first two dimensions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_eig(color_matrix_ca, geom = &amp;#39;bar&amp;#39;) # visualizing eigenvalues &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Following the book’s example in 9.4.2, we can explicitly calculate a residual matrix, which shows the difference between expected (assuming random distribution) and observed values for given row/column intercepts, in the following steps. This doesn’t feed into visualizations, but it may be helpful to have a quantitative reference for residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rowsums_colors &amp;lt;- as.matrix(apply(color_matrix, 1, sum))
colsums_colors &amp;lt;- as.matrix(apply(color_matrix, 2, sum))
expected_colors &amp;lt;- rowsums_colors %*% t(colsums_colors) / sum(colsums_colors) # using matrix multiplication to see what 
# &amp;quot;average&amp;quot; values should look like if row and column sums are distributed evenly across the dataset

#sum((color_matrix - expected_colors)^2 / expected_colors)

# subtracting the &amp;quot;expected&amp;quot; matrix from the observed  values to see discrepancies

(residual_table_colors &amp;lt;- color_matrix - expected_colors %&amp;gt;% 

    t() %&amp;gt;% 

    round())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              black    white    green     blue   orange  purple    grey
## happy      2604538  7252731  6644019  7090159  3616948 2332753 1890798
## angry     -6856953   -19511  -241131   891748   657779  448415  620320
## quiet     -6291637   848427  1103422  1745469   859372  639948  797493
## lively    -6766161   610622   693006   868322 -1000836  381433  587529
## clever    -2852964   868979   700121  -965911  -261613  317732  427122
## depressed -1374026   750108 -1383422  -359058  -550269    8671  111484
## virtuous  -2513797 -3678280 -1290876 -1133364  -811323  -31532   -2510
## perplexed -3113357 -2153156 -1204300 -1081265  -414128  -15750   -2339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we can see that, for instance, the combination of “happy” and “black” in searches occurs significantly more often (~26,000,000 additional instances) than we’d expect given no correlations between colors and sentiments, while e.g. “happy” and “grey” is much rarer.&lt;/p&gt;
&lt;p&gt;To take these patterns more intuitively, we can use a mosaic plot to visualize the proportional distribution of color/sentiment terms in searches (e.g. “back” and “happy” are both popular terms, so could be expected to occur more frequently than e.g. “perplexed” and “purple” in any case), as well as color coding for residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mosaicplot(t(color_matrix / 2000), las = 2, shade = TRUE, type = &amp;#39;pearson&amp;#39;) # dividing values by 2,000 because the mosaic plot function doesn&amp;#39;t seem to auto-scale colors, meaning that the unaltered matrix is all &amp;quot;flattened&amp;quot; to the &amp;lt;4 or &amp;gt;4 category.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  #transposing is just aesthetic; seems easier to follow with sentiments on the y axis as far as labels and visual row/column continuity. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this pattern, it’s easier to see broad patterns and associations among colors and sentiments. For instance, if we focus on colors, we can see &lt;code&gt;black&lt;/code&gt; is quite distinct from most colors, with more than expected with &lt;code&gt;happy&lt;/code&gt; and lower for other sentiments. All other sentiments behave more like each other than &lt;code&gt;black&lt;/code&gt;, but do show a smaller division between &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, and &lt;code&gt;green&lt;/code&gt;; this is distinguished from &lt;code&gt;orange&lt;/code&gt;, &lt;code&gt;purple&lt;/code&gt;, and &lt;code&gt;grey&lt;/code&gt; by being rarer than expected (under random distribution) with &lt;code&gt;angry&lt;/code&gt; and more common with &lt;code&gt;depressed&lt;/code&gt;, &lt;code&gt;virtuous&lt;/code&gt;, and &lt;code&gt;perplexed&lt;/code&gt;. Based on this, in a 2D projection we might expect to see the largest separation between &lt;code&gt;black&lt;/code&gt; and all other colors, with a smaller but obvious distinction between the two other color clusters. Because &lt;code&gt;quiet&lt;/code&gt;, &lt;code&gt;lively&lt;/code&gt;, and &lt;code&gt;clever&lt;/code&gt; are more common than expected for everything but black, the will likely be about equidistant between these clusters.&lt;/p&gt;
&lt;p&gt;To check how close we got with these eyeballed estimates, we can use the &lt;code&gt;factoextra&lt;/code&gt; biplot visualization function &lt;code&gt;fviz_ca_biplot&lt;/code&gt; for correspondence analysis to see our biplot for sentiment and color searches. I thought using the option to represent one as vector arrow, rather than points, also improves legibility (e.g. the relationship between &lt;code&gt;angry&lt;/code&gt; and &lt;code&gt;orange&lt;/code&gt;/&lt;code&gt;purple&lt;/code&gt; become more obvious):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_ca_biplot(color_matrix_ca, arrows = c(FALSE, TRUE), repel = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/fviz%20biplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, sentiment and color groups show similar relationships from what we might expect comparing patterns of positive/negative residuals in the mosaic plot. However, this makes it easier to see some patterns, such as the strong opposition between &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;grey&lt;/code&gt; on dimension 1, or the fact that most of dimension 2 is due to the differences of &lt;code&gt;green-white-blue&lt;/code&gt; and &lt;code&gt;perplexed-depressed-virtuous&lt;/code&gt; from the rest of the data. We can also make out some smaller trends that weren’t obvious (at least to me) from the mosaic visualization, like &lt;code&gt;grey&lt;/code&gt; being more distinct from &lt;code&gt;orange&lt;/code&gt; and &lt;code&gt;purple&lt;/code&gt; than we could make out with the mosaic plot’s effective “resolution”. This separation appears to be driven by higher co-occurrence with &lt;code&gt;lively&lt;/code&gt;, &lt;code&gt;quiet&lt;/code&gt;, and &lt;code&gt;clever&lt;/code&gt;. Looking back to our mosaic plot, the latter three have lighter shades of blue in &lt;code&gt;orange&lt;/code&gt; and &lt;code&gt;purple&lt;/code&gt;, with no obvious difference with &lt;code&gt;angry&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-directly-with-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting directly with &lt;code&gt;ggplot&lt;/code&gt;:&lt;/h1&gt;
&lt;p&gt;While &lt;code&gt;factoextra&lt;/code&gt; uses custom functions to streamline the process, it’s possible to approximate the same visualization using &lt;code&gt;ggplot&lt;/code&gt; and components of the &lt;code&gt;dudi&lt;/code&gt; object. In the &lt;code&gt;color_matrix_ca&lt;/code&gt; object, the ‘row’ and ‘column’ factor coordinates (emotion and color, respectively) are stored at &lt;code&gt;.$li&lt;/code&gt; and &lt;code&gt;.$co&lt;/code&gt;, This allows direct plotting; you could also look at e.g. normalized scores in &lt;code&gt;.$l1&lt;/code&gt; and &lt;code&gt;.$c1&lt;/code&gt; as well. I was able to get a general sense of how the &lt;code&gt;factoextra&lt;/code&gt; authors approached this using the call &lt;code&gt;View(fviz_ca_biplot)&lt;/code&gt; to pull up the function’s R code; this ultimately pointed to the more fundamental &lt;code&gt;fviz&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Using this information, we can render single plots for color:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Single plots: (roughly equivalent to default output for `fviz_ca_col` and `fviz_ca_row`)

color_nmds &amp;lt;- color_matrix_ca$co %&amp;gt;%
  ggplot() + 
  aes(x = Comp1, y = Comp2) +
  geom_point(color = &amp;#39;blue&amp;#39;) +
  geom_text(label = rownames(color_matrix_ca$co), nudge_y = 0.01, color = &amp;#39;blue&amp;#39;) + 
  coord_fixed()

color_nmds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/ggplot_single_plots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and sentiment (&lt;em&gt;below&lt;/em&gt;). We can also use the &lt;code&gt;geom_segment&lt;/code&gt; function in &lt;code&gt;ggplot&lt;/code&gt; to replicate the arrows seen in the &lt;code&gt;factoExtra&lt;/code&gt; biplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_nmds &amp;lt;- color_matrix_ca$li %&amp;gt;%
  ggplot() + 
  aes(x = Axis1, y = Axis2) +
  geom_point(color = &amp;#39;red&amp;#39;) +
  ggrepel::geom_text_repel(label = rownames(color_matrix_ca$li), nudge_y = 0.01, color = &amp;#39;red&amp;#39;) +
  geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2), 

               arrow = arrow(length = unit(0.3,&amp;quot;cm&amp;quot;)), 

               color = &amp;quot;red&amp;quot;) +
  coord_fixed()
  

emotion_nmds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/emotion_nmds_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can combine these elements into a biplot by combining the above dataframes, with one column for each value, and plotting the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Biplot (there are probably more efficient/correct approaches)

#manually joining the two datasets using a common index (generates a partial row of NAs, with 8 sentiments and 7 colors)

color_component &amp;lt;- color_matrix_ca$co %&amp;gt;%
  rownames_to_column(var = &amp;quot;color&amp;quot;) %&amp;gt;%
  rownames_to_column(var = &amp;quot;index&amp;quot;)

emotion_component &amp;lt;- color_matrix_ca$li %&amp;gt;%
  rownames_to_column(var = &amp;quot;emotion&amp;quot;) %&amp;gt;%
  rownames_to_column(var = &amp;quot;index&amp;quot;)

biplot_composite &amp;lt;- color_component %&amp;gt;%
  full_join(emotion_component, by = &amp;quot;index&amp;quot;)

(biplot_composite)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   index  color       Comp1       Comp2   emotion       Axis1       Axis2
## 1     1  black  0.17794592  0.04039331     happy  0.11532145  0.01602901
## 2     2  white -0.05317953 -0.10399481     angry -0.10756934  0.09837664
## 3     3  green -0.03347037 -0.03237667     quiet -0.20048082 -0.01564279
## 4     4   blue -0.03552655 -0.04588027    lively -0.20105512  0.01174266
## 5     5 orange -0.09284092  0.07047109    clever -0.17886743 -0.03423825
## 6     6 purple -0.15049601  0.15422498 depressed  0.03935532 -0.24782949
## 7     7   grey -0.36826914  0.10335528  virtuous  0.05572888 -0.24684976
## 8     8   &amp;lt;NA&amp;gt;          NA          NA perplexed -0.04792928 -0.22173448&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot_composite_plot &amp;lt;- biplot_composite %&amp;gt;%
  ggplot() +
  aes(x = Comp1, y = Comp2) +
  geom_point(color = &amp;#39;red&amp;#39;) +
  geom_text(label = biplot_composite$color, nudge_y = 0.01, color = &amp;#39;red&amp;#39;) +
   geom_segment(aes(x = 0, y = 0, xend = Comp1, yend = Comp2), 

               arrow = arrow(length = unit(0.3,&amp;quot;cm&amp;quot;)), 

               color = &amp;quot;red&amp;quot;) + 
  geom_point(aes(x = Axis1, y = Axis2), color = &amp;#39;blue&amp;#39;) +
  geom_text_repel(aes(x = Axis1, y = Axis2), label = biplot_composite$emotion, nudge_y = 0.01, color = &amp;#39;blue&amp;#39;) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  labs(x = paste0(&amp;quot;Dim1(&amp;quot;,round(color_matrix_ca$eig[1]/sum(color_matrix_ca$eig)*100, 1),&amp;quot;%)&amp;quot;),
       y = paste0(&amp;quot;Dim2(&amp;quot;,round(color_matrix_ca$eig[2]/sum(color_matrix_ca$eig)*100, 1),&amp;quot;%)&amp;quot;) +
       coord_fixed()
       ) 

biplot_composite_plot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_text).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/biplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second Approach&lt;/h1&gt;
&lt;p&gt;Dr. Anderson also worked out a more-efficient approach making use of &lt;code&gt;purrr&lt;/code&gt; package function, &lt;code&gt;unclass&lt;/code&gt;, &lt;code&gt;map_at&lt;/code&gt; and other tools to unite the two &lt;code&gt;coa / dudi&lt;/code&gt; objects with fewer intermediate steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix_ca %&amp;gt;% 
  # `unclass` to work with this as a regular list
  unclass() %&amp;gt;% 
  # `keep` lets you keep just some elements of a list. We&amp;#39;ll keep &amp;quot;co&amp;quot; and &amp;quot;li&amp;quot; elements
  keep(names(.) %in% c(&amp;quot;co&amp;quot;, &amp;quot;li&amp;quot;)) %&amp;gt;% 
  # both of these have important info in their rownames, so move those into a column.
  # `map` allows you to do the same thing to every element of the list (now just &amp;quot;co&amp;quot; and &amp;quot;li&amp;quot;
  map(rownames_to_column) %&amp;gt;% 
  # `map_at` lets you do something to *just* some elements of a list. So here, to be able
  # to bind the two dataframe elements in the list into one dataframe, you need to 
  # make sure they have the same column names. Currently, they don&amp;#39;t, so we need to 
  # change the column names for one of them.
  map_at(&amp;quot;co&amp;quot;, ~ rename(.x, Axis1 = Comp1, Axis2 = Comp2)) %&amp;gt;% 
  # Now that we have a list where each element is a dataframe with the same number
  # of columns, and where those columns have the same names and data types, you 
  # can use `bind_rows` to stick them together into one dataframe (it turns out that
  # this is a *super* helpful function). After this step, you have a tidy dataframe. The
  # `.id` parameter is adding a column with the original list element name, so you can 
  # tell which rows originally came from &amp;quot;co&amp;quot; and which from &amp;quot;li&amp;quot;
  bind_rows(.id = &amp;quot;id&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = Axis1, y = Axis2, color = id)) + 
  geom_point() + 
  # You can add arrows to your segments with the `arrow` function (from the
  # `grid` package, which is very old school graphics and what ggplot is built on)
  # To have everything come from the center, you set the starting point to 0 for
  # both x- and y-axes
  geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2),
               arrow = arrow(length = unit(0.1, &amp;quot;inches&amp;quot;))) + 
  geom_text_repel(aes(label = rowname)) + 
  # `coord_fixed` ensures that the x- and y-axes are scaled so they have the 
  # same unit size
  coord_fixed() + 
  # `str_c` lets you stick character strings together (`paste0` would also work here)
  labs(x = str_c(&amp;quot;Dim 1 (&amp;quot;, 
                 round(100 * color_matrix_ca$eig[1] / sum(color_matrix_ca$eig), 1), 
                 &amp;quot;%)&amp;quot;), 
       y = str_c(&amp;quot;Dim 2 (&amp;quot;, 
                 round(100 * color_matrix_ca$eig[2] / sum(color_matrix_ca$eig), 1), 
                 &amp;quot;%)&amp;quot;)) + 
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) + 
  # Get rid of the color legend
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  # Add some reference lines for 0 on the x- and y-axes
  geom_hline(aes(yintercept = 0), linetype = 3) + 
  geom_vline(aes(xintercept = 0), linetype = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 7</title>
      <link>/post/exercise-solution-for-chapter-7/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-7/</guid>
      <description>


&lt;div id=&#34;exercise-7.4-from-modern-statistics-for-modern-biology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 7.4 from Modern Statistics for Modern Biology&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Let’s revisit the Hiiragi data and compare the weighted and unweighted approaches. 7.4a Make a correlation circle for the unweighted Hiiragi data &lt;code&gt;xwt&lt;/code&gt;. Which genes have the best projections on the first principal plane (best approximation)? 7.4b Make a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;read-in-and-clean-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in and clean data&lt;/h2&gt;
&lt;p&gt;We start by loading the libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Hiiragi2013&amp;quot;)
library(&amp;quot;ade4&amp;quot;)
library(&amp;quot;factoextra&amp;quot;)
library(&amp;quot;pander&amp;quot;)
library(&amp;quot;knitr&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;ggrepel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can install the libraries using &lt;code&gt;install.packages&lt;/code&gt; for the &lt;code&gt;ade4&lt;/code&gt; and &lt;code&gt;factoextra&lt;/code&gt; packages and such, and the following line for &lt;code&gt;Hiiragi2013&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BiocManager::install(&amp;quot;Hiiragi2013&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will analyze data from the &lt;code&gt;Hiiragi2013&lt;/code&gt; package containing a gene expression microarray dataset describing the transcriptomes of about 100 cells from mouse embryos at different times during early development.&lt;/p&gt;
&lt;p&gt;I copied the code from the textbook to clean the data. A tidyverse version of this code is available on
&lt;a href=&#34;https://github.com/geanders/csu_msmb_practice/blob/master/ch_7_examples.pdf&#34;&gt;Brooke Andersons’s github&lt;/a&gt;. In either case, we select the wildtype (WT) samples. Then we select the 100 features with the largest variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;x&amp;quot;, package = &amp;quot;Hiiragi2013&amp;quot;)
xwt &amp;lt;- x[, x$genotype == &amp;quot;WT&amp;quot;]
sel &amp;lt;- order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt &amp;lt;- xwt[sel, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting data is of the &lt;code&gt;ExpressionSet&lt;/code&gt; class, a class designed to combine many types of data such as microarray data, metadata, and protocol information. The data is shown below. I transposed the data so the rows correspond to samples and the first 101 columns correspond to different genes. I only showed the first the columns and then skipped to the end for brevity. The last columns give additional data about each sample, such as the total number of cells and the scan date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(as.data.frame(xwt)[,c(1:3,102:108)]))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1434584_a_at&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1437325_x_at&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1420085_at&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Embryonic.day&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Total.number.of.cells&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;lineage&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genotype&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ScanDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sampleGroup&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sampleColour&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.24888&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.332223&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.027715&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.98757&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.475742&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.293017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.72695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.955642&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.940142&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.51926&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.061255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.715243&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.01299&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.308667&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.924228&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.50750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.202948&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.325952&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;a-make-a-correlation-circle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;a) Make a correlation circle&lt;/h2&gt;
&lt;p&gt;Next we plot a correlation circle. This allows us to plot the original genes projected onto the first two principal axes. We can interpret the angles between the vectors as a measure of the correlation between the two corresponding genes. The length of the arrows indicates the correlation of a gene with the first principal axes. This allows us to visualize the correlation between genes, as well as see which genes are best described by our first two principal components. The first principal component is the linear combination of the genes with maximum variance. The second principal component is the linear combination of the genes with maximum variance while being orthogonal to the first principal component.&lt;/p&gt;
&lt;p&gt;First I used &lt;code&gt;dudi.pca&lt;/code&gt; to perform principal component analysis and get a pca and dudi class object. In the text book, they performed a weighted PCA because the groups had very different sample sizes ranging from 4 to 36. A weighted analysis would give the groups equal weight, while an unweighted would give each observation equal weight (and thus give less weight to groups with less members). We are interested in the difference in the genes at the various developmental phases, so in general it would make sense to do a weighted analysis in order to let each developmental phase group have an equal say. To compare the weighted with an unweighted PCA I didn’t use the &lt;code&gt;row.w&lt;/code&gt; option as the textbook did in order to fit an unweighted PCA.&lt;/p&gt;
&lt;p&gt;In the following code &lt;code&gt;t&lt;/code&gt; takes the transpose so the rows correspond to samples. The &lt;code&gt;exprs&lt;/code&gt; function is used to access the expression and error measurements of the data. I chose to center and scale the data, meaning make each column have a mean of 0 and a variance of 1. The &lt;code&gt;scannf&lt;/code&gt; option prevents the function from plotting a scree plot and &lt;code&gt;nf&lt;/code&gt; tells it to keep two axes (i.e. 2 principal components).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcaMouse &amp;lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
                            center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I used the &lt;code&gt;fviz_pca_var&lt;/code&gt; function to plot the correlation circle. I used &lt;code&gt;geom= &#34;arrow&#34;&lt;/code&gt; to remove the labels of each arrow as they were all overlapping. The default for this argument is to print both arrows and text labels for each line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_pca_var(pcaMouse, col.circle = &amp;quot;black&amp;quot;,geom= &amp;quot;arrow&amp;quot;) + ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare that plot to the one below showing the correlation circle from a weighted analysis. We can see in the weighted analysis the first principal plane does a better job of explaining the variation as indicated by the percent of variation explained by each dimension as well as the length of the arrows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab = table(xwt$sampleGroup)
xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])
pcaMouseWeighted &amp;lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
                    row.w = xwt$weight,
                    center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)
fviz_pca_var(pcaMouseWeighted, col.circle = &amp;quot;black&amp;quot;,geom= &amp;quot;arrow&amp;quot;) + ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The genes that have the best projections on the first principal plane (meaning those with the strongest correlation with the principal axes) are those with the longest arrows on the plots above. I extracted the genes with arrow lengths greater than 0.8 from the unweighted PCA plot with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrCircle &amp;lt;- fviz_pca_var(pcaMouse, col.circle = &amp;quot;black&amp;quot;)$data
arrowLengths &amp;lt;- sqrt(corrCircle$x^2+corrCircle$y^2)
cutoff &amp;lt;- 0.8
kpInd &amp;lt;- order(arrowLengths, decreasing=TRUE)[1:sum(arrowLengths&amp;gt;cutoff)]
genes &amp;lt;- corrCircle[kpInd,&amp;quot;name&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;1456270_s_at&lt;/em&gt;, &lt;em&gt;1450624_at&lt;/em&gt;, &lt;em&gt;1449134_s_at&lt;/em&gt;, &lt;em&gt;1418153_at&lt;/em&gt;, &lt;em&gt;1420085_at&lt;/em&gt;, &lt;em&gt;1420086_x_at&lt;/em&gt;, &lt;em&gt;1434584_a_at&lt;/em&gt;, &lt;em&gt;1450843_a_at&lt;/em&gt;, &lt;em&gt;1429483_at&lt;/em&gt;, &lt;em&gt;1437308_s_at&lt;/em&gt;, &lt;em&gt;1456598_at&lt;/em&gt;, &lt;em&gt;1460605_at&lt;/em&gt;, &lt;em&gt;1429388_at&lt;/em&gt;, &lt;em&gt;1426990_at&lt;/em&gt;, &lt;em&gt;1436392_s_at&lt;/em&gt; and &lt;em&gt;1452270_s_at&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To summarize, the code above saves output from the &lt;code&gt;fviz_pca_var&lt;/code&gt; function to an object called &lt;code&gt;corrCircle&lt;/code&gt;. I used the output (which included x and y coordinates) to calculate the arrow length for each gene. I then created a vector, &lt;code&gt;kpInd&lt;/code&gt;, with the indice number for genes that had a length greater then the cutoff of 0.8. Finally, I output the names of those genes with length great than 0.8. Below is a plot showing just these genes with the ``best projection,&#34; meaning they are most correlated with the plane of maximum variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrCircle %&amp;gt;% 
  mutate(length = sqrt(x^2 + y^2)) %&amp;gt;% 
  filter(length &amp;gt;= 0.8) %&amp;gt;% 
  ggplot(aes(x = 0, xend = x, y = 0, yend = y)) + 
  geom_segment(arrow = arrow(length = unit(0.1, &amp;quot;inches&amp;quot;))) + 
  geom_label_repel(aes(x = x, y = y, label = name), 
                   size = 2, alpha = 0.7) + 
  coord_fixed()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-make-a-biplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;b) Make a biplot&lt;/h2&gt;
&lt;p&gt;Next we make a biplot to visualize samples and the genes in one plot. I used the &lt;code&gt;fviz_pca_biplot&lt;/code&gt; function. The &lt;code&gt;col.var&lt;/code&gt; and &lt;code&gt;col.ind&lt;/code&gt; options allow you to color the different genes and sample points, respectively, by particular groups. I used &lt;code&gt;label=&#34;&#34;&lt;/code&gt; to remove the labels on the vectors and points. The &lt;code&gt;above8&lt;/code&gt; variable is simply a vector of either “Less than 0.8” or “Greater than 0.8” to indicate if each gene’s vector length was less than or greater than our cutoff value on the correlation circle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;above8 &amp;lt;- rep(&amp;quot;Less than 0.8&amp;quot;, dim(xwt)[1])
above8[1:100 %in% kpInd] &amp;lt;- &amp;quot;Greater than 0.8&amp;quot;
fviz_pca_biplot(pcaMouse, col.var=above8, col.ind=xwt$sampleGroup, label=&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The colors/shapes of the point indicate the sample group of each sample point. The colors of the arrows indicate whether or not the corresponding gene had length greater than or less than 0.8 on the correlation circle.&lt;/p&gt;
&lt;p&gt;We can see the EPI and PE groups (the groups with the fewest samples) appear more extreme in the first principle plane when using the unweighted PCA. This was not the case in the weighted PCA (shown below) because then groups were weighted equally. Here, the PCA mostly depends on the larger groups (to best understand the most observations), so the smaller groups appear more extreme on the principal plane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;above8 &amp;lt;- rep(&amp;quot;Less than 0.8&amp;quot;, dim(xwt)[1])
above8[1:100 %in% kpInd] &amp;lt;- &amp;quot;Greater than 0.8&amp;quot;
fviz_pca_biplot(pcaMouseWeighted, col.var=above8, col.ind=xwt$sampleGroup, label=&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise Solution for 5.1</title>
      <link>/post/exercise-solution-for-5-1/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-5-1/</guid>
      <description>


&lt;p&gt;This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;silhouette index&lt;/strong&gt; is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\displaystyle S(i) = \frac{B(i) - A(i)}{max_i(A(i), B(i))} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The book explains the equation by first defining that the average dissimilarity of a point &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to a cluster &lt;span class=&#34;math inline&#34;&gt;\(C_k\)&lt;/span&gt; is the average of the distances from &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to all of the points in &lt;span class=&#34;math inline&#34;&gt;\(C_k\)&lt;/span&gt;. From this, let &lt;span class=&#34;math inline&#34;&gt;\(A(i)\)&lt;/span&gt; be the average dissimlarity of all points in the cluster that &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; belongs to, and &lt;span class=&#34;math inline&#34;&gt;\(B(i)\)&lt;/span&gt; is the lowest average of dissimlarity of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to any other cluster of which &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is NOT a member.Basically, we are subtracting the mean distance to other instances in the same cluster from the mean distance to the instances of the next closest cluster, and dividing it by which of the two values is larger. The output is a coefficient that will vary between -1 and 1, where a value closer to 1 implies that the instance is closest to the correct cluster.&lt;/p&gt;
&lt;p&gt;The solution to this exercise requires the following R packages to be loaded into your environment.&lt;/p&gt;
&lt;div id=&#34;required-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Libraries&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cluster)
library(dplyr)
library(ggplot2)
library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part A&lt;/h2&gt;
&lt;p&gt;Question 5.1.a asks us to compute the silhouette index for the &lt;code&gt;simdat&lt;/code&gt; data that was simulated in Section &lt;strong&gt;5.7&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The provided code is used to simulate data coming from four separate groups. They use the pipe operator to concatenate four different, randomly generated, data sets. The &lt;code&gt;ggplot2&lt;/code&gt; package is used to take a look at the data as a barchart of the within-groups sum of squared distances (WSS) obtained from the &lt;em&gt;k&lt;/em&gt; means method.&lt;/p&gt;
&lt;p&gt;First off, we need to set the seed to ensure reproducible results with a randomly generated data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following chunk of code utilizes the &lt;code&gt;lapply&lt;/code&gt; function two times to generate a datset with four distinct clusters. The &lt;code&gt;lapply&lt;/code&gt; function comes from base R, and is most often used to apply a function over an entire list or vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)


simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

simdatxy = simdat[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)] # data without class label&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The technique the authors used to generate a clustered dataset is tricky. The &lt;code&gt;lapply&lt;/code&gt; within an &lt;code&gt;lapply&lt;/code&gt;, paired with two &lt;code&gt;bind_rows&lt;/code&gt; functions can be confusing. The next sections are included to demonstrate what the data looks like through various steps in this process, and help bring understanding to the reader how the code is working.&lt;/p&gt;
&lt;div id=&#34;the-inner-lapply-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The inner &lt;code&gt;lapply&lt;/code&gt; function&lt;/h4&gt;
&lt;p&gt;The first &lt;code&gt;lapply&lt;/code&gt; is generating a vector of n = 100 normally distributed random numbers, and creating two separate dataframes packed into a list that consist of the mean (&lt;code&gt;my&lt;/code&gt;) and standard deviation, respectively. Each individual value is specifically assigned a 0 or an 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simdatmy = lapply(c(0,8), function(my) {
    
    tibble(y = rnorm(100, mean = my, sd = 2),
           class = paste(my, sep = &amp;quot;:&amp;quot;))
   })
summary(simdatmy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Length Class  Mode
## [1,] 2      tbl_df list
## [2,] 2      tbl_df list&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-outer-lapply-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The outer &lt;code&gt;lapply&lt;/code&gt; function&lt;/h4&gt;
&lt;p&gt;The second (outer) &lt;code&gt;lapply&lt;/code&gt; uses the same idea to apply the same, random, 0 or 8 assignment to values in the &lt;code&gt;mx&lt;/code&gt; function. The ouput is now four separate dataframes within a list that contain all of the &lt;code&gt;mx&lt;/code&gt; data and all of the &lt;code&gt;my&lt;/code&gt; data. Within the &lt;code&gt;tibble&lt;/code&gt; function, they include the code &lt;code&gt;class =&lt;/code&gt; to ensure that each row in each of the 4 the lists is assigned one of the four possible two-way combinations of 0 and 8. This is important to simulate a clustered dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simdatmx = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
    })})
summary(simdatmx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Length Class  Mode
## [1,] 2      -none- list
## [2,] 2      -none- list&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-together&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Putting it together&lt;/h4&gt;
&lt;p&gt;The last step is to bind the list of dataframes into one single dataframe. The final dataframe includes all of the x and y data, each with assigned classes, defined by a combination of 0 and 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)


simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

head(simdat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##        x       y class
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 -1.25  -1.24   0:0  
## 2  0.367  0.0842 0:0  
## 3 -1.67  -1.82   0:0  
## 4  3.19   0.316  0:0  
## 5  0.659 -1.31   0:0  
## 6 -1.64   3.53   0:0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(simdat$class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;0:0&amp;quot; &amp;quot;0:8&amp;quot; &amp;quot;8:0&amp;quot; &amp;quot;8:8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;simdat&lt;/code&gt; dataframe includes 400 random points witih an assigned class to simulate clustering. We can look at the data using a simple &lt;code&gt;ggplot&lt;/code&gt; scatterplot, color coded by the class of each point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(simdat, aes(x = x, y = y, color = class)) + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next part of exploring the data is to compute the within-groups sum of squares (WSS) for the clusters that we just generated. The goal of this section is to observe how the WSS changes as the number of clusters is increased from 1 to 8 when using the k-means. Chapter 5 provides us with the following code and graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1
wss = tibble(k = 1:8, value = NA_real_)

#2
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)

#3
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}

ggplot(wss, aes(x = k, y = value)) + geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is really going on here?&lt;/p&gt;
&lt;p&gt;This first chunk is setting up a one-column dataframe with blank &lt;code&gt;NA&lt;/code&gt; values. The &lt;code&gt;NA&lt;/code&gt;s will be filled in with values as the rest of the code processes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1
wss = tibble(k = 1:8, value = NA_real_)
wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k value
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1    NA
## 2     2    NA
## 3     3    NA
## 4     4    NA
## 5     5    NA
## 6     6    NA
## 7     7    NA
## 8     8    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second chunk of code is calculating the value for k = 1 individually. They use the &lt;code&gt;scale&lt;/code&gt; function to scale down the value for k = 1 because it is so much larger than the rest of the k-values. Without scaling down the k = 1 value, it would be difficult to observe any sharp decreases that might indicate a “potential sweet spot” for the number of clusters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k  value
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1 15781.
## 2     2    NA 
## 3     3    NA 
## 4     4    NA 
## 5     5    NA 
## 6     6    NA 
## 7     7    NA 
## 8     8    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last part of this chunk is running a k-means clustering on the remaining k 2 through 8 and then pulling out the &lt;code&gt;withinss&lt;/code&gt; value for all of the observations, summing it, and assigning that value to each individual k-value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}

km$withinss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 275.2533 165.2368 199.8292 257.9090 285.8292 131.9047 239.8457 339.1308&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k  value
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1 15781.
## 2     2  9055.
## 3     3  5683.
## 4     4  3088.
## 5     5  2755.
## 6     6  2441.
## 7     7  2152.
## 8     8  1895.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These corresponding values are then neatly displayed in a barchart of the WSS stastistic as a function of k. The sharp decrease between k = 3 and k = 4 (at the &lt;em&gt;elbow&lt;/em&gt;) is indicative of the number of clusters present in the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(wss, aes(x = k, y = value)) + geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-the-silhouette-index-for-simdat&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Computing the silhouette index for &lt;code&gt;simdat&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Next up is the code necessary to plot the silhouette index. The &lt;code&gt;silhouette&lt;/code&gt; function comes from the &lt;code&gt;cluster&lt;/code&gt; package, and the resulting graph provides an average silhouette width for k = 4 clusters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4, border = NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pam&lt;/code&gt; (partitioning around medoids) function is doing the same thing as the &lt;code&gt;kmeans&lt;/code&gt; call from the earlier chunk of code, but using the &lt;code&gt;cluster&lt;/code&gt; package’s algorithm to calculate the k-means clustering. We use the &lt;code&gt;pam&lt;/code&gt; function here because the we need the “pam” and “partition” output class to run the &lt;code&gt;silhouette&lt;/code&gt; function. With this information, we can then compute the silhouette index and view the output summary and plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pam4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;pam&amp;quot;       &amp;quot;partition&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sil = silhouette(pam4, 4, border = NA)
summary(sil)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Silhouette of 400 units in 4 clusters from pam(x = simdatxy, k = 4) :
##  Cluster sizes and average silhouette widths:
##       103       100        99        98 
## 0.5279715 0.5047895 0.4815427 0.4785642 
## Individual silhouette widths:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.04754  0.41232  0.54916  0.49858  0.63554  0.71440&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we explore the final output plot, it might be interesting to look at plots of the simulated values with their respective cluster assignments based on &lt;code&gt;pam&lt;/code&gt; k-means clustering and the silhouette index. With some (a lot of) help from Brooke, we have the following code to view this.&lt;/p&gt;
&lt;p&gt;For the most part, all of the points were assigned to the same cluster as the original, with the occational border point mis-assigned to the neighboring cluster. Interestingly, the silhouette index approaches zero when you near the border of of the cluster and is much higher near the center of the cluster. Although we would expect this, it can be helpful to view this graphically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sil %&amp;gt;% 
  unclass() %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  tibble::rownames_to_column(var = &amp;quot;orig_order&amp;quot;) %&amp;gt;% 
  arrange(as.numeric(orig_order)) %&amp;gt;% 
  bind_cols(simdat) %&amp;gt;% 
  ggplot(aes(x = x, y = y, shape = as.factor(cluster), color = sil_width)) + 
  geom_point() + 
  facet_wrap(~ class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, a silhouette plot with the n = 400 data points. The average silhouette width is a metric that we can use to summarize everything at a level of the full clustering process. Essentially, the closer that this average is to 0.5, then the more accurate our number of clusters &lt;em&gt;k&lt;/em&gt; is. This concept is further explored in the &lt;strong&gt;Part B&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sil, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part B&lt;/h2&gt;
&lt;p&gt;Question 5.1.b asks us to change the number of clusters &lt;em&gt;k&lt;/em&gt; and assess which &lt;em&gt;k&lt;/em&gt; value produces the best silhouette index.&lt;/p&gt;
&lt;p&gt;In this example, there are a couple of ways to assess which k gives the best silhouette index.One method would be trial and error and determining which k-value produces the highest silhouette index. This method works out for this example, but is impractical for much larger and complex datasets. Included below is the code for testing multiple different k-values and the resulting coefficient values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam2 = pam(simdatxy, 2)
sil2 = silhouette(pam2, 2)
plot(sil2, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam3 = pam(simdatxy, 3)
sil3 = silhouette(pam3, 3)
plot(sil3, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4)
plot(sil, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam12 = pam(simdatxy, 12)
sil12 = silhouette(pam12, 12)
plot(sil12, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam40 = pam(simdatxy, 40)
sil40 = silhouette(pam40, 40)
plot(sil40, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This trial and error method indicates that the highest silhouette index (that was tested) is achieved with k = 4.&lt;/p&gt;
&lt;p&gt;A different (seemingly more appropriate) method is to write a piece of code that will test a range of k-values automatically. This next piece of code is adapted from Amy Fox and the group that she worked with during class. This is a much more practical method that provides a clear answer of which &lt;em&gt;k&lt;/em&gt; gives the best silhouette index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- c(2:10)
df_test &amp;lt;- data.frame()
for (i in 2:10){
  
  pam_run &amp;lt;- pam(simdatxy, i)
  sil_run &amp;lt;- silhouette(pam_run, i)
  
  row_to_add &amp;lt;- data.frame(i, width = summary(sil_run)$avg.width)
  
  df_test &amp;lt;- rbind(df_test, row_to_add)
}
df_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    i     width
## 1  2 0.4067400
## 2  3 0.4000560
## 3  4 0.4985801
## 4  5 0.4401518
## 5  6 0.3957347
## 6  7 0.3717875
## 7  8 0.3699929
## 8  9 0.3670770
## 9 10 0.3516570&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_test, aes(i, width)) +
  geom_point() +
  geom_line() +
  xlab(&amp;quot;k&amp;quot;) +
  ylab(&amp;quot;Silhouette Index&amp;quot;) +
  ggtitle(&amp;quot;Testing different k values for Silhouette Index&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sil_run)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Silhouette of 400 units in 10 clusters from pam(x = simdatxy, k = i) :
##  Cluster sizes and average silhouette widths:
##        63        38        40        52        33        40        35        33 
## 0.3885059 0.3273800 0.3622990 0.3703291 0.3573781 0.3257945 0.4429236 0.2807700 
##        31        35 
## 0.3944945 0.2335738 
## Individual silhouette widths:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.1778  0.2389  0.3703  0.3517  0.4946  0.6623&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of &lt;code&gt;summary(sil_run)&lt;/code&gt; matches the trial and error method, but in a more efficient manner.&lt;/p&gt;
&lt;p&gt;In summary, k = 4 provides us with the best silhouette index value. This is because there truly are four groups in the dataset based on how we created it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part C&lt;/h2&gt;
&lt;p&gt;The last part of this exercise asks us to repeat by calculating the silhouette index on a uniform (unclustered) data distribution over a range of values.&lt;/p&gt;
&lt;p&gt;Here, a new data set is generated without clustering the randomly generated data. The 0 and 8 assignment values have been removed and replaced with a singular 1. This assigns all of the values to have the same class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

simdat1 = lapply(c(1), function(mx) {
  lapply(c(1), function(my) {
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

simdatxy1 = simdat1[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]

ggplot(simdatxy1, aes(x = x, y = y)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4.1 = pam(simdatxy1, 4)
sil.1 = silhouette(pam4.1, 4)
plot(sil.1, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-20-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average silhouette width is 0.33, which is much lower than the clustered value of 0.50 that we see with the first simulation. It should be pointed out that several of the points end up with negative silhouette widths. These observations were assigned to the wrong group entirely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/Chap-Clustering.html#ques:ques-WSSclusters&#34;&gt;Modern Statistics for Modern Biology - Chapter 5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Silhouette_(clustering)&#34;&gt;Silhouette Clustering - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@jyotiyadav99111/selecting-optimal-number-of-clusters-in-kmeans-algorithm-silhouette-score-c0d9ebb11308&#34;&gt;Blog on Selecting Optimal Number of Clusters&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 2, Part 2</title>
      <link>/post/ex-2-6/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/ex-2-6/</guid>
      <description>


&lt;div id=&#34;exercise-2.6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.6&lt;/h1&gt;
&lt;p&gt;The first part of the exercise asks you to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Choose your own prior for the parameters of the beta distribution. You can do this by sketching it here: &lt;a href=&#34;https://jhubiostatistics.shinyapps.io/drawyourprior&#34; class=&#34;uri&#34;&gt;https://jhubiostatistics.shinyapps.io/drawyourprior&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After sketching a plot, I chose the parameters to set up a prior: &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; = 2.47 and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; = 8.5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-this-prior&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using this prior&lt;/h1&gt;
&lt;p&gt;Next, the exercise asks you:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once you have set up a prior, re-analyse the data from Section 2.9.2, where we saw Y = 40 successes out of n = 300 trials.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To be able to use the &lt;code&gt;loglikelihood&lt;/code&gt; function from the text, I first needed to redefine it here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loglikelihood = function(theta, n = 300, k = 40) { ## Function definition from the textbook
  log(choose(n, k)) + k * log(theta) + (n - k) * log(1 - theta)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I created a vector of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values between 0 and 1, spaced 0.001 units wide. The plot below shows different possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the log likelihood for each of these values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetas = seq(0, 1, by = 0.001)
plot(thetas, loglikelihood(thetas), xlab = expression(theta),
     ylab = expression(paste(&amp;quot;log f(&amp;quot;, theta, &amp;quot; | y)&amp;quot;)),type = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, I used &lt;code&gt;rbeta&lt;/code&gt; to draw 1,000,000 random samples from a beta distribution with my new picks for the parameters for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rtheta = rbeta(1000000, shape1 = 2.47, shape2 = 8.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running the above, for each of these &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values, we then generate a random sample of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as observed in the histogram (with orange bars):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = vapply(rtheta, function(th) {
  rbinom(1, prob = th, size = 300)
}, numeric(1))
hist(y, breaks = 50, col = &amp;quot;orange&amp;quot;, main = &amp;quot;&amp;quot;, xlab = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our next step is to use this information to generate a posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at a fixed &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; value. In this example they used &lt;span class=&#34;math inline&#34;&gt;\(Y=40\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After running the above, for each of these thetas, we generated simulated values for the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(Y=40\)&lt;/span&gt; as observed in this histogram (with green bars).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetaPostEmp = rtheta[ y == 40 ]
hist(thetaPostEmp, breaks = 40, col = &amp;quot;chartreuse4&amp;quot;, main = &amp;quot;&amp;quot;,
     probability = TRUE, xlab = expression(&amp;quot;posterior&amp;quot;~theta), ylim=c(0,40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;densPostTheory  =  dbeta(thetas, 42.47, 268.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check how this compares to the theoretical posterior distribution
for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(Y = 40\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(thetaPostEmp, breaks = 40, col = &amp;quot;chartreuse4&amp;quot;, main = &amp;quot;&amp;quot;,
  probability = TRUE, xlab = expression(&amp;quot;posterior&amp;quot;~theta))
lines(thetas, densPostTheory, type=&amp;quot;l&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also check the means of both distributions computed above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(thetaPostEmp) # Empirical&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365577&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtheta = thetas[2]-thetas[1]
sum(thetas * densPostTheory * dtheta) # Theoretical&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365727&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monte-carlo-integration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo integration&lt;/h2&gt;
&lt;p&gt;We can use Monte Carlo integration instead and then check the agreement between our Monte Carlo sample &lt;code&gt;thetaPostMC&lt;/code&gt; and our sample &lt;code&gt;thetaPostEmp&lt;/code&gt; with a QQ plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetaPostMC = rbeta(n = 1e6, 42.47, 268.5)
mean(thetaPostMC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365684&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(thetaPostMC, thetaPostEmp, type = &amp;quot;l&amp;quot;, asp = 1)
abline(a = 0, b = 1, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;densPost2 = dbeta(thetas, 42.47, 268.5)
mcPost2   = rbeta(1e6, 42.47, 268.5)
sum(thetas * densPost2 * dtheta)  # mean, by numeric integration&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365727&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mcPost2)                     # mean, by MC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365618&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetas[which.max(densPost2)]      # MAP estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(mcPost2, c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.1007828 0.1767282&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 2, Part 1</title>
      <link>/post/exercise-solution-for-chapter-2/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-2/</guid>
      <description>


&lt;p&gt;As always, load libraries first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercise-2.3-from-modern-statistics-for-modern-biologists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 2.3 from Modern Statistics for Modern Biologists&lt;/h2&gt;
&lt;p&gt;A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus &lt;span class=&#34;math inline&#34;&gt;\(4^3\)&lt;/span&gt; would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.) The code is redundant. But is the redundancy even among codons that code for the same amino acid? In other words, if alanine is coded by 4 different codons, do these codons code for alanine equally (each 25%), or do some codons appear more often than others? Here we use the tuberculosis genome to explore codon bias.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-explore-the-data-mtb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;a) Explore the data, &lt;code&gt;mtb&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Use &lt;code&gt;table&lt;/code&gt; to tabulate the &lt;code&gt;AmAcid&lt;/code&gt; and &lt;code&gt;Codon&lt;/code&gt; variables.&lt;/p&gt;
&lt;p&gt;Each amino acid is encoded by 1–6 tri-nucleotide combinations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtb = read.table(&amp;quot;example_datasets/M_tuberculosis.txt&amp;quot;, header = TRUE)
codon_no &amp;lt;- rowSums(table(mtb))
codon_no&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ala Arg Asn Asp Cys End Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Thr Trp Tyr 
##   4   6   2   2   2   3   2   2   4   2   3   6   2   1   2   4   6   4   1   2 
## Val 
##   4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PerThousands&lt;/code&gt; of each codon can be visualized, where each plot represents an amino acid and each bar represents a different codon that codes for that amino acid. But what does the &lt;code&gt;PerThousands&lt;/code&gt; variable mean?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtb, aes(x=Codon, y=PerThous)) +
  geom_col()+
  facet_wrap(~AmAcid, scales=&amp;quot;free&amp;quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-19-exercise-solution-for-chapter-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-the-perthous-variable&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;b) The &lt;code&gt;PerThous&lt;/code&gt; variable&lt;/h1&gt;
&lt;p&gt;How was the &lt;code&gt;PerThous&lt;/code&gt; variable created?&lt;/p&gt;
&lt;p&gt;The sum of all of the numbers of codons gives you the total number of codons in the M. tuberculosis genome: &lt;code&gt;all_codons&lt;/code&gt;. Remember that this is not the size of the M. tuberculosis genome, but the number of codons in all M. tuberculosis genes. To get the size of the genome, multiply each codon by 3 (for each nucleotide) and add all non-coding nucleotides (which we do not know from this data set).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_codons = sum(mtb$Number)
all_codons&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1344223&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PerThousands&lt;/code&gt; variable is derived by dividing the number of occurrences of the codon of interest by the total number of codons. Because this number is small and hard to interpret, multiplying it by 1000 gives a value that is easy to make sense of. Here is an example for proline. The four values returned align to the four codons that each code for proline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pro  =  mtb[mtb$AmAcid == &amp;quot;Pro&amp;quot;, &amp;quot;Number&amp;quot;]
pro / all_codons * 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.560240  6.121752  3.405685 17.032144&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;c-codon-bias&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;c) Codon bias&lt;/h1&gt;
&lt;p&gt;Write an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.&lt;/p&gt;
&lt;p&gt;First, let’s look at the expected frequencies of each codon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_expected &amp;lt;- data.frame(codon_no) %&amp;gt;%
  rownames_to_column(var = &amp;quot;AmAcid&amp;quot;) %&amp;gt;%
  mutate(prob_codon = 1/codon_no)
codon_expected&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    AmAcid codon_no prob_codon
## 1     Ala        4  0.2500000
## 2     Arg        6  0.1666667
## 3     Asn        2  0.5000000
## 4     Asp        2  0.5000000
## 5     Cys        2  0.5000000
## 6     End        3  0.3333333
## 7     Gln        2  0.5000000
## 8     Glu        2  0.5000000
## 9     Gly        4  0.2500000
## 10    His        2  0.5000000
## 11    Ile        3  0.3333333
## 12    Leu        6  0.1666667
## 13    Lys        2  0.5000000
## 14    Met        1  1.0000000
## 15    Phe        2  0.5000000
## 16    Pro        4  0.2500000
## 17    Ser        6  0.1666667
## 18    Thr        4  0.2500000
## 19    Trp        1  1.0000000
## 20    Tyr        2  0.5000000
## 21    Val        4  0.2500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, calculate the observed frequencies for each codon seen in the data set and use the chi-squared test statistic to determine if the difference between expected and observed codon frequencies is even or if some codon sequences are used more than others.&lt;/p&gt;
&lt;p&gt;To start, you can group the data by amino acid and then determine a few things about
the amino acid or the possible codons for it, including the total observations
across all codons for the amino acid (&lt;code&gt;total&lt;/code&gt;), the number of codons for that
amino acid (&lt;code&gt;n_codons&lt;/code&gt;), and the expected count for each codon for that amino acid
(the total number of observations for that amino acid divided by the number of
codons, giving an expected number that’s the same for all codons of an amino
acid; &lt;code&gt;expected&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_compared &amp;lt;- mtb %&amp;gt;% 
  group_by(AmAcid) %&amp;gt;% 
  mutate(total = sum(Number),
         n_codons = n(),
         expected = total / n_codons)
codon_compared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 7
## # Groups:   AmAcid [21]
##    AmAcid Codon Number PerThous  total n_codons expected
##    &amp;lt;fct&amp;gt;  &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Gly    GGG    25874    19.2  132810        4   33202.
##  2 Gly    GGA    13306     9.9  132810        4   33202.
##  3 Gly    GGT    25320    18.8  132810        4   33202.
##  4 Gly    GGC    68310    50.8  132810        4   33202.
##  5 Glu    GAG    41103    30.6   62870        2   31435 
##  6 Glu    GAA    21767    16.2   62870        2   31435 
##  7 Asp    GAT    21165    15.8   77852        2   38926 
##  8 Asp    GAC    56687    42.2   77852        2   38926 
##  9 Val    GTG    53942    40.1  114991        4   28748.
## 10 Val    GTA     6372     4.74 114991        4   28748.
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;mutate&lt;/code&gt; function is used after &lt;code&gt;group_by&lt;/code&gt; to do all this
within each amino acid group of codons, but without collapsing to one row per
amino acid, as a &lt;code&gt;summarize&lt;/code&gt; call would.&lt;/p&gt;
&lt;p&gt;To convince yourself that this has worked out correctly, you can repeat
the plot we made before and see that the bars for the expected values are
always equal across all codons for an amino acid:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(codon_compared, aes(x=Codon, y=expected)) +
     geom_col()+
     facet_wrap(~AmAcid, scales=&amp;quot;free&amp;quot;) +
     theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-19-exercise-solution-for-chapter-2_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can calculate the chi-squared (&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;) statistic and compare it to the
chi-squared distribution to get the p-value when testing against the null hypothesis
that the amino acid observations are uniformly distributed across codons. The &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;
is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\chi^2 = \sum_i{\frac{(O_i-E_i)^2}{E_i}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(O_i\)&lt;/span&gt; is the observed value of data point &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;code&gt;Number&lt;/code&gt; in our data); and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_i\)&lt;/span&gt; is the expected value of data point &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;code&gt;expected&lt;/code&gt; in our data)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our data, we can calculate the contribution to the total &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic
from each data point (in this case, each codon within an amino acid) using
&lt;code&gt;mutate&lt;/code&gt;, and then
add these values up using &lt;code&gt;group_by&lt;/code&gt; to group by amino acid followed by
&lt;code&gt;summarize&lt;/code&gt; to sum up across all the data points for an amino acid.
The other information we need to get is the number of codons for the
amino acid, because we’ll need this to determine the degrees of freedom
for the chi-squared distribution. Next, we used &lt;code&gt;mutate&lt;/code&gt; with
&lt;code&gt;pchisq&lt;/code&gt; to determine the p-values within each amino acid group for the
test against the null that the codons are uniformly distributed for that
amino acid (i.e., that there isn’t codon bias). These p-values turn out to
be super small, so we’re using a technique to get the log-transform versions of
them instead, which we explain a bit more later. Finally, we used &lt;code&gt;arrange&lt;/code&gt; to
list the amino acids by evidence against uniform distribution of the codons,
from most evidence against (smallest p-value so most negative log(p-value))
to least evidence against (although still plenty of evidence against) and added
an &lt;code&gt;index&lt;/code&gt; with the ranking for each codon by adding a column with the sequence
of numbers from 1 to the number of rows in the data (&lt;code&gt;n()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_compared %&amp;gt;% 
  filter(n_codons &amp;gt; 1) %&amp;gt;% 
  group_by(AmAcid) %&amp;gt;% 
  mutate(chi_squared = ((Number - expected)^2/expected)) %&amp;gt;% 
  summarise(chi_squared = sum(chi_squared),
            n = n()) %&amp;gt;% 
  mutate(p_value = pchisq(chi_squared, df = n-1, log = TRUE, lower.tail = FALSE)) %&amp;gt;% 
  arrange(p_value) %&amp;gt;% 
  mutate(rank = 1:n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 5
##    AmAcid chi_squared     n p_value  rank
##    &amp;lt;fct&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 Leu        135432.     6 -67700.     1
##  2 Ala         75620.     4 -37805.     2
##  3 Arg         72183.     6 -36076.     3
##  4 Thr         58767.     4 -29378.     4
##  5 Val         58737.     4 -29363.     5
##  6 Ile         56070.     3 -28035.     6
##  7 Gly         52534.     4 -26262.     7
##  8 Pro         45400.     4 -22695.     8
##  9 Ser         36742.     6 -18357.     9
## 10 Asp         16208.     2  -8109.    10
## 11 Phe         13444.     2  -6727.    11
## 12 Asn         11404.     2  -5707.    12
## 13 Gln          9376.     2  -4693.    13
## 14 Lys          6382.     2  -3195.    14
## 15 Glu          5947.     2  -2978.    15
## 16 His          5346.     2  -2678.    16
## 17 Tyr          4738.     2  -2373.    17
## 18 Cys          2958.     2  -1483.    18
## 19 End           928.     3   -464.    19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you may notice, these log transforms of the p-values (which we got rather than untransformed p-values in the &lt;code&gt;pchisq&lt;/code&gt; call because we used the option &lt;code&gt;log = TRUE&lt;/code&gt;) are large in magnitude and negative (so very tiny once you take the exponent if you re-transformed them to p-values) values. If you tried to calculate the untransformed p-values (and we did!), this number is so small (0.00000000e+00) that it is too small for R—it shows up as exactly zero in R, even though it actually is a very tiny, but still non-zero, number. To get around this issue, we told &lt;code&gt;pchisq&lt;/code&gt; to work on these p-values as log transforms, and then we left the p-value as that log-transformed value. A group of numbers that are log transformed will be in the same order as their untransformed versions, so we don’t need to convert back to figure out which amino acid had that smallest p-value. We can just sort the amino acids from most negative to less negative using these log-transformed versions of the p-values. We now have the amino acids ranked from most biased codons (1) to least (19).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to create an exercise solution blog post</title>
      <link>/post/how-to-create-an-exercise-solution-blog-post/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/how-to-create-an-exercise-solution-blog-post/</guid>
      <description>


&lt;p&gt;Each of you will be responsible once or twice over the semester to create
a blog post that provides a clean, clearly-presented solution to the
in-class exercise for the week. This blog post provides the technical
instructions for writing and submitting that exercise.&lt;/p&gt;
&lt;p&gt;Your exercise solution should be posted &lt;strong&gt;before&lt;/strong&gt; the next class meeting.
Since it will need to be reviewed by the faculty before it can be officially
posted, please plan to submit it by the &lt;strong&gt;Tuesday after&lt;/strong&gt; the class for your
exercise. Student assignments for the exercises are given in the
Schedule section of our course website.&lt;/p&gt;
&lt;div id=&#34;overview-of-creating-a-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview of creating a post&lt;/h2&gt;
&lt;p&gt;You will be submitting your exercise solution as a blog post. Creating
one for our website will follow all the same steps as creating a blog
post for a vocabulary list, just with different content. Please read
the &lt;a href=&#34;https://kind-neumann-789611.netlify.com/post/creating-a-vocabulary-list-blog-post/&#34;&gt;post on creating a vocabulary list&lt;/a&gt;
and follow the steps there to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update your fork of the website&lt;/li&gt;
&lt;li&gt;Make a new blog post&lt;/li&gt;
&lt;li&gt;Use RMarkdown syntax to write the blog post&lt;/li&gt;
&lt;li&gt;Submit the blog post&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;content-for-the-blog-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Content for the blog post&lt;/h2&gt;
&lt;p&gt;The blog post should provide a walk-through of the solution to that week’s in-course
exercise. We have posted an example for &lt;a href=&#34;https://kind-neumann-789611.netlify.com/post/exercise-solution-for-chapter-1/&#34;&gt;the exercise for Chapter 1&lt;/a&gt;
to give you an idea of what you should aim to write.&lt;/p&gt;
&lt;p&gt;Generally, this exercise will be a resource for everyone in the class, to make sure
they’ve understood the exercise, as well as to see how someone else tackled the problem.
Your solution should cover all parts of the exercise (for example, if there’s a
part A and B, you should cover both). You can start by writing it as you would if you
were assigned the exercise as a homework problem, but then you should do a second step
of revision to provide some context and dig a bit deeper into how you tackled
the question. Since we are only requiring you to write up exercise answers once
or twice over the semester (rather than submitting homework for exercises every
week), we expect this product to be more in-depth and polished than a typical
homework solution.&lt;/p&gt;
&lt;p&gt;First, make sure that you have provided text explaining what the
exercise asks for, in case the reader hasn’t recently read the exercise prompt.
Second, please add a few details either about how you tackled the problem through code
or how the statistical principles covered in the exercise could apply to other problems
you’ve come across in your research or coursework.&lt;/p&gt;
&lt;p&gt;To help in preparing your post, plan to spend the exercise time in class during the
week of your exercise visiting the different groups of students working on the
exercise. You can talk to them about how they’re approaching the problem, how they
interpret it, etc., to help you develop your own answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tips&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be sure to refresh yourself on all the Markdown formatting tags you can use to improve
the appearance of your post. Be sure to include things like section headings and
italics or bold as appropriate. RStudio’s website has some nice cheatsheets on
RMarkdown that can help.&lt;/li&gt;
&lt;li&gt;Make sure you include R code if appropriate. If you put parentheses around an
assignment expression in R, it will print out the assigned object and make the
assignment in the same call—you might find this useful in writing concise code
while still showing what’s in the objects you create.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;$&lt;/code&gt; and &lt;code&gt;$$&lt;/code&gt; tags in RMarkdown to include mathematical equations in your blog post
when appropriate.&lt;/li&gt;
&lt;li&gt;If you need to read in a dataset for R code in your blog post, save it in the
website directory’s “content/post/example_datasets” subdirectory. If your data
comes from an online source or from an R library, you won’t need to do this,
only if you need a “local” copy of the datafile to run your RMarkdown code.&lt;/li&gt;
&lt;li&gt;You are welcome to draw from (and cite) other statistics textbooks or dictionaries
if you’d like to in explaining the problem and your approach to it.&lt;/li&gt;
&lt;li&gt;For the code, look at vignettes and helpfiles, especially for packages you are not
familiar with.&lt;/li&gt;
&lt;li&gt;For a lot of Bioconductor packages, object-oriented programming is used pretty
heavily. This means that associated data in R packages will often be stored in a
format that you haven’t used yet. Look up more information on data classes used in
your exercise if you aren’t familiar with them. You can use the &lt;code&gt;class&lt;/code&gt; function
to determine the class of an object as well as the name of the package that defines
that class. The &lt;code&gt;str&lt;/code&gt; function is often helpful for exploring a data object class, as well.
Many of the Bioconductor object classes will have special &lt;em&gt;accessor methods&lt;/em&gt;, which are
functions that allow you to extract certain elements from the object—check the helpfile
for the object class, as these methods are often listed there with examples.&lt;/li&gt;
&lt;li&gt;Googling can also be very helpful for learning more about functions, packages, and
datasets in R, especially if you don’t yet know what package the item is from.&lt;/li&gt;
&lt;li&gt;Most Bioconductor packages have very nice vignettes available online and from your
R session once you have installed the package. These are a great place to start to find
out more about how to use the functions and object classes that come with the package.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 1</title>
      <link>/post/exercise-solution-for-chapter-1/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-1/</guid>
      <description>


&lt;p&gt;This exercise asks us to explore the frequency of each of the four nucleotides
(A, C, G, and T) in the genome of &lt;em&gt;C. elegans&lt;/em&gt;, a type of worm used frequently
in scientific research.&lt;/p&gt;
&lt;p&gt;This solution requires that several R extension packages be loaded in your R
session. If you do not have these packages installed to your computer yet, you
should follow &lt;a href=&#34;https://kind-neumann-789611.netlify.com/post/chapter-1-exercise-setup/&#34;&gt;instructions we’ve posted
separately&lt;/a&gt;
describing the required set-up for this exercise. Once you have installed these
packages on your computer, you can load them into your current R session using
the &lt;code&gt;library&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;BSgenome.Celegans.UCSC.ce2&amp;quot;)
library(&amp;quot;Biostrings&amp;quot;)

library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;knitr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part A&lt;/h2&gt;
&lt;p&gt;Part A of the question asks us to explore the nucleotide frequency of the &lt;em&gt;C.
elegans&lt;/em&gt; genome. This genome is available in the &lt;code&gt;Celegans&lt;/code&gt; data that comes with
the &lt;code&gt;BSgenome.Clegans.UCSC.ce2&lt;/code&gt; package and is stored within a &lt;code&gt;BSgenome&lt;/code&gt; class,
which is a special object class provided by the &lt;code&gt;Biostrings&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;There is a dedicated function called &lt;code&gt;letterFrequency&lt;/code&gt; in the &lt;code&gt;Biostrings&lt;/code&gt;
package that can be used to count the frequency of letters in a string (like a
genome) in an R object like this. In a call to this function, you must also
include the possible letters in your “alphabet”—that is, the possible letters
that each position in your string could take.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(nuc_freq &amp;lt;- letterFrequency(Celegans$chrM, letters=c(&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;T&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    A    C    G    T 
## 4335 1225 2055 6179&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To explore and plot this data, I put this summary data into a &lt;code&gt;tibble&lt;/code&gt;, so I
could more easily use &lt;code&gt;tidyverse&lt;/code&gt; tools with the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nuc_freq_df &amp;lt;- tibble(nucleotide = names(nuc_freq), 
             n = nuc_freq)
nuc_freq_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   nucleotide     n
##   &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
## 1 A           4335
## 2 C           1225
## 3 G           2055
## 4 T           6179&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this format, you can use &lt;code&gt;tidyverse&lt;/code&gt; tools to explore the data a bit more.
For example, you can determine the total number of nucleotides in the genome
and, with that calculate the proportion of each nucleotide across the genome.
Along with the &lt;code&gt;kable&lt;/code&gt; function from the &lt;code&gt;knitr&lt;/code&gt; package, I created a formatted
table with this information:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nuc_freq_df %&amp;gt;% 
  mutate(prop = n / sum(n)) %&amp;gt;% 
  kable(digits = 2, 
        caption = &amp;quot;Nucleotide frequencies and proportions in *C. elegans*&amp;quot;,
        col.names = c(&amp;quot;Nucleotide&amp;quot;, &amp;quot;Frequency&amp;quot;, &amp;quot;Proportion&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 1: &lt;/span&gt;Nucleotide frequencies and proportions in &lt;em&gt;C. elegans&lt;/em&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Nucleotide&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Frequency&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Proportion&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4335&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1225&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;G&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;T&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6179&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For some presentations, it might be clearer to present this information in a
slightly different table format, using &lt;code&gt;pivot_longer&lt;/code&gt; and then &lt;code&gt;pivot_wider&lt;/code&gt; to
reformat the table for presentation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nuc_freq_df %&amp;gt;% 
  mutate(prop = n / sum(n),
         n = prettyNum(n, big.mark = &amp;quot;,&amp;quot;),
         prop = prettyNum(prop, digits = 2)) %&amp;gt;% 
  pivot_longer(cols = c(&amp;quot;n&amp;quot;, &amp;quot;prop&amp;quot;)) %&amp;gt;% 
  pivot_wider(names_from = &amp;quot;nucleotide&amp;quot;) %&amp;gt;% 
  mutate(name = case_when(
    name == &amp;quot;n&amp;quot; ~ &amp;quot;Frequency of nucleotide&amp;quot;,
    name == &amp;quot;prop&amp;quot; ~ &amp;quot;Proportion of all nucleotides&amp;quot;
  )) %&amp;gt;%  
  rename(` ` = name) %&amp;gt;% 
  kable(align = c(&amp;quot;rcccc&amp;quot;), 
        caption = &amp;quot;Nucleotide frequencies and proportions in *C. elegans*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 2: &lt;/span&gt;Nucleotide frequencies and proportions in &lt;em&gt;C. elegans&lt;/em&gt;&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;A&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;C&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;G&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;T&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Frequency of nucleotide&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4,335&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1,225&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2,055&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6,179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Proportion of all nucleotides&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.31&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.089&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.15&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here is a plot of the frequency of each of the four nucleotides for the &lt;em&gt;C.
elegans&lt;/em&gt; nucleotide:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(nuc_freq_df, aes(x = nucleotide, y = n)) + 
  geom_col(fill = &amp;quot;lavender&amp;quot;, color = &amp;quot;black&amp;quot;) + 
  theme_classic() + 
  scale_y_continuous(label = scales::comma) + 
  theme(axis.title = element_blank()) + 
  labs(title = expression(paste(italic(&amp;quot;C. elegans&amp;quot;), &amp;quot; neucleotide frequency&amp;quot;)),
       caption = expression(paste(&amp;quot;Based on data from the &amp;quot;, italic(&amp;quot;BSgenome.Celegans.UCSC.ce2&amp;quot;), 
                                  &amp;quot; package.&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-exercise-solution-for-chapter-1_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph uses a few elements to improve its appearance that you might want to
explore if you’re not already familiar with them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;labs&lt;/code&gt; function is used to add both a title and a caption to the plot.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;paste&lt;/code&gt;, &lt;code&gt;expression&lt;/code&gt;, and &lt;code&gt;italic&lt;/code&gt; functions are used together to put “C.
elegans” and an R package name in italics in some of the labels on the plot.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;scales&lt;/code&gt; package is used inside a scale layer for the &lt;code&gt;ggplot2&lt;/code&gt; code to
make the y-axis labels a bit nicer.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;theme&lt;/code&gt; calls are used to apply a simpler overall theme than the default and to
remove the x- and y-axis titles (with &lt;code&gt;element_blank&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;The color and fill of the bars are customized in the geom layer (&lt;code&gt;geom_col&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From this plot, it certainly looks like the nucleotides are &lt;strong&gt;not&lt;/strong&gt; uniformly
distributed in the &lt;em&gt;C. elegans&lt;/em&gt; genome. This question will be investigated more
in the next part of the exercise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part B&lt;/h2&gt;
&lt;p&gt;The second part of the exercise asks us to test whether the observed nucleotide
data for &lt;em&gt;C. elegans&lt;/em&gt; is consistent with the uniform model that all nucleotide
frequencies are the same.&lt;/p&gt;
&lt;p&gt;First, we can simulate several datasets under this null model and see how a plot
of nucleotide frequencies compares to the plot that we obtained with the observed
&lt;em&gt;C. elegans&lt;/em&gt; data. To make these plots, I first simulated 20 samples under the
null model that the distribution is uniform across the four nucleotides, using
the &lt;code&gt;rmultinom&lt;/code&gt; function with the &lt;code&gt;size&lt;/code&gt; argument set to the number of nucleotides in
the original &lt;em&gt;C. elegans&lt;/em&gt; genome data and the &lt;code&gt;prob&lt;/code&gt; argument set to have an equal
probability of each nucleotide at each spot on the genome:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sim_nuc_freq &amp;lt;- rmultinom(n = 20, 
                          size = sum(nuc_freq_df$n), 
                          prob = rep(1 / 4, 4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
## [1,] 3540 3449 3544 3437 3447 3402 3518 3461 3412  3477  3351  3354  3524  3534
## [2,] 3435 3447 3388 3549 3466 3443 3480 3473 3478  3432  3480  3518  3478  3490
## [3,] 3328 3496 3447 3378 3488 3435 3459 3418 3461  3452  3523  3452  3340  3449
## [4,] 3491 3402 3415 3430 3393 3514 3337 3442 3443  3433  3440  3470  3452  3321
##      [,15] [,16] [,17] [,18] [,19] [,20]
## [1,]  3438  3530  3460  3523  3323  3540
## [2,]  3378  3438  3487  3339  3510  3340
## [3,]  3463  3395  3374  3505  3451  3500
## [4,]  3515  3431  3473  3427  3510  3414&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I moved this into a tibble so I could more easily rearrange and plot the data using
facetting in &lt;code&gt;ggplot2&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_nuc_freq_df &amp;lt;- as_tibble(sim_nuc_freq) %&amp;gt;% 
  mutate(nucleotide = c(&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;T&amp;quot;)) %&amp;gt;% 
  pivot_longer(-nucleotide, names_to = &amp;quot;sample&amp;quot;) %&amp;gt;% 
  mutate(sample = sample %&amp;gt;% str_remove(&amp;quot;V&amp;quot;) %&amp;gt;% as.numeric()) %&amp;gt;% 
  arrange(sample, nucleotide)

sim_nuc_freq_df %&amp;gt;% 
  slice(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    nucleotide sample value
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 A               1  3540
##  2 C               1  3435
##  3 G               1  3328
##  4 T               1  3491
##  5 A               2  3449
##  6 C               2  3447
##  7 G               2  3496
##  8 T               2  3402
##  9 A               3  3544
## 10 C               3  3388&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_nuc_freq_df, aes(x = nucleotide, y = value)) + 
  geom_col(fill = &amp;quot;lavender&amp;quot;, color = &amp;quot;black&amp;quot;) + 
  theme_classic() + 
  scale_y_continuous(label = scales::comma) + 
  theme(axis.title = element_blank()) + 
  labs(title = &amp;quot;Simulated neucleotide frequencies under a uniform model&amp;quot;) +
  facet_wrap(~ sample) + 
  expand_limits(y = max(nuc_freq_df$n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-exercise-solution-for-chapter-1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The y-axis limits were expanded here to cover the same range as that shown for the
observed &lt;em&gt;C. elegans&lt;/em&gt; nucleotide frequencies, to help make it easier to compare these plots
with the plot of our observed data. These plots of data simulated under the null model do
show some variation in frequencies among the nucleotides, but it’s certainly much less than
in the observed data for &lt;em&gt;C. elegans&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Next, I repeated this simulation process, but I increased the number of simulations to 1,000:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_nuc_freq_df &amp;lt;- rmultinom(n = 1000, 
                          size = sum(nuc_freq_df$n), 
                          prob = rep(1 / 4, 4)) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  mutate(nucleotide = c(&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;T&amp;quot;)) %&amp;gt;% 
  pivot_longer(-nucleotide, names_to = &amp;quot;sample&amp;quot;) %&amp;gt;% 
  mutate(sample = sample %&amp;gt;% str_remove(&amp;quot;V&amp;quot;) %&amp;gt;% as.numeric()) %&amp;gt;% 
  arrange(sample, nucleotide)

sim_nuc_freq_df %&amp;gt;% 
  slice(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    nucleotide sample value
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 A               1  3444
##  2 C               1  3468
##  3 G               1  3370
##  4 T               1  3512
##  5 A               2  3492
##  6 C               2  3380
##  7 G               2  3460
##  8 T               2  3462
##  9 A               3  3507
## 10 C               3  3443&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using this dataframe of simulations, we can measure the mean, minimum, and maximum frequencies
of each nucleotide across all 1,000 simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sim_summary &amp;lt;- sim_nuc_freq_df %&amp;gt;% 
  group_by(nucleotide) %&amp;gt;% 
  summarize(mean_freq = mean(value),
            min_freq = min(value), 
            max_freq = max(value)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   nucleotide mean_freq min_freq max_freq
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;
## 1 A              3449.     3281     3637
## 2 C              3450.     3307     3589
## 3 G              3447.     3313     3638
## 4 T              3449.     3289     3612&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To help compare this with the observed data, we can create a table with information from
both the original data and the simulations under the null model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nuc_freq_df %&amp;gt;% 
  left_join(sim_summary, by = &amp;quot;nucleotide&amp;quot;) %&amp;gt;% 
  mutate_at(c(&amp;quot;mean_freq&amp;quot;, &amp;quot;min_freq&amp;quot;, &amp;quot;max_freq&amp;quot;, &amp;quot;n&amp;quot;), 
            prettyNum, big.mark = &amp;quot;,&amp;quot;, digits = 0) %&amp;gt;% 
  mutate(simulations = paste0(mean_freq, &amp;quot; (&amp;quot;, min_freq, &amp;quot;, &amp;quot;, max_freq, &amp;quot;)&amp;quot;)) %&amp;gt;% 
  select(nucleotide, n, simulations) %&amp;gt;% 
  kable(col.names = c(&amp;quot;Nucleotide&amp;quot;,              
        &amp;quot;Frequency in C. elegans genome&amp;quot;,
        &amp;quot;Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations&amp;quot;), 
        align = &amp;quot;c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Nucleotide&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Frequency in C. elegans genome&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4,335&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3,449 (3,281, 3,637)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1,225&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3,450 (3,307, 3,589)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;G&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2,055&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3,447 (3,313, 3,638)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;T&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6,179&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3,449 (3,289, 3,612)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This helps clarify how unusual the observed data would be under the null model—the
counts of all four nucleotides in the &lt;em&gt;C. elegans&lt;/em&gt; genome are completely outside the
range of frequencies in the simulated data.&lt;/p&gt;
&lt;p&gt;Another way to look at this is with histograms of the distribution of frequencies
of each nucleotide under the null model compared to the observed frequencies in
the &lt;em&gt;C. elegans&lt;/em&gt; nucleotide:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_nuc_freq_df, aes(x = value)) + 
  geom_histogram(binwidth = 10) + 
  facet_wrap(~ nucleotide) + 
  theme_classic() + 
  scale_x_continuous(name = &amp;quot;Frequency of nucleotide in the simulation under the null model&amp;quot;,
                     labels = scales::comma) + 
  scale_y_continuous(name = &amp;quot;# of simulations (out of 1,000)&amp;quot;) + 
  geom_vline(data = nuc_freq_df, aes(xintercept = n), color = &amp;quot;red&amp;quot;) + 
  labs(title = expression(paste(&amp;quot;Nucleotide frequency in &amp;quot;,
                                italic(&amp;quot;C. elegans&amp;quot;), 
                                &amp;quot; compared null model simulations&amp;quot;)),
       caption = &amp;quot;Red line shows the frequency observed for the nucleotide in C. elegans&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-exercise-solution-for-chapter-1_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, to help in answering this question, it would be interesting to look at a
single measure for each simulation (and for the observed data) rather than comparing
each nucleotide one at a time. Chapter 1 gives the equation for a statistic to
measure variability in multinomial data by calculating the sum of squares for the
differences between the observed and expected count of nucleotides for each of the
four nucleotides in a sample (p. 12).&lt;/p&gt;
&lt;p&gt;I calculated this statistic for the observed data and then for each of the 1,000
simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(obs_stat &amp;lt;- nuc_freq_df %&amp;gt;% 
  mutate(expected = mean(n),
         stat_input = (n - expected) ^ 2 / expected) %&amp;gt;% 
  summarize(variability_stat = sum(stat_input)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   variability_stat
##              &amp;lt;dbl&amp;gt;
## 1            4387.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_stat &amp;lt;- sim_nuc_freq_df %&amp;gt;% 
  mutate(expected = mean(value), 
         stat_input = (value - expected) ^ 2 / expected) %&amp;gt;% 
  group_by(sample) %&amp;gt;% 
  summarize(variability_stat = sum(stat_input))

sim_stat %&amp;gt;% 
  slice(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   sample variability_stat
##    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1      1             3.07
## 2      2             2.00
## 3      3             1.83
## 4      4             8.64
## 5      5             1.56&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a plot of the distribution of this statistic across the 1,000 simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_stat, aes(x = variability_stat)) + 
  geom_rect(data = sim_stat, aes(xmin = quantile(variability_stat, prob = 0.025),
                                 xmax = quantile(variability_stat, prob = 0.975),
                                 ymin = 0, ymax = Inf), 
            fill = &amp;quot;beige&amp;quot;, alpha = 0.5) +
  geom_histogram(bins = 30, fill = &amp;quot;white&amp;quot;, color = &amp;quot;tan&amp;quot;, alpha = 0.5) +
  theme_classic() + 
  labs(title = &amp;quot;Variability from expected values&amp;quot;,
       subtitle = &amp;quot;Values from simulations under the null&amp;quot;,
       x = &amp;quot;Value of variability statistic&amp;quot;, 
       y = &amp;quot;Number of simulations with given value&amp;quot;,
       caption = &amp;quot;The shaded yellow area shows the region of the central 95% of\nstatistic values for the 1,000 simulations under the null model.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-11-exercise-solution-for-chapter-1_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The value of this statistic for the observed nucleotide frequencies for &lt;em&gt;C.
elegans&lt;/em&gt; is 4387, which is much larger (indicating greater variability
from expected values under the null model) than the value observed under most of
the simulations. It is, in fact, far outside the central 95% range of values
observed in simulations.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
