<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>exercises | CSU MSMB Group Study</title>
    <link>/categories/exercises/</link>
      <atom:link href="/categories/exercises/index.xml" rel="self" type="application/rss+xml" />
    <description>exercises</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 13 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>exercises</title>
      <link>/categories/exercises/</link>
    </image>
    
    <item>
      <title>Exercise Solution for Chapter 6</title>
      <link>/post/exercise-solution-for-chapter-6/</link>
      <pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-6/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;chapter-6-exercise-6.4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chapter 6, Exercise 6.4&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Make a less extreme example of correlated test statistics than the data duplication at the end of Section 6.5. Simulate data with true null hypotheses only, and let the data morph from having completely independent replicates (columns) to highly correlated as a function of some continuous-valued control parameter. Check type-I error control (e.g., with the p-value histogram) as a function of this control parameter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We’ll go through a few data clean up and exploration steps first, then we’ll walk through the code related specifically to the exercise a little further down in the post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(purrr)
library(ggbeeswarm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this exercise we use the PlantGrowth dataset from the &lt;code&gt;datasets&lt;/code&gt; package in R. The dataset includes results from an experiment on plant growth comparing yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;PlantGrowth&amp;quot;)
head(PlantGrowth)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   weight group
## 1   4.17  ctrl
## 2   5.58  ctrl
## 3   5.18  ctrl
## 4   6.11  ctrl
## 5   4.50  ctrl
## 6   4.61  ctrl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’re plotting the outcomes of the three groups (&lt;code&gt;ctrl&lt;/code&gt;, &lt;code&gt;trt1&lt;/code&gt;, and &lt;code&gt;trt2&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlantGrowth %&amp;gt;% 
   mutate(group = fct_recode(group, 
                            Control = &amp;quot;ctrl&amp;quot;, 
                            `Treatment 1` = &amp;quot;trt1&amp;quot;, 
                            `Treatment 2` = &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = group, y = weight)) + 
  geom_beeswarm() + 
  labs(x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-13-exercise-solution-for-chapter-6_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Often in research one of the simplest comaparisons of data we can make is between two groups. The t-test is one of the many statistics used in hypothesis testing and can help us determine if the differences between two group means is “significant” or if the differences could have happened by chance. The test statistic is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t=\frac{\bar{m_1}-\bar{m_2}}{\sqrt{s_1^2/N_1 + s_2^2/N_2}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{m_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{m_2}\)&lt;/span&gt; are the sample means in group 1 and
2, respectively, which have &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; observations each, and
&lt;span class=&#34;math inline&#34;&gt;\(s_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_2\)&lt;/span&gt; are the sample variances for each of the two groups.&lt;/p&gt;
&lt;p&gt;Next we’ll apply a t-test comparing weights in the &lt;code&gt;ctrl&lt;/code&gt; group to the &lt;code&gt;trt2&lt;/code&gt; group. We’re testing against the null hypothesis that there is no difference in mean dried plant weights between the two groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlantGrowth %&amp;gt;% 
  filter(group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
    t.test(weight ~ group, data  = .) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  weight by group
## t = -2.134, df = 16.786, p-value = 0.0479
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.98287213 -0.00512787
## sample estimates:
## mean in group ctrl mean in group trt2 
##              5.032              5.526&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the tidy version:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;broom&amp;quot;)
(ttest_orig &amp;lt;- PlantGrowth %&amp;gt;% 
  filter(group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
  t.test(weight ~ group, data  = .) %&amp;gt;% 
    tidy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1   -0.494      5.03      5.53     -2.13  0.0479      16.8   -0.983  -0.00513
## # … with 2 more variables: method &amp;lt;chr&amp;gt;, alternative &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we’ll duplicate the data, adding a second copy of the dataframe, before running the t-test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(ttest_dup &amp;lt;- PlantGrowth %&amp;gt;% 
  bind_rows(PlantGrowth) %&amp;gt;% # Add the duplicate of the dataset
  filter(group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
  t.test(weight ~ group, data  = .) %&amp;gt;% 
  tidy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1   -0.494      5.03      5.53     -3.10 0.00377      35.4   -0.817    -0.171
## # … with 2 more variables: method &amp;lt;chr&amp;gt;, alternative &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the p-value is smaller (now less than 0.05) even though the group means haven’t changed. This is due solely to the increase in sample size.&lt;/p&gt;
&lt;p&gt;However, we’ve violated the t-test assumption of independence. Paired (in this case blatantly duplicated) data is dependent and would be more appropriately tested by a two-sample paired test. A similar example with experimental data would involve taking duplicate measures (technical replicates) on 15 subjects (e.g., leaf, mouse, human) and assuming you now have 30 independent measurements; that would be incorrect. Taking repeated measurements on 15 subjects would leave you with just 15 independent measurments (and the
other 15 fully dependent on those).&lt;/p&gt;
&lt;p&gt;If the data sampled for a t-test violates one or more of the t-test assumptions, such as independence or normal distribution, results can be incorrect and misleading.&lt;/p&gt;
&lt;p&gt;Next we resample only half the data and then duplicated that sampled half to create a datset; so the size of the data set is the same as the original dataset (n = 30), but now half of the observations are fully dependent (exactly the same) as other observations in the dataset.&lt;/p&gt;
&lt;p&gt;The purpose of the &lt;code&gt;set.seed&lt;/code&gt; function is to set the seed of R’s random number generator. We use it below in our simulations so that the results of our sampled data don’t change each time the document is re-rendered and therefore can be reproduced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(4234)

pg1 &amp;lt;- PlantGrowth %&amp;gt;% 
  sample_frac(size = 0.5) %&amp;gt;% 
  bind_rows(., .) 

(pg1_ttest &amp;lt;- pg1 %&amp;gt;% 
  filter(group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
  t.test(weight ~ group, data  = .) %&amp;gt;% 
  tidy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1   -0.492      5.23      5.72     -1.98  0.0722      11.3    -1.04    0.0524
## # … with 2 more variables: method &amp;lt;chr&amp;gt;, alternative &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value increases from 0.004 with a fully duplicated dataset to a value typically much higher with half the data duplicated.&lt;/p&gt;
&lt;p&gt;As discussed in section 6.5 of the text, the t-test function uses asymptotic theory to compute the t-statistic: “this theory states that under the null hypothesis of means in both groups, the statistic follows a known, mathematical distribution, the so called t-distribution with &lt;span class=&#34;math inline&#34;&gt;\(n^1 + n^2\)&lt;/span&gt; degrees of freedom” As we’ve discussed above, the theory also makes the assumption of independence, normality, and equal variance.&lt;/p&gt;
&lt;p&gt;There are often variations from these assumptions in real world data. In our plant growth example, weights are always positive while the normal distribution spans over the entire axis.&lt;/p&gt;
&lt;p&gt;Below we use permutation tests to assess whether the deviation from theoretical assumptions makes a difference. We first use &lt;code&gt;replicate&lt;/code&gt; to
run a lot of t-tests where we test our original data, but with the
&lt;code&gt;group&lt;/code&gt; column randomly re-ordered, so any true relationship between
the group and weight measurements is broken:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pg1_null = with(
  dplyr::filter(pg1, group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)),
    replicate(10000,
      abs(t.test(weight ~ sample(group))$statistic)))

# You can see that this results in a long vector of t statistic
# values, each estimated from a version of the data where we 
# randomized the group labels
head(pg1_null)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         t         t         t         t         t         t 
## 0.5065581 0.9543497 0.5703743 0.5760347 0.5101952 1.6371872&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These should represent the pattern of how t statistics for this
data would be distributed under the null hypothesis that there is no
difference between the average weights of the two groups.
We can plot a histogram of all these t statistic values to visualize
the distribution of the t statistic under the null hypothesis, adding
a line to show the t statistic we observed in the real data (without
shuffling the &lt;code&gt;group&lt;/code&gt; column):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tibble(`|t|` = pg1_null), aes(x = `|t|`)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  geom_vline(xintercept = abs(pg1_ttest$statistic), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-13-exercise-solution-for-chapter-6_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general, a permutation test is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of the observed data points (&lt;a href=&#34;https://wikipedia.org&#34; class=&#34;uri&#34;&gt;https://wikipedia.org&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The test essentially exchanges labels between samples and what they measure; if the connections between label and measurements are broken our t-test should show no connection between the two, demonstrating a true null hypothesis that there is no difference between group means.&lt;/p&gt;
&lt;p&gt;Now we’ll take the dataset with half resampled, and add random noise to each observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

pg2 &amp;lt;- pg1 %&amp;gt;% 
  mutate(noise = rnorm(30, mean = 0, sd = 0.2), 
         weight = weight + noise) %&amp;gt;% 
  select(-noise)
head(pg2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     weight group
## 1 5.908587  trt2
## 2 6.165486  ctrl
## 3 6.526888  trt2
## 4 4.030860  ctrl
## 5 4.775825  trt1
## 6 5.241211  ctrl&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(pg2_ttest &amp;lt;- pg2 %&amp;gt;% 
  filter(group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)) %&amp;gt;% 
  t.test(weight ~ group, data  = .) %&amp;gt;% 
  tidy())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1   -0.562      5.14      5.70     -1.97  0.0767      10.3    -1.20    0.0721
## # … with 2 more variables: method &amp;lt;chr&amp;gt;, alternative &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Permutation test data with half duplicated and random noise added:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pg2_null = with(
  dplyr::filter(pg2, group %in% c(&amp;quot;ctrl&amp;quot;, &amp;quot;trt2&amp;quot;)),
    replicate(10000,
      abs(t.test(weight ~ sample(group))$statistic)))

head(pg2_null)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          t          t          t          t          t          t 
## 0.63521740 1.71201809 0.73541313 0.61678119 0.02298951 1.38779279&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tibble(`|t|` = pg2_null), aes(x = `|t|`)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  geom_vline(xintercept = abs(pg2_ttest$statistic), col = &amp;quot;red&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-13-exercise-solution-for-chapter-6_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the t-test statistic we see in our permutation test looks different compared to our observed data then we can most likely conclude that our data was not generated under the null hypothesis of no significant difference between the treatment groups. We would therefore reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;The table below summarizes the four different types of datasets we’ve explored with t-tests. Important to note is that all of these results are from random samples; the exact p-values and test statistics are variable and if someone were to run a code with a different seed, they would get different results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kableExtra)

data_summary &amp;lt;- bind_rows(ttest_orig, ttest_dup, 
                          pg1_ttest, pg2_ttest) %&amp;gt;% 
  mutate(data = c(&amp;#39;original&amp;#39;, 
                  &amp;#39;doubled without random noise&amp;#39;,
                  &amp;#39;same size as original, half observations replicated&amp;#39;,
                  &amp;#39;half replicated, random noise added to each replicate&amp;#39;), 
         n = c(30, 60, 30 ,30)) %&amp;gt;% 
  select(data, n, statistic, p.value)

data_summary %&amp;gt;% 
   kable(align = c(&amp;quot;l&amp;quot;)) %&amp;gt;% 
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;,
                                      &amp;quot;condensed&amp;quot;)) %&amp;gt;% 
  column_spec(1, bold = T, border_right = T) %&amp;gt;% 
  column_spec(2:3, width = &amp;quot;6em&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
data
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
p.value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;border-right:1px solid;&#34;&gt;
original
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
-2.134021
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0478993
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;border-right:1px solid;&#34;&gt;
doubled without random noise
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
60
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
-3.100660
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0037738
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;border-right:1px solid;&#34;&gt;
same size as original, half observations replicated
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
-1.982154
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0722163
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;border-right:1px solid;&#34;&gt;
half replicated, random noise added to each replicate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
-1.967597
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.0766878
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise Solution for Chapter 11</title>
      <link>/post/exercise-solution-for-chapter-11/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-11/</guid>
      <description>


&lt;p&gt;To begin, we want to load (or download) the packages we will need for this exercise using &lt;code&gt;library&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# BiocManager::install(&amp;quot;EBImage&amp;quot;)
library(EBImage)
# install.packages(&amp;quot;spatstat&amp;quot;)
library(&amp;quot;spatstat&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;extending-the-analysis-in-section-11.17-for-all-cell-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extending the analysis in section 11.17 for all cell types&lt;/h2&gt;
&lt;p&gt;This exercise asks us to analyze an image of a lymph node and evaluate the spatial dependence for all of the cell types in the lymph node. In this case, the null hypothesis is that each of the cell types are evenly distributed (via a homogenous Poisson process) throughout the lymph node. The cell types we will look at are T cells (T_cells), tumor cells (Tumor), dendritic cells (DCs), and other cells (other_cells).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-image-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The image data&lt;/h1&gt;
&lt;p&gt;The image data has already been processed from the original image of a stained lymph node slide to a data type known as marked point process. This example uses the following files from the &lt;em&gt;Modern Statistics for Modern Biology&lt;/em&gt;’s data folder: “99_452SD-DCs.txt”, “99_452SD-other_cells.txt”, “99_452SD-T_cells.txt”, and &#34;“99_452SD-Tumor.txt”.&lt;/p&gt;
&lt;p&gt;Reading in the data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview-of-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview of the data&lt;/h1&gt;
&lt;p&gt;Taking a look at the first few rows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(brcalymphnode)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##       x     y class  
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;  
## 1  6355 10382 T_cells
## 2  6356 10850 T_cells
## 3  6357 11070 T_cells
## 4  6357 11082 T_cells
## 5  6358 10600 T_cells
## 6  6361 10301 T_cells&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many cells are in each cell type?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(brcalymphnode$class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##         DCs other_cells     T_cells       Tumor 
##         878       77081      103681       27822&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualizing the data for each cell type using ggplot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(filter(brcalymphnode, class %in% cellclasses),
       aes(x = x, y = y, col = class)) + geom_point(shape = &amp;quot;.&amp;quot;) +
  facet_wrap( . ~ class) + guides(col = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we are familiar with what the data looks like, we will prepare it to be analyzed using the spatstat package, which allows us to put the data into a “ppp” object. The authors of the book describe the advantage of a “ppp” object by its ability to “capture realizations of a spatial point process,” which is convenient to work with when our data is formatted as individual points in the xy plane.&lt;/p&gt;
&lt;p&gt;The first step in this process is to change the class of our dataframe into a “ppp” object, which is found in the &lt;code&gt;spatstat&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# put our current dataframe into a ppp object named ln
ln = with(brcalymphnode,
          ppp(x = x, y = y, marks = class, xrange = range(x), yrange = range(y)))
# calling the object gives us summary statistics
ln&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Marked planar point pattern: 209462 points
## Multitype, with levels = DCs, other_cells, T_cells, Tumor 
## window: rectangle = [3839, 17276] x [6713, 23006] units&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can create a tighter region to be analyzed by creating a convex hull. This way we are analyzing the shape of the lymph node instead of a rectangle that would be inconsistent with the shape of the image we are interested in analyzing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the book uses the convhulln function to accomplish this, but you can also use the chull function in base R
# the convhulln function is found in the geometry package
library(&amp;quot;geometry&amp;quot;)
coords = cbind(ln$x, ln$y)
chull = convhulln(coords)

# ppp functions need the hull to be defined as closed polygons
# this is the code the book gives to accomplish this
pidx = integer(nrow(chull) + 1)
pidx[1:2] = chull[1, ]
chull[1, ] = NA
for(j in 3:length(pidx)) {
  wh = which(chull == pidx[j-1], arr.ind = TRUE)
  stopifnot(nrow(wh )== 1)
  wh[, &amp;quot;col&amp;quot;] = 3 - wh[, &amp;quot;col&amp;quot;] ## 2-&amp;gt;1, 1-&amp;gt;2
  pidx[j] = chull[wh]
  chull[wh[, &amp;quot;row&amp;quot;], ] = NA
}
pidx = rev(pidx)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the hull by graphing it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the book gives the following code 
ggplot(tibble(x = ln$x, y = ln$y)[pidx, ], aes(x = x, y = y)) +
  geom_point() + geom_path() + coord_fixed()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you compare the shape of the hull to Figure 11.22 in the book, this looks like a better representation of the shape we actually want to analyze.&lt;/p&gt;
&lt;p&gt;Now that we have defined the hull as a closed polygon, we can convert our data into a “ppp” object class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# take brcalymphnode and use its data within the region of the hull and store it in ln
ln = with(brcalymphnode,
          ppp(x = x, y = y, marks = class, poly = coords[ pidx, ],
              check = FALSE))
# calling the object gives a basic summary
ln &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Marked planar point pattern: 209462 points
## Multitype, with levels = DCs, other_cells, T_cells, Tumor 
## window: polygonal boundary
## enclosing rectangle: [3839, 17276] x [6713, 23006] units&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to visualize our object, we can plot it
plot(ln, col = 1:4, pch = 19, cex = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;investigating-spatial-dependence-our-exercise-this-week-from-section-11.17&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Investigating Spatial Dependence (our exercise this week from section 11.17)&lt;/h1&gt;
&lt;p&gt;Now that we have the data in a “ppp” object, we can start to test our null hypothesis that all of the cell types are evenly distributed throughout the lymph node (via a homogenous Poisson process). This will basically allow us to see if there is a spatial dependence in the location of the different cell types. Rejecting our null hypothesis would mean that the cells are spatially dependent on the location of other cells of their type.&lt;/p&gt;
&lt;p&gt;If we want to see if a Poisson distribution is a relatively decent assumption with our data, we can use the &lt;code&gt;Gest&lt;/code&gt; function in the &lt;code&gt;spatstat&lt;/code&gt; package to look at this. Basically this estimates cumulative distribution function of the distance of a random point to its nearest neighbor. The plot will automatically include the CDF for a homogenous Poisson distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(Gest(ln))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While it looks like our data may differ from a homogenous Poisson CDF, the general shape of our data seems to support this distribution is a fairly reasonable assumption.&lt;/p&gt;
&lt;p&gt;If we want to estimate the L function (see equations 11.10 and 11.11 in the book for details), we can use the &lt;code&gt;Linhom&lt;/code&gt; function in the &lt;code&gt;spatstat&lt;/code&gt; package. The basic idea is to see if the distance of between cells of a certain type follow a random pattern (represented by Lpois).&lt;/p&gt;
&lt;p&gt;T cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the L function for T_cells (this may take a minute to run)
Lln_T_cells &amp;lt;- Linhom(subset(ln, marks == &amp;quot;T_cells&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calling the object gives us some descriptions about what each estimate is
Lln_T_cells&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Function value object (class &amp;#39;fv&amp;#39;)
## for the function r -&amp;gt; L[inhom](r)
## ................................................................................
##            Math.label                
## r          r                         
## theo       L[pois](r)                
## border     {hat(L)[inhom]^{bord}}(r) 
## bord.modif {hat(L)[inhom]^{bordm}}(r)
##            Description                                      
## r          distance argument r                              
## theo       theoretical Poisson L[inhom](r)                  
## border     border-corrected estimate of L[inhom](r)         
## bord.modif modified border-corrected estimate of L[inhom](r)
## ................................................................................
## Default plot formula:  .~.x
## where &amp;quot;.&amp;quot; stands for &amp;#39;bord.modif&amp;#39;, &amp;#39;border&amp;#39;, &amp;#39;theo&amp;#39;
## Recommended range of argument r: [0, 694.7]
## Available range of argument r: [0, 694.7]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates via plot
plot(Lln_T_cells, col = c(2, 3, 1), lty = 1, main = &amp;quot;L function estimation for T cells&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distance estimator for the border-corrected estimate and modified border-corrected estimate of the L function for T cells differs quite a bit from our homogenous Poisson; this indicates that the distance between T cells in the lymph node is not evenly distributed.&lt;/p&gt;
&lt;p&gt;Tumor cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the L function for tumor cells
Lln_Tumor &amp;lt;- Linhom(subset(ln, marks == &amp;quot;Tumor&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates via plot
plot(Lln_Tumor, col = c(2, 3, 1), lty = 1, main = &amp;quot;L function estimation for Tumor&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As with the T cells, tumor cells seem to be inhomogenously distributed within the lymph node, based on our L function estimation results.&lt;/p&gt;
&lt;p&gt;Dendritic Cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the L function for DCs
Lln_DCs &amp;lt;- Linhom(subset(ln, marks == &amp;quot;DCs&amp;quot;))
# we can see different types of estimation in DCs
# calling the object gives explanations about which estimates were calculated
Lln_DCs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Function value object (class &amp;#39;fv&amp;#39;)
## for the function r -&amp;gt; L[inhom](r)
## ................................................................................
##            Math.label                
## r          r                         
## theo       {L[inhom]^{pois}}(r)      
## border     {hat(L)[inhom]^{bord}}(r) 
## bord.modif {hat(L)[inhom]^{bordm}}(r)
## trans      {hat(L)[inhom]^{trans}}(r)
## iso        {hat(L)[inhom]^{iso}}(r)  
##            Description                                        
## r          distance argument r                                
## theo       theoretical Poisson L[inhom](r)                    
## border     border-corrected estimate of L[inhom](r)           
## bord.modif modified border-corrected estimate of L[inhom](r)  
## trans      translation-correction estimate of L[inhom](r)     
## iso        Ripley isotropic correction estimate of L[inhom](r)
## ................................................................................
## Default plot formula:  .~.x
## where &amp;quot;.&amp;quot; stands for &amp;#39;iso&amp;#39;, &amp;#39;trans&amp;#39;, &amp;#39;bord.modif&amp;#39;, &amp;#39;border&amp;#39;, &amp;#39;theo&amp;#39;
## Recommended range of argument r: [0, 3359.2]
## Available range of argument r: [0, 3359.2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates via plot
plot(Lln_DCs, col = c(2:5, 1), lty = 1, main = &amp;quot;L function estimation for DCs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For all of the L function estimates for DCs, their distances estimates seem to differ from the homogenous Poisson, indicating that these are also potentially inhomogenously distributed.&lt;/p&gt;
&lt;p&gt;Other cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the L function for other cells (this may take a minute to run)
Lln_Other &amp;lt;- Linhom(subset(ln, marks == &amp;quot;other_cells&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates via plot
plot(Lln_Other, col = c(2, 3, 1), lty = 1, main = &amp;quot;L function estimation for other cells&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For border correction estimates of the L function for other cells, their distance estimates also differ from the homogenous Poisson. This indicates they are also inhomogenously distributed.&lt;/p&gt;
&lt;p&gt;Another way we can analyze our data to see if they are inhomogenously distributed would be to look at the pair correlation function (see equation 11.12) using the &lt;code&gt;pcf&lt;/code&gt; and &lt;code&gt;Kinhom&lt;/code&gt; functions in the &lt;code&gt;spatstat&lt;/code&gt; package. The pair correlation function gives us information about the density of our data points from a reference point. For the homogenous Poisson, the pair correlation function equals 1. Anything less than one suggests inhibition of those points being close, and anything greater than one suggests the points are clustered.&lt;/p&gt;
&lt;p&gt;T cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the pcf for T cells (takes a minute to run)
pcf_Tcells &amp;lt;- pcf(Kinhom(subset(ln, marks == &amp;quot;T_cells&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates by plotting
plot(pcf_Tcells, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# zooming in to first 10 points
plot(pcf_Tcells, lty = 1, xlim = c(0, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because the pcf for T cells stays well above 1 for most values of &lt;em&gt;r&lt;/em&gt;, T cells appear to cluster together in the lymph node, indicating a distribution that differs from a homogenous Poisson process. For &lt;em&gt;r&lt;/em&gt; &amp;lt; 2, it makes sense that there would be some inhibition, as cells take up a certain volume that would inhibit them from being that close together.&lt;/p&gt;
&lt;p&gt;Tumor cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the pcf for Tumor
pcf_Tumor &amp;lt;- pcf(Kinhom(subset(ln, marks == &amp;quot;Tumor&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates by plotting
plot(pcf_Tumor, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# zooming in to first 20 points
plot(pcf_Tumor, lty = 1, xlim = c(0, 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At most values for &lt;em&gt;r&lt;/em&gt;, the pcf &amp;gt; 1, indicating that tumor cells also tend to cluster together. Interestingly, the &lt;em&gt;r&lt;/em&gt; values for which the pcf &amp;lt; 1 for tumor cells is greater than we saw with T cells, which may indicate that these cells may be larger in volume than T cells (causing their inhhibition from being closer together than a random Poisson process) or that there is something else that would inhibit them from being as dense for those &lt;em&gt;r&lt;/em&gt; values compared to T cells.&lt;/p&gt;
&lt;p&gt;Dendritic Cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the pcf for DCs
pcf_DCs &amp;lt;- pcf(Kinhom(subset(ln, marks == &amp;quot;DCs&amp;quot;)))
# visualizing the estimates by plotting
plot(pcf_DCs, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# zooming in to first 1000 points
plot(pcf_DCs, lty = 1, xlim = c(0, 1000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The values for the pcf for DCs are much greater than we saw for T cells or tumor cells, indicating that there is more clustering at certain values of &lt;em&gt;r&lt;/em&gt; in this cell type than the others. They do eventually seem to settle close to a homogenous Poisson process after &lt;em&gt;r&lt;/em&gt; gets relatively large. It may be worth noting that &lt;em&gt;r&lt;/em&gt; has a much greater range for DCs than the other cell types.&lt;/p&gt;
&lt;p&gt;Other cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimating the pcf for other cells (takes a minute to run)
pcf_Other &amp;lt;- pcf(Kinhom(subset(ln, marks == &amp;quot;other_cells&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of data points exceeds 1000 - computing border correction estimate only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualizing the estimates by plotting
plot(pcf_Other, lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# zooming in to first 10 points
plot(pcf_Other, lty = 1, xlim = c(0, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-11_files/figure-html/unnamed-chunk-19-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Values for the pcf of ‘other’ cells overall seem to be less than other cell types, indicating that these may be clustering but to a lesser extent than other cell types. The range of values for &lt;em&gt;r&lt;/em&gt; in which the pcf &amp;lt; 1 are in between what we saw for T cells and tumor cells, indicating that the ‘other’ cells may have volumes in between that of the T cells and the tumor cells that could explain an inhibition in density at those points. It appears as though other cells are also inhomogenously distributed and do not follow a homogenous Poisson process.&lt;/p&gt;
&lt;p&gt;Overall, with all cell types, we saw some levels of spatial dependence based on our L function and pcf analyses. For most distances, or values of &lt;em&gt;r&lt;/em&gt;, we saw differences in the distance and density distributions when comparing a homogenous Poisson distribution to the distribution of each cell type. Depending on a specific amount we consider to be significantly different than our null hypothesis, we may reject our null hypothesis that different cell types are homogenously distributed throughout the lymph node.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 8, Part 1</title>
      <link>/post/ex-8-1/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/ex-8-1/</guid>
      <description>


&lt;div id=&#34;exercise-8.1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exercise 8.1&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Do the analyses of Section 8.5 with the &lt;code&gt;edgeR&lt;/code&gt; package and compare the results: make a scatterplot of the
log 10 p-values, pick some genes where there are large differences,
and visualize the raw data to see what is going on. Based on this can you explain the differences?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most of the following code is taken straight from the book in section 8.5 for data cleaning/wrangling and the &lt;code&gt;DESeq2&lt;/code&gt; analysis. The steps performed in &lt;code&gt;edgeR&lt;/code&gt; are quite similar but we do see some differences that we will get to towards the end.&lt;/p&gt;
&lt;p&gt;First, we load our libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pasilla)
library(edgeR)
library(dplyr)
library(DESeq2)
library(ggplot2)
library(pheatmap)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the same data used in the &lt;code&gt;DESeq2&lt;/code&gt; examples from the section 8.5.&lt;/p&gt;
&lt;p&gt;Load the example data using the &lt;code&gt;system.file()&lt;/code&gt; call for R data stored as part of a R package.
We then convert our data to a matrix object called &lt;code&gt;counts&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fn = system.file(&amp;quot;extdata&amp;quot;, &amp;quot;pasilla_gene_counts.tsv&amp;quot;,
                  package = &amp;quot;pasilla&amp;quot;, mustWork = TRUE)
counts = as.matrix(read.csv(fn, sep = &amp;quot;\t&amp;quot;, row.names = &amp;quot;gene_id&amp;quot;))

head(counts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
## FBgn0000018        583        761        245        310      722      299
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## FBgn0000018      308&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The matrix tallies the number of reads seen for each gene in each sample. We call it the count table. It has 14599 rows, corresponding to the genes, and 7 columns, corresponding to the samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(counts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14599     7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;edger&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;edgeR&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now begins the analysis with the &lt;code&gt;edgeR&lt;/code&gt; package. To do this, we follow the vignette for the package that is a downloadable .pdf file that you can get in your Rstudio session &lt;code&gt;vignette(&#34;edgeR&#34;)&lt;/code&gt; or online with &lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&#34; class=&#34;uri&#34;&gt;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we make the &lt;code&gt;group&lt;/code&gt; object which is a vector with values representing the treatment status of each of the 7 samples, where 1 refers to the untreated group, and 2 refers to the treated group. With this &lt;code&gt;group&lt;/code&gt; object we can make a &lt;code&gt;DGEList()&lt;/code&gt;, from the &lt;code&gt;edgeR&lt;/code&gt; package, with our count data and the groups we just made. A &lt;code&gt;DGEList()&lt;/code&gt; object is very similar to any traditional R list object and can be manipulated like any other list in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group &amp;lt;- factor(c(1,1,1,1,2,2,2))
x &amp;lt;- DGEList(counts=counts, group=group)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RNA-seq provides a measure of the relative abundance of each gene in each RNA
sample, but does not provide any measure of the total RNA output on a per-cell basis. In other words, RNA-seq measures relative expression rather than absolute expression. This can become an issue in RNA-seq when a small number of highly expressed genes consume a substantial proportion of the total library for a sample causing under sampling of the other expressed genes.&lt;br /&gt;
To help combat this we turn to normalization. &lt;code&gt;calcNormFactors&lt;/code&gt; normalizes by finding a set of scaling factors for the library sizes that minimizes the log-fold changes between the samples for most genes. Using a trimmed mean of M-values (TMM) between each pair of samples. If we receive a &lt;code&gt;norm.factors&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\lt\)&lt;/span&gt; 1 that means a small number of high count genes are monopolizing the sequencing reducing the counts of other genes. Conversely, a &lt;code&gt;norm.factors&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\gt\)&lt;/span&gt; 1 scales up the library size, analogous to downscaling the counts. This normalization can help account for things like varying sequencing depth, length of genes, and RNA composition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;normalization &amp;lt;- calcNormFactors(x)
normalization&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## An object of class &amp;quot;DGEList&amp;quot;
## $counts
##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## 14594 more rows ...
## 
## $samples
##            group lib.size norm.factors
## untreated1     1 13972512    0.9995731
## untreated2     1 21911438    1.0081519
## untreated3     1  8358426    0.9843974
## untreated4     1  9841335    0.9525077
## treated1       2 18670279    1.0651817
## treated2       2  9571826    0.9957012
## treated3       2 10343856    0.9978557&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;model.matrix()&lt;/code&gt; function creates a design matrix which is a matrix of values of &lt;strong&gt;explanatory variables&lt;/strong&gt; of a set of objects. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The purpose of this conversion is to prepare the data in a manner that facilitates regression-like modelling (ex. &lt;code&gt;glm&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;design &amp;lt;- model.matrix(~group)
head(design)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      0
## 4           1      0
## 5           1      1
## 6           1      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;estimateDisp()&lt;/code&gt; function, “Maximizes the negative binomial likelihood to give the estimate of the common, trended and tagwise dispersions across all tags.” We have to use this negative binomial (aka gamma-Poisson) model since our experiments vary from replicate to replicate more than the traditional Poisson can account for. This variance can be due to seemingly miniscule experimental conditions such as, temperature of cell culture, pipettor calibration, etc. In the case of the gamma-Poisson we have two inputs for variance and mean instead of just having &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for both variance and mean in the normal Poisson.
An important consideration that the &lt;code&gt;edgeR&lt;/code&gt; package has taken into account is the fact that RNA-seq and other Next Generation Sequencing projects are extremely expensive and generally have few samples. Accounting for dispersion with a small number of samples can be challenging and the &lt;code&gt;edgeR&lt;/code&gt; package tackles this conundrum using a qCML method. The qCML method calculates the likelihood by conditioning on the total counts for each tag, and uses pseudo counts after adjusting for library sizes. Given a table of counts or a &lt;code&gt;DGEList&lt;/code&gt; object, the qCML common dispersion and tagwise dispersions can be estimated using the &lt;code&gt;estimateDisp()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;design_matrix &amp;lt;- estimateDisp(normalization, design)
design_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## An object of class &amp;quot;DGEList&amp;quot;
## $counts
##             untreated1 untreated2 untreated3 untreated4 treated1 treated2
## FBgn0000003          0          0          0          0        0        0
## FBgn0000008         92        161         76         70      140       88
## FBgn0000014          5          1          0          0        4        0
## FBgn0000015          0          2          1          2        1        0
## FBgn0000017       4664       8714       3564       3150     6205     3072
##             treated3
## FBgn0000003        1
## FBgn0000008       70
## FBgn0000014        0
## FBgn0000015        0
## FBgn0000017     3334
## 14594 more rows ...
## 
## $samples
##            group lib.size norm.factors
## untreated1     1 13972512    0.9995731
## untreated2     1 21911438    1.0081519
## untreated3     1  8358426    0.9843974
## untreated4     1  9841335    0.9525077
## treated1       2 18670279    1.0651817
## treated2       2  9571826    0.9957012
## treated3       2 10343856    0.9978557
## 
## $design
##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      0
## 4           1      0
## 5           1      1
## 6           1      1
## 7           1      1
## attr(,&amp;quot;assign&amp;quot;)
## [1] 0 1
## attr(,&amp;quot;contrasts&amp;quot;)
## attr(,&amp;quot;contrasts&amp;quot;)$group
## [1] &amp;quot;contr.treatment&amp;quot;
## 
## 
## $common.dispersion
## [1] 0.0228685
## 
## $trended.dispersion
## [1] 0.12060195 0.04196786 0.11986264 0.12040360 0.01632837
## 14594 more elements ...
## 
## $tagwise.dispersion
## [1] 0.12060195 0.02742256 0.70902130 0.09430132 0.01321566
## 14594 more elements ...
## 
## $AveLogCPM
## [1] -2.636763  2.953356 -1.966526 -2.223010  8.454625
## 14594 more elements ...
## 
## $trend.method
## [1] &amp;quot;locfit&amp;quot;
## 
## $prior.df
## [1] 5.886884
## 
## $prior.n
## [1] 1.177377
## 
## $span
## [1] 0.3013916&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our &lt;code&gt;design_matrix&lt;/code&gt; we can begin fitting modified linear models to it. Here we use a quasi-likelihood negative binomial generalized log-linear model, which is a mouth full. “Quasi-likelihood estimation is one way of allowing for overdispersion, that is, greater variability in the data than would be expected from the statistical model used.” Since we have already stated that we will have variation in our experiments, possibly due to the most minute factors, this issue of overdispersion is apparent. Instead of using traditional probability functions, a variance function is used (variance as a function of the mean) and allows for an overdispersion parameter input which is used to “fix” the variance function to resemble that of an existing probability function (ex. Poisson).&lt;/p&gt;
&lt;p&gt;The goal of this fit is to identify genes where the intensity level (gene expression level) is notably different between our treated and untreated groups.
Running the &lt;code&gt;glmQLF...()&lt;/code&gt; functions gives the null model against which the full model is compared. Tags can then be ranked in order of evidence for differential expression, based on the p-value computed for each tag.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glmQLFit(design_matrix, design)
qlf &amp;lt;- glmQLFTest(fit, coef=2)
edgeRoutput &amp;lt;- topTags(qlf)

edgeRoutput&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Coefficient:  group2 
##                 logFC   logCPM        F       PValue          FDR
## FBgn0039155 -4.601317 5.882317 953.1722 1.646470e-14 2.403682e-10
## FBgn0025111  2.905756 6.923428 714.2877 1.257310e-13 9.177735e-10
## FBgn0035085 -2.548257 5.684922 452.3866 3.068717e-12 1.311741e-08
## FBgn0003360 -3.173036 8.452776 442.2207 3.594058e-12 1.311741e-08
## FBgn0029167 -2.188103 8.221274 412.0926 5.866109e-12 1.404698e-08
## FBgn0039827 -4.142255 4.397963 408.8548 6.195926e-12 1.404698e-08
## FBgn0034736 -3.492036 4.186934 403.9614 6.735313e-12 1.404698e-08
## FBgn0029896 -2.434452 5.305827 336.3777 2.386477e-11 4.355023e-08
## FBgn0000071  2.685868 4.795202 288.1793 6.900283e-11 1.119303e-07
## FBgn0034434 -3.624878 3.214994 282.6144 7.884818e-11 1.151105e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(as.data.frame(qlf))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   logFC    logCPM          F     PValue
## FBgn0000003  1.90105753 -2.636763 1.40991069 0.25938432
## FBgn0000008  0.01020453  2.953356 0.00282599 0.95833875
## FBgn0000014 -0.21077864 -1.966526 0.03020978 0.86444776
## FBgn0000015 -1.61118380 -2.223010 1.65428857 0.21877991
## FBgn0000017 -0.23044399  8.454625 3.99686462 0.06492692
## FBgn0000018 -0.09673451  5.088412 0.53070377 0.47805393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our outputs can be summarized by looking at our &lt;code&gt;logFC&lt;/code&gt; column (log fold change) where the higher the number the higher the count of that particular gene was read during the sequencing run. Secondly our &lt;code&gt;PValue&lt;/code&gt;, if it meets threshold (typically pvalue &amp;lt;= 0.05) allows the rejection of our null hypothesis, which is, there is equal expression regardless of what gene you look at.&lt;/p&gt;
&lt;p&gt;Now we want to visualize the data points after their regression fits. We must tidy up the data sets a bit to apply some &lt;code&gt;tidyverse&lt;/code&gt; magic. First the data is converted to a data frame, using &lt;code&gt;as.data.frame()&lt;/code&gt;, then we use the &lt;code&gt;rownames_to_column()&lt;/code&gt; function which sounds like its name, and pulls the row names, in this case &lt;code&gt;gene_id&lt;/code&gt;, into a new column of the dataframe. Lastly, we subset the data using the &lt;code&gt;select()&lt;/code&gt; function for only the columns we want, and order the data using the &lt;code&gt;arrange()&lt;/code&gt; function to start with the largest values with respect to the &lt;code&gt;padj&lt;/code&gt; via the &lt;code&gt;desc()&lt;/code&gt; argument inside &lt;code&gt;arrange()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_pasilla &amp;lt;- pasilla %&amp;gt;% 
  results() %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  rownames_to_column(var = &amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  select(gene_id, pvalue, padj) %&amp;gt;% 
  arrange(desc(padj))

tidy_edgeR &amp;lt;- qlf %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  rownames_to_column(var = &amp;quot;gene_id&amp;quot;) %&amp;gt;%
  select(gene_id, PValue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;full_join()&lt;/code&gt; from &lt;code&gt;dplyr&lt;/code&gt; package we are able to subset these two data frames based on the &lt;code&gt;gene_id&lt;/code&gt; column and keeping all other matching columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_join(tidy_pasilla, tidy_edgeR, 
          &amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = pvalue, y = PValue)) +
  labs(x = &amp;quot;DESeq2 pvalue&amp;quot;, y = &amp;quot;edgeR PValue&amp;quot;) +
  scale_x_log10() +
  scale_y_log10() +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = &amp;quot;blue&amp;quot;) +
  geom_vline(xintercept = 1e-25, color = &amp;quot;red&amp;quot;) +
  ggtitle(&amp;quot;DESeq2 vs EdgeR&amp;quot;, subtitle = &amp;quot;Each point represents a single gene, p-value is for whether 
          the gene has differential expression between groups&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-30-exercise-solution-for-chapter-8_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reference line drawn in blue here using &lt;code&gt;geom_abline()&lt;/code&gt; is to show what the data would look like if the two methods were identical.
Looking at the graph above, we see that most of the data are falling relatively close to one another when our pvalue &lt;span class=&#34;math inline&#34;&gt;\(\gt\)&lt;/span&gt; 1e-25. We know this because the alpha of the &lt;code&gt;ggplot&lt;/code&gt; object is set to 0.5, so if we see a black dot, it means there are two points, one on top of each other. This is what we would expect considering these packages &lt;code&gt;DESeq2&lt;/code&gt; and &lt;code&gt;edgeR&lt;/code&gt; have the same purpose in mind and is why we are comparing their outputs in this exercise!&lt;/p&gt;
&lt;p&gt;Below we subset the data again, this time selecting those points with pvalues &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; 1e-25 (log10 transform). When plotting these we don’t see much overlapping, supporting the variation between the &lt;code&gt;edgeR&lt;/code&gt; and &lt;code&gt;DESeq2&lt;/code&gt; modes of analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_join(tidy_pasilla, tidy_edgeR,&amp;quot;gene_id&amp;quot;) %&amp;gt;% 
  filter(pvalue &amp;lt;= 1e-25) %&amp;gt;%
  ggplot(aes(x = pvalue, y = PValue)) +
  labs(x = &amp;quot;DESeq2 pvalue&amp;quot;, y = &amp;quot;edgeR PValue&amp;quot;) +
  geom_point(alpha = 0.5) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle(&amp;quot;Genes with DESeq pvalue &amp;lt;= -25&amp;quot;, subtitle = &amp;quot;Each point represents a single gene, p-value is for whether 
          the gene has differential expression between groups&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-30-exercise-solution-for-chapter-8_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&#34; class=&#34;uri&#34;&gt;https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Quasi-likelihood&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Quasi-likelihood&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/Chap-CountData.html&#34; class=&#34;uri&#34;&gt;http://web.stanford.edu/class/bios221/book/Chap-CountData.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&#34; class=&#34;uri&#34;&gt;https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://geanders.github.io/RProgrammingForResearch/&#34; class=&#34;uri&#34;&gt;https://geanders.github.io/RProgrammingForResearch/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 9</title>
      <link>/post/exercise-solution-for-chapter-9/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-9/</guid>
      <description>


&lt;div id=&#34;exercise-9.2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 9.2&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;“Correspondence Analysis on color association tables:
Here is an example of data collected by looking at the number of Google hits resulting from queries of pairs of words. The numbers in Table 9.4 &lt;em&gt;[not reproduced]&lt;/em&gt; are to be multiplied by 1000. For instance, the combination of the words “quiet” and “blue” returned 2,150,000 hits. Perform a correspondence analysis of these data. What do you notice when you look at the two-dimensional biplot?&#34;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this exercise, we will essentially be repeating the correspondence analysis process seen in section 9.4.2, this time using associations between color and sentiment terms in search engine queries, rather than hair and eye color. Rather than testing whether certain hair/eye combinations are more likely to co-occur, we will be exploring whether a given color is more or less likely to be associated with certain sentiments (e.g. would we expect “orange” and “happy” to co-occur more frequently than would be expected by chance?). The same general steps can be repeated without many changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-loading-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 1: Loading libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ade4)
library(sva)
library(tidyverse)
library(factoextra)
library(janitor) #optional; function `clean_names()` makes column names easier to work with
library(ggplot2)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-two-ways-to-load-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Step 2: Two ways to load data:&lt;/h1&gt;
&lt;p&gt;At least working from the online version of the text, there are two ways to obtain (roughly) equal data for this exercise. One option is to copy table 9.4 directly from the book into Excel, shift the header one cell to the right to align columns with their proper names, export a .csv (&lt;code&gt;ex_9.2_color_table.csv&lt;/code&gt; in this example), and load it into R using a command like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix &amp;lt;- read_csv(&amp;quot;example_datasets/ex_9_2_color_table.csv&amp;quot;, col_names = TRUE) %&amp;gt;%
  column_to_rownames(var = &amp;#39;X1&amp;#39;) %&amp;gt;%
  as.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something to note is that this data is rounded to units of thousands (e.g. “19” is ~19,000), while the course data in the downloadable &lt;code&gt;data&lt;/code&gt; file gives more-precise values. I don’t know that this would disrupt any major correlations, but it could cause some minor discrepancies on comparison.&lt;/p&gt;
&lt;p&gt;Alternatively, the file is included in the course data as &lt;code&gt;colorsentiment.csv&lt;/code&gt;, although in a different, three-column format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_csv(&amp;quot;example_datasets/colorsentiment.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   `&amp;lt;X&amp;gt;` = col_character(),
##   `&amp;lt;Y&amp;gt;` = col_character(),
##   Results = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   `&amp;lt;X&amp;gt;` `&amp;lt;Y&amp;gt;`     Results
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 blue  happy     8310000
## 2 blue  depressed  957000
## 3 blue  lively    1250000
## 4 blue  clever    1270000
## 5 blue  perplexed   71300
## 6 blue  virtuous    80200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be converted to match our correspondence table format using the &lt;code&gt;pivot_wider&lt;/code&gt; function and other &lt;code&gt;tidyverse&lt;/code&gt; formatting tools:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix &amp;lt;- read_csv(&amp;quot;example_datasets/colorsentiment.csv&amp;quot;) %&amp;gt;%
  janitor::clean_names() %&amp;gt;% #standardizes column name format
  arrange(desc(results)) %&amp;gt;% # Rearranging to match row/col order in table 9.4
  pivot_wider(names_from = x, values_from = results) %&amp;gt;% # converts colors from column entries (`x`) to column names
  column_to_rownames(var = &amp;#39;y&amp;#39;) 

color_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              black   white   green    blue  orange  purple    grey
## happy     19300000 9150000 8730000 8310000 4220000 2610000 1920000
## angry      2970000 1730000 1740000 1530000 1040000  710000  752000
## quiet      2770000 2510000 2140000 2150000 1220000  821000  875000
## lively     1840000 1480000 1350000 1250000  621000  488000  659000
## clever     1650000 1420000 1320000 1270000  693000  416000  495000
## depressed  1480000 1270000  983000  957000  330000  102000  147000
## virtuous    179000  165000  102000   80200   24700   17300   20000
## perplexed   110000  109000   80100   71300   23300   15200   18900&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correspondence-analysis-following-section-9.4.2-as-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correspondence Analysis (following section 9.4.2 as a model)&lt;/h1&gt;
&lt;p&gt;Setting up the correspondence analysis object using the correspondence analysis &lt;code&gt;dudi&lt;/code&gt; function from the &lt;code&gt;ade4&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix_ca &amp;lt;- dudi.coa(color_matrix, n = 2, scannf = FALSE) # scannf = FALSE stops automatic printing of eigenvalues&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a special “&lt;em&gt;Du&lt;/em&gt;ality &lt;em&gt;Di&lt;/em&gt;agram” list object (used by the &lt;code&gt;ade4&lt;/code&gt; package for correspondence analysis, but also principal component analysis and other methods) containing a variety of data generated by the analysis; the call &lt;code&gt;?dudi()&lt;/code&gt; will give a list of what each component contains (axis weights, point coordinates, etc.).Stored features include as the base data table (&lt;code&gt;tab&lt;/code&gt;), a vector of eigenvalues (&lt;code&gt;eig&lt;/code&gt;), and a variety of information on row and column data (e.g. weights, coordinates, and principal components).&lt;/p&gt;
&lt;p&gt;Question 9.2 specifies that we use two dimensions, but visualizing the eigenvalues with the &lt;code&gt;factoextra&lt;/code&gt; package confirms that this is a good representation of the system, with almost all variance being explained by the first two dimensions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_eig(color_matrix_ca, geom = &amp;#39;bar&amp;#39;) # visualizing eigenvalues &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Following the book’s example in 9.4.2, we can explicitly calculate a residual matrix, which shows the difference between expected (assuming random distribution) and observed values for given row/column intercepts, in the following steps. This doesn’t feed into visualizations, but it may be helpful to have a quantitative reference for residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rowsums_colors &amp;lt;- as.matrix(apply(color_matrix, 1, sum))
colsums_colors &amp;lt;- as.matrix(apply(color_matrix, 2, sum))
expected_colors &amp;lt;- rowsums_colors %*% t(colsums_colors) / sum(colsums_colors) # using matrix multiplication to see what 
# &amp;quot;average&amp;quot; values should look like if row and column sums are distributed evenly across the dataset

#sum((color_matrix - expected_colors)^2 / expected_colors)

# subtracting the &amp;quot;expected&amp;quot; matrix from the observed  values to see discrepancies

(residual_table_colors &amp;lt;- color_matrix - expected_colors %&amp;gt;% 

    t() %&amp;gt;% 

    round())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              black    white    green     blue   orange  purple    grey
## happy      2604538  7252731  6644019  7090159  3616948 2332753 1890798
## angry     -6856953   -19511  -241131   891748   657779  448415  620320
## quiet     -6291637   848427  1103422  1745469   859372  639948  797493
## lively    -6766161   610622   693006   868322 -1000836  381433  587529
## clever    -2852964   868979   700121  -965911  -261613  317732  427122
## depressed -1374026   750108 -1383422  -359058  -550269    8671  111484
## virtuous  -2513797 -3678280 -1290876 -1133364  -811323  -31532   -2510
## perplexed -3113357 -2153156 -1204300 -1081265  -414128  -15750   -2339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we can see that, for instance, the combination of “happy” and “black” in searches occurs significantly more often (~26,000,000 additional instances) than we’d expect given no correlations between colors and sentiments, while e.g. “happy” and “grey” is much rarer.&lt;/p&gt;
&lt;p&gt;To take these patterns more intuitively, we can use a mosaic plot to visualize the proportional distribution of color/sentiment terms in searches (e.g. “back” and “happy” are both popular terms, so could be expected to occur more frequently than e.g. “perplexed” and “purple” in any case), as well as color coding for residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mosaicplot(t(color_matrix / 2000), las = 2, shade = TRUE, type = &amp;#39;pearson&amp;#39;) # dividing values by 2,000 because the mosaic plot function doesn&amp;#39;t seem to auto-scale colors, meaning that the unaltered matrix is all &amp;quot;flattened&amp;quot; to the &amp;lt;4 or &amp;gt;4 category.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  #transposing is just aesthetic; seems easier to follow with sentiments on the y axis as far as labels and visual row/column continuity. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this pattern, it’s easier to see broad patterns and associations among colors and sentiments. For instance, if we focus on colors, we can see &lt;code&gt;black&lt;/code&gt; is quite distinct from most colors, with more than expected with &lt;code&gt;happy&lt;/code&gt; and lower for other sentiments. All other sentiments behave more like each other than &lt;code&gt;black&lt;/code&gt;, but do show a smaller division between &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, and &lt;code&gt;green&lt;/code&gt;; this is distinguished from &lt;code&gt;orange&lt;/code&gt;, &lt;code&gt;purple&lt;/code&gt;, and &lt;code&gt;grey&lt;/code&gt; by being rarer than expected (under random distribution) with &lt;code&gt;angry&lt;/code&gt; and more common with &lt;code&gt;depressed&lt;/code&gt;, &lt;code&gt;virtuous&lt;/code&gt;, and &lt;code&gt;perplexed&lt;/code&gt;. Based on this, in a 2D projection we might expect to see the largest separation between &lt;code&gt;black&lt;/code&gt; and all other colors, with a smaller but obvious distinction between the two other color clusters. Because &lt;code&gt;quiet&lt;/code&gt;, &lt;code&gt;lively&lt;/code&gt;, and &lt;code&gt;clever&lt;/code&gt; are more common than expected for everything but black, the will likely be about equidistant between these clusters.&lt;/p&gt;
&lt;p&gt;To check how close we got with these eyeballed estimates, we can use the &lt;code&gt;factoextra&lt;/code&gt; biplot visualization function &lt;code&gt;fviz_ca_biplot&lt;/code&gt; for correspondence analysis to see our biplot for sentiment and color searches. I thought using the option to represent one as vector arrow, rather than points, also improves legibility (e.g. the relationship between &lt;code&gt;angry&lt;/code&gt; and &lt;code&gt;orange&lt;/code&gt;/&lt;code&gt;purple&lt;/code&gt; become more obvious):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_ca_biplot(color_matrix_ca, arrows = c(FALSE, TRUE), repel = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/fviz%20biplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, sentiment and color groups show similar relationships from what we might expect comparing patterns of positive/negative residuals in the mosaic plot. However, this makes it easier to see some patterns, such as the strong opposition between &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;grey&lt;/code&gt; on dimension 1, or the fact that most of dimension 2 is due to the differences of &lt;code&gt;green-white-blue&lt;/code&gt; and &lt;code&gt;perplexed-depressed-virtuous&lt;/code&gt; from the rest of the data. We can also make out some smaller trends that weren’t obvious (at least to me) from the mosaic visualization, like &lt;code&gt;grey&lt;/code&gt; being more distinct from &lt;code&gt;orange&lt;/code&gt; and &lt;code&gt;purple&lt;/code&gt; than we could make out with the mosaic plot’s effective “resolution”. This separation appears to be driven by higher co-occurrence with &lt;code&gt;lively&lt;/code&gt;, &lt;code&gt;quiet&lt;/code&gt;, and &lt;code&gt;clever&lt;/code&gt;. Looking back to our mosaic plot, the latter three have lighter shades of blue in &lt;code&gt;orange&lt;/code&gt; and &lt;code&gt;purple&lt;/code&gt;, with no obvious difference with &lt;code&gt;angry&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-directly-with-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting directly with &lt;code&gt;ggplot&lt;/code&gt;:&lt;/h1&gt;
&lt;p&gt;While &lt;code&gt;factoextra&lt;/code&gt; uses custom functions to streamline the process, it’s possible to approximate the same visualization using &lt;code&gt;ggplot&lt;/code&gt; and components of the &lt;code&gt;dudi&lt;/code&gt; object. In the &lt;code&gt;color_matrix_ca&lt;/code&gt; object, the ‘row’ and ‘column’ factor coordinates (emotion and color, respectively) are stored at &lt;code&gt;.$li&lt;/code&gt; and &lt;code&gt;.$co&lt;/code&gt;, This allows direct plotting; you could also look at e.g. normalized scores in &lt;code&gt;.$l1&lt;/code&gt; and &lt;code&gt;.$c1&lt;/code&gt; as well. I was able to get a general sense of how the &lt;code&gt;factoextra&lt;/code&gt; authors approached this using the call &lt;code&gt;View(fviz_ca_biplot)&lt;/code&gt; to pull up the function’s R code; this ultimately pointed to the more fundamental &lt;code&gt;fviz&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Using this information, we can render single plots for color:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Single plots: (roughly equivalent to default output for `fviz_ca_col` and `fviz_ca_row`)

color_nmds &amp;lt;- color_matrix_ca$co %&amp;gt;%
  ggplot() + 
  aes(x = Comp1, y = Comp2) +
  geom_point(color = &amp;#39;blue&amp;#39;) +
  geom_text(label = rownames(color_matrix_ca$co), nudge_y = 0.01, color = &amp;#39;blue&amp;#39;) + 
  coord_fixed()

color_nmds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/ggplot_single_plots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and sentiment (&lt;em&gt;below&lt;/em&gt;). We can also use the &lt;code&gt;geom_segment&lt;/code&gt; function in &lt;code&gt;ggplot&lt;/code&gt; to replicate the arrows seen in the &lt;code&gt;factoExtra&lt;/code&gt; biplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_nmds &amp;lt;- color_matrix_ca$li %&amp;gt;%
  ggplot() + 
  aes(x = Axis1, y = Axis2) +
  geom_point(color = &amp;#39;red&amp;#39;) +
  ggrepel::geom_text_repel(label = rownames(color_matrix_ca$li), nudge_y = 0.01, color = &amp;#39;red&amp;#39;) +
  geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2), 

               arrow = arrow(length = unit(0.3,&amp;quot;cm&amp;quot;)), 

               color = &amp;quot;red&amp;quot;) +
  coord_fixed()
  

emotion_nmds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/emotion_nmds_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can combine these elements into a biplot by combining the above dataframes, with one column for each value, and plotting the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Biplot (there are probably more efficient/correct approaches)

#manually joining the two datasets using a common index (generates a partial row of NAs, with 8 sentiments and 7 colors)

color_component &amp;lt;- color_matrix_ca$co %&amp;gt;%
  rownames_to_column(var = &amp;quot;color&amp;quot;) %&amp;gt;%
  rownames_to_column(var = &amp;quot;index&amp;quot;)

emotion_component &amp;lt;- color_matrix_ca$li %&amp;gt;%
  rownames_to_column(var = &amp;quot;emotion&amp;quot;) %&amp;gt;%
  rownames_to_column(var = &amp;quot;index&amp;quot;)

biplot_composite &amp;lt;- color_component %&amp;gt;%
  full_join(emotion_component, by = &amp;quot;index&amp;quot;)

(biplot_composite)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   index  color       Comp1       Comp2   emotion       Axis1       Axis2
## 1     1  black  0.17794592  0.04039331     happy  0.11532145  0.01602901
## 2     2  white -0.05317953 -0.10399481     angry -0.10756934  0.09837664
## 3     3  green -0.03347037 -0.03237667     quiet -0.20048082 -0.01564279
## 4     4   blue -0.03552655 -0.04588027    lively -0.20105512  0.01174266
## 5     5 orange -0.09284092  0.07047109    clever -0.17886743 -0.03423825
## 6     6 purple -0.15049601  0.15422498 depressed  0.03935532 -0.24782949
## 7     7   grey -0.36826914  0.10335528  virtuous  0.05572888 -0.24684976
## 8     8   &amp;lt;NA&amp;gt;          NA          NA perplexed -0.04792928 -0.22173448&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;biplot_composite_plot &amp;lt;- biplot_composite %&amp;gt;%
  ggplot() +
  aes(x = Comp1, y = Comp2) +
  geom_point(color = &amp;#39;red&amp;#39;) +
  geom_text(label = biplot_composite$color, nudge_y = 0.01, color = &amp;#39;red&amp;#39;) +
   geom_segment(aes(x = 0, y = 0, xend = Comp1, yend = Comp2), 

               arrow = arrow(length = unit(0.3,&amp;quot;cm&amp;quot;)), 

               color = &amp;quot;red&amp;quot;) + 
  geom_point(aes(x = Axis1, y = Axis2), color = &amp;#39;blue&amp;#39;) +
  geom_text_repel(aes(x = Axis1, y = Axis2), label = biplot_composite$emotion, nudge_y = 0.01, color = &amp;#39;blue&amp;#39;) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  labs(x = paste0(&amp;quot;Dim1(&amp;quot;,round(color_matrix_ca$eig[1]/sum(color_matrix_ca$eig)*100, 1),&amp;quot;%)&amp;quot;),
       y = paste0(&amp;quot;Dim2(&amp;quot;,round(color_matrix_ca$eig[2]/sum(color_matrix_ca$eig)*100, 1),&amp;quot;%)&amp;quot;) +
       coord_fixed()
       ) 

biplot_composite_plot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_text).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/biplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Second Approach&lt;/h1&gt;
&lt;p&gt;Dr. Anderson also worked out a more-efficient approach making use of &lt;code&gt;purrr&lt;/code&gt; package function, &lt;code&gt;unclass&lt;/code&gt;, &lt;code&gt;map_at&lt;/code&gt; and other tools to unite the two &lt;code&gt;coa / dudi&lt;/code&gt; objects with fewer intermediate steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_matrix_ca %&amp;gt;% 
  # `unclass` to work with this as a regular list
  unclass() %&amp;gt;% 
  # `keep` lets you keep just some elements of a list. We&amp;#39;ll keep &amp;quot;co&amp;quot; and &amp;quot;li&amp;quot; elements
  keep(names(.) %in% c(&amp;quot;co&amp;quot;, &amp;quot;li&amp;quot;)) %&amp;gt;% 
  # both of these have important info in their rownames, so move those into a column.
  # `map` allows you to do the same thing to every element of the list (now just &amp;quot;co&amp;quot; and &amp;quot;li&amp;quot;
  map(rownames_to_column) %&amp;gt;% 
  # `map_at` lets you do something to *just* some elements of a list. So here, to be able
  # to bind the two dataframe elements in the list into one dataframe, you need to 
  # make sure they have the same column names. Currently, they don&amp;#39;t, so we need to 
  # change the column names for one of them.
  map_at(&amp;quot;co&amp;quot;, ~ rename(.x, Axis1 = Comp1, Axis2 = Comp2)) %&amp;gt;% 
  # Now that we have a list where each element is a dataframe with the same number
  # of columns, and where those columns have the same names and data types, you 
  # can use `bind_rows` to stick them together into one dataframe (it turns out that
  # this is a *super* helpful function). After this step, you have a tidy dataframe. The
  # `.id` parameter is adding a column with the original list element name, so you can 
  # tell which rows originally came from &amp;quot;co&amp;quot; and which from &amp;quot;li&amp;quot;
  bind_rows(.id = &amp;quot;id&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = Axis1, y = Axis2, color = id)) + 
  geom_point() + 
  # You can add arrows to your segments with the `arrow` function (from the
  # `grid` package, which is very old school graphics and what ggplot is built on)
  # To have everything come from the center, you set the starting point to 0 for
  # both x- and y-axes
  geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2),
               arrow = arrow(length = unit(0.1, &amp;quot;inches&amp;quot;))) + 
  geom_text_repel(aes(label = rowname)) + 
  # `coord_fixed` ensures that the x- and y-axes are scaled so they have the 
  # same unit size
  coord_fixed() + 
  # `str_c` lets you stick character strings together (`paste0` would also work here)
  labs(x = str_c(&amp;quot;Dim 1 (&amp;quot;, 
                 round(100 * color_matrix_ca$eig[1] / sum(color_matrix_ca$eig), 1), 
                 &amp;quot;%)&amp;quot;), 
       y = str_c(&amp;quot;Dim 2 (&amp;quot;, 
                 round(100 * color_matrix_ca$eig[2] / sum(color_matrix_ca$eig), 1), 
                 &amp;quot;%)&amp;quot;)) + 
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) + 
  # Get rid of the color legend
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  # Add some reference lines for 0 on the x- and y-axes
  geom_hline(aes(yintercept = 0), linetype = 3) + 
  geom_vline(aes(xintercept = 0), linetype = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-16-chapter-9-exercise-solution_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 7</title>
      <link>/post/exercise-solution-for-chapter-7/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-7/</guid>
      <description>


&lt;div id=&#34;exercise-7.4-from-modern-statistics-for-modern-biology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 7.4 from Modern Statistics for Modern Biology&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Let’s revisit the Hiiragi data and compare the weighted and unweighted approaches. 7.4a Make a correlation circle for the unweighted Hiiragi data &lt;code&gt;xwt&lt;/code&gt;. Which genes have the best projections on the first principal plane (best approximation)? 7.4b Make a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;read-in-and-clean-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in and clean data&lt;/h2&gt;
&lt;p&gt;We start by loading the libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Hiiragi2013&amp;quot;)
library(&amp;quot;ade4&amp;quot;)
library(&amp;quot;factoextra&amp;quot;)
library(&amp;quot;pander&amp;quot;)
library(&amp;quot;knitr&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;ggrepel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can install the libraries using &lt;code&gt;install.packages&lt;/code&gt; for the &lt;code&gt;ade4&lt;/code&gt; and &lt;code&gt;factoextra&lt;/code&gt; packages and such, and the following line for &lt;code&gt;Hiiragi2013&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BiocManager::install(&amp;quot;Hiiragi2013&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will analyze data from the &lt;code&gt;Hiiragi2013&lt;/code&gt; package containing a gene expression microarray dataset describing the transcriptomes of about 100 cells from mouse embryos at different times during early development.&lt;/p&gt;
&lt;p&gt;I copied the code from the textbook to clean the data. A tidyverse version of this code is available on
&lt;a href=&#34;https://github.com/geanders/csu_msmb_practice/blob/master/ch_7_examples.pdf&#34;&gt;Brooke Andersons’s github&lt;/a&gt;. In either case, we select the wildtype (WT) samples. Then we select the 100 features with the largest variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;x&amp;quot;, package = &amp;quot;Hiiragi2013&amp;quot;)
xwt &amp;lt;- x[, x$genotype == &amp;quot;WT&amp;quot;]
sel &amp;lt;- order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt &amp;lt;- xwt[sel, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting data is of the &lt;code&gt;ExpressionSet&lt;/code&gt; class, a class designed to combine many types of data such as microarray data, metadata, and protocol information. The data is shown below. I transposed the data so the rows correspond to samples and the first 101 columns correspond to different genes. I only showed the first the columns and then skipped to the end for brevity. The last columns give additional data about each sample, such as the total number of cells and the scan date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(head(as.data.frame(xwt)[,c(1:3,102:108)]))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1434584_a_at&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1437325_x_at&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;X1420085_at&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Embryonic.day&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Total.number.of.cells&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;lineage&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genotype&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ScanDate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sampleGroup&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sampleColour&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.24888&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.332223&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.027715&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.98757&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.475742&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.293017&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.72695&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.955642&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.940142&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.51926&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.061255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.715243&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.01299&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.308667&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.924228&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6 E3.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.50750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.202948&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.325952&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;WT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2011-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E3.25&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;#CAB2D6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;a-make-a-correlation-circle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;a) Make a correlation circle&lt;/h2&gt;
&lt;p&gt;Next we plot a correlation circle. This allows us to plot the original genes projected onto the first two principal axes. We can interpret the angles between the vectors as a measure of the correlation between the two corresponding genes. The length of the arrows indicates the correlation of a gene with the first principal axes. This allows us to visualize the correlation between genes, as well as see which genes are best described by our first two principal components. The first principal component is the linear combination of the genes with maximum variance. The second principal component is the linear combination of the genes with maximum variance while being orthogonal to the first principal component.&lt;/p&gt;
&lt;p&gt;First I used &lt;code&gt;dudi.pca&lt;/code&gt; to perform principal component analysis and get a pca and dudi class object. In the text book, they performed a weighted PCA because the groups had very different sample sizes ranging from 4 to 36. A weighted analysis would give the groups equal weight, while an unweighted would give each observation equal weight (and thus give less weight to groups with less members). We are interested in the difference in the genes at the various developmental phases, so in general it would make sense to do a weighted analysis in order to let each developmental phase group have an equal say. To compare the weighted with an unweighted PCA I didn’t use the &lt;code&gt;row.w&lt;/code&gt; option as the textbook did in order to fit an unweighted PCA.&lt;/p&gt;
&lt;p&gt;In the following code &lt;code&gt;t&lt;/code&gt; takes the transpose so the rows correspond to samples. The &lt;code&gt;exprs&lt;/code&gt; function is used to access the expression and error measurements of the data. I chose to center and scale the data, meaning make each column have a mean of 0 and a variance of 1. The &lt;code&gt;scannf&lt;/code&gt; option prevents the function from plotting a scree plot and &lt;code&gt;nf&lt;/code&gt; tells it to keep two axes (i.e. 2 principal components).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcaMouse &amp;lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
                            center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I used the &lt;code&gt;fviz_pca_var&lt;/code&gt; function to plot the correlation circle. I used &lt;code&gt;geom= &#34;arrow&#34;&lt;/code&gt; to remove the labels of each arrow as they were all overlapping. The default for this argument is to print both arrows and text labels for each line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fviz_pca_var(pcaMouse, col.circle = &amp;quot;black&amp;quot;,geom= &amp;quot;arrow&amp;quot;) + ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare that plot to the one below showing the correlation circle from a weighted analysis. We can see in the weighted analysis the first principal plane does a better job of explaining the variation as indicated by the percent of variation explained by each dimension as well as the length of the arrows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab = table(xwt$sampleGroup)
xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])
pcaMouseWeighted &amp;lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
                    row.w = xwt$weight,
                    center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)
fviz_pca_var(pcaMouseWeighted, col.circle = &amp;quot;black&amp;quot;,geom= &amp;quot;arrow&amp;quot;) + ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The genes that have the best projections on the first principal plane (meaning those with the strongest correlation with the principal axes) are those with the longest arrows on the plots above. I extracted the genes with arrow lengths greater than 0.8 from the unweighted PCA plot with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrCircle &amp;lt;- fviz_pca_var(pcaMouse, col.circle = &amp;quot;black&amp;quot;)$data
arrowLengths &amp;lt;- sqrt(corrCircle$x^2+corrCircle$y^2)
cutoff &amp;lt;- 0.8
kpInd &amp;lt;- order(arrowLengths, decreasing=TRUE)[1:sum(arrowLengths&amp;gt;cutoff)]
genes &amp;lt;- corrCircle[kpInd,&amp;quot;name&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;1456270_s_at&lt;/em&gt;, &lt;em&gt;1450624_at&lt;/em&gt;, &lt;em&gt;1449134_s_at&lt;/em&gt;, &lt;em&gt;1418153_at&lt;/em&gt;, &lt;em&gt;1420085_at&lt;/em&gt;, &lt;em&gt;1420086_x_at&lt;/em&gt;, &lt;em&gt;1434584_a_at&lt;/em&gt;, &lt;em&gt;1450843_a_at&lt;/em&gt;, &lt;em&gt;1429483_at&lt;/em&gt;, &lt;em&gt;1437308_s_at&lt;/em&gt;, &lt;em&gt;1456598_at&lt;/em&gt;, &lt;em&gt;1460605_at&lt;/em&gt;, &lt;em&gt;1429388_at&lt;/em&gt;, &lt;em&gt;1426990_at&lt;/em&gt;, &lt;em&gt;1436392_s_at&lt;/em&gt; and &lt;em&gt;1452270_s_at&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To summarize, the code above saves output from the &lt;code&gt;fviz_pca_var&lt;/code&gt; function to an object called &lt;code&gt;corrCircle&lt;/code&gt;. I used the output (which included x and y coordinates) to calculate the arrow length for each gene. I then created a vector, &lt;code&gt;kpInd&lt;/code&gt;, with the indice number for genes that had a length greater then the cutoff of 0.8. Finally, I output the names of those genes with length great than 0.8. Below is a plot showing just these genes with the ``best projection,&#34; meaning they are most correlated with the plane of maximum variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrCircle %&amp;gt;% 
  mutate(length = sqrt(x^2 + y^2)) %&amp;gt;% 
  filter(length &amp;gt;= 0.8) %&amp;gt;% 
  ggplot(aes(x = 0, xend = x, y = 0, yend = y)) + 
  geom_segment(arrow = arrow(length = unit(0.1, &amp;quot;inches&amp;quot;))) + 
  geom_label_repel(aes(x = x, y = y, label = name), 
                   size = 2, alpha = 0.7) + 
  coord_fixed()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-make-a-biplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;b) Make a biplot&lt;/h2&gt;
&lt;p&gt;Next we make a biplot to visualize samples and the genes in one plot. I used the &lt;code&gt;fviz_pca_biplot&lt;/code&gt; function. The &lt;code&gt;col.var&lt;/code&gt; and &lt;code&gt;col.ind&lt;/code&gt; options allow you to color the different genes and sample points, respectively, by particular groups. I used &lt;code&gt;label=&#34;&#34;&lt;/code&gt; to remove the labels on the vectors and points. The &lt;code&gt;above8&lt;/code&gt; variable is simply a vector of either “Less than 0.8” or “Greater than 0.8” to indicate if each gene’s vector length was less than or greater than our cutoff value on the correlation circle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;above8 &amp;lt;- rep(&amp;quot;Less than 0.8&amp;quot;, dim(xwt)[1])
above8[1:100 %in% kpInd] &amp;lt;- &amp;quot;Greater than 0.8&amp;quot;
fviz_pca_biplot(pcaMouse, col.var=above8, col.ind=xwt$sampleGroup, label=&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The colors/shapes of the point indicate the sample group of each sample point. The colors of the arrows indicate whether or not the corresponding gene had length greater than or less than 0.8 on the correlation circle.&lt;/p&gt;
&lt;p&gt;We can see the EPI and PE groups (the groups with the fewest samples) appear more extreme in the first principle plane when using the unweighted PCA. This was not the case in the weighted PCA (shown below) because then groups were weighted equally. Here, the PCA mostly depends on the larger groups (to best understand the most observations), so the smaller groups appear more extreme on the principal plane.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;above8 &amp;lt;- rep(&amp;quot;Less than 0.8&amp;quot;, dim(xwt)[1])
above8[1:100 %in% kpInd] &amp;lt;- &amp;quot;Greater than 0.8&amp;quot;
fviz_pca_biplot(pcaMouseWeighted, col.var=above8, col.ind=xwt$sampleGroup, label=&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-04-09-exercise-solution-for-chapter-7_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise Solution for 5.1</title>
      <link>/post/exercise-solution-for-5-1/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-5-1/</guid>
      <description>


&lt;p&gt;This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;silhouette index&lt;/strong&gt; is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\displaystyle S(i) = \frac{B(i) - A(i)}{max_i(A(i), B(i))} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The book explains the equation by first defining that the average dissimilarity of a point &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to a cluster &lt;span class=&#34;math inline&#34;&gt;\(C_k\)&lt;/span&gt; is the average of the distances from &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to all of the points in &lt;span class=&#34;math inline&#34;&gt;\(C_k\)&lt;/span&gt;. From this, let &lt;span class=&#34;math inline&#34;&gt;\(A(i)\)&lt;/span&gt; be the average dissimlarity of all points in the cluster that &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; belongs to, and &lt;span class=&#34;math inline&#34;&gt;\(B(i)\)&lt;/span&gt; is the lowest average of dissimlarity of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; to any other cluster of which &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is NOT a member.Basically, we are subtracting the mean distance to other instances in the same cluster from the mean distance to the instances of the next closest cluster, and dividing it by which of the two values is larger. The output is a coefficient that will vary between -1 and 1, where a value closer to 1 implies that the instance is closest to the correct cluster.&lt;/p&gt;
&lt;p&gt;The solution to this exercise requires the following R packages to be loaded into your environment.&lt;/p&gt;
&lt;div id=&#34;required-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Libraries&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cluster)
library(dplyr)
library(ggplot2)
library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-a&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part A&lt;/h2&gt;
&lt;p&gt;Question 5.1.a asks us to compute the silhouette index for the &lt;code&gt;simdat&lt;/code&gt; data that was simulated in Section &lt;strong&gt;5.7&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The provided code is used to simulate data coming from four separate groups. They use the pipe operator to concatenate four different, randomly generated, data sets. The &lt;code&gt;ggplot2&lt;/code&gt; package is used to take a look at the data as a barchart of the within-groups sum of squared distances (WSS) obtained from the &lt;em&gt;k&lt;/em&gt; means method.&lt;/p&gt;
&lt;p&gt;First off, we need to set the seed to ensure reproducible results with a randomly generated data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following chunk of code utilizes the &lt;code&gt;lapply&lt;/code&gt; function two times to generate a datset with four distinct clusters. The &lt;code&gt;lapply&lt;/code&gt; function comes from base R, and is most often used to apply a function over an entire list or vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)


simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

simdatxy = simdat[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)] # data without class label&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The technique the authors used to generate a clustered dataset is tricky. The &lt;code&gt;lapply&lt;/code&gt; within an &lt;code&gt;lapply&lt;/code&gt;, paired with two &lt;code&gt;bind_rows&lt;/code&gt; functions can be confusing. The next sections are included to demonstrate what the data looks like through various steps in this process, and help bring understanding to the reader how the code is working.&lt;/p&gt;
&lt;div id=&#34;the-inner-lapply-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The inner &lt;code&gt;lapply&lt;/code&gt; function&lt;/h4&gt;
&lt;p&gt;The first &lt;code&gt;lapply&lt;/code&gt; is generating a vector of n = 100 normally distributed random numbers, and creating two separate dataframes packed into a list that consist of the mean (&lt;code&gt;my&lt;/code&gt;) and standard deviation, respectively. Each individual value is specifically assigned a 0 or an 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simdatmy = lapply(c(0,8), function(my) {
    
    tibble(y = rnorm(100, mean = my, sd = 2),
           class = paste(my, sep = &amp;quot;:&amp;quot;))
   })
summary(simdatmy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Length Class  Mode
## [1,] 2      tbl_df list
## [2,] 2      tbl_df list&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-outer-lapply-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The outer &lt;code&gt;lapply&lt;/code&gt; function&lt;/h4&gt;
&lt;p&gt;The second (outer) &lt;code&gt;lapply&lt;/code&gt; uses the same idea to apply the same, random, 0 or 8 assignment to values in the &lt;code&gt;mx&lt;/code&gt; function. The ouput is now four separate dataframes within a list that contain all of the &lt;code&gt;mx&lt;/code&gt; data and all of the &lt;code&gt;my&lt;/code&gt; data. Within the &lt;code&gt;tibble&lt;/code&gt; function, they include the code &lt;code&gt;class =&lt;/code&gt; to ensure that each row in each of the 4 the lists is assigned one of the four possible two-way combinations of 0 and 8. This is important to simulate a clustered dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simdatmx = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
    })})
summary(simdatmx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Length Class  Mode
## [1,] 2      -none- list
## [2,] 2      -none- list&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-together&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Putting it together&lt;/h4&gt;
&lt;p&gt;The last step is to bind the list of dataframes into one single dataframe. The final dataframe includes all of the x and y data, each with assigned classes, defined by a combination of 0 and 8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)


simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

head(simdat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##        x       y class
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 -1.25  -1.24   0:0  
## 2  0.367  0.0842 0:0  
## 3 -1.67  -1.82   0:0  
## 4  3.19   0.316  0:0  
## 5  0.659 -1.31   0:0  
## 6 -1.64   3.53   0:0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(simdat$class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;0:0&amp;quot; &amp;quot;0:8&amp;quot; &amp;quot;8:0&amp;quot; &amp;quot;8:8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;simdat&lt;/code&gt; dataframe includes 400 random points witih an assigned class to simulate clustering. We can look at the data using a simple &lt;code&gt;ggplot&lt;/code&gt; scatterplot, color coded by the class of each point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(simdat, aes(x = x, y = y, color = class)) + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next part of exploring the data is to compute the within-groups sum of squares (WSS) for the clusters that we just generated. The goal of this section is to observe how the WSS changes as the number of clusters is increased from 1 to 8 when using the k-means. Chapter 5 provides us with the following code and graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1
wss = tibble(k = 1:8, value = NA_real_)

#2
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)

#3
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}

ggplot(wss, aes(x = k, y = value)) + geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is really going on here?&lt;/p&gt;
&lt;p&gt;This first chunk is setting up a one-column dataframe with blank &lt;code&gt;NA&lt;/code&gt; values. The &lt;code&gt;NA&lt;/code&gt;s will be filled in with values as the rest of the code processes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1
wss = tibble(k = 1:8, value = NA_real_)
wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k value
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1    NA
## 2     2    NA
## 3     3    NA
## 4     4    NA
## 5     5    NA
## 6     6    NA
## 7     7    NA
## 8     8    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second chunk of code is calculating the value for k = 1 individually. They use the &lt;code&gt;scale&lt;/code&gt; function to scale down the value for k = 1 because it is so much larger than the rest of the k-values. Without scaling down the k = 1 value, it would be difficult to observe any sharp decreases that might indicate a “potential sweet spot” for the number of clusters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k  value
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1 15781.
## 2     2    NA 
## 3     3    NA 
## 4     4    NA 
## 5     5    NA 
## 6     6    NA 
## 7     7    NA 
## 8     8    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last part of this chunk is running a k-means clustering on the remaining k 2 through 8 and then pulling out the &lt;code&gt;withinss&lt;/code&gt; value for all of the observations, summing it, and assigning that value to each individual k-value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}

km$withinss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 275.2533 165.2368 199.8292 257.9090 285.8292 131.9047 239.8457 339.1308&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##       k  value
##   &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     1 15781.
## 2     2  9055.
## 3     3  5683.
## 4     4  3088.
## 5     5  2755.
## 6     6  2441.
## 7     7  2152.
## 8     8  1895.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These corresponding values are then neatly displayed in a barchart of the WSS stastistic as a function of k. The sharp decrease between k = 3 and k = 4 (at the &lt;em&gt;elbow&lt;/em&gt;) is indicative of the number of clusters present in the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(wss, aes(x = k, y = value)) + geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-the-silhouette-index-for-simdat&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Computing the silhouette index for &lt;code&gt;simdat&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Next up is the code necessary to plot the silhouette index. The &lt;code&gt;silhouette&lt;/code&gt; function comes from the &lt;code&gt;cluster&lt;/code&gt; package, and the resulting graph provides an average silhouette width for k = 4 clusters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4, border = NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pam&lt;/code&gt; (partitioning around medoids) function is doing the same thing as the &lt;code&gt;kmeans&lt;/code&gt; call from the earlier chunk of code, but using the &lt;code&gt;cluster&lt;/code&gt; package’s algorithm to calculate the k-means clustering. We use the &lt;code&gt;pam&lt;/code&gt; function here because the we need the “pam” and “partition” output class to run the &lt;code&gt;silhouette&lt;/code&gt; function. With this information, we can then compute the silhouette index and view the output summary and plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pam4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;pam&amp;quot;       &amp;quot;partition&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sil = silhouette(pam4, 4, border = NA)
summary(sil)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Silhouette of 400 units in 4 clusters from pam(x = simdatxy, k = 4) :
##  Cluster sizes and average silhouette widths:
##       103       100        99        98 
## 0.5279715 0.5047895 0.4815427 0.4785642 
## Individual silhouette widths:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.04754  0.41232  0.54916  0.49858  0.63554  0.71440&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we explore the final output plot, it might be interesting to look at plots of the simulated values with their respective cluster assignments based on &lt;code&gt;pam&lt;/code&gt; k-means clustering and the silhouette index. With some (a lot of) help from Brooke, we have the following code to view this.&lt;/p&gt;
&lt;p&gt;For the most part, all of the points were assigned to the same cluster as the original, with the occational border point mis-assigned to the neighboring cluster. Interestingly, the silhouette index approaches zero when you near the border of of the cluster and is much higher near the center of the cluster. Although we would expect this, it can be helpful to view this graphically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sil %&amp;gt;% 
  unclass() %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  tibble::rownames_to_column(var = &amp;quot;orig_order&amp;quot;) %&amp;gt;% 
  arrange(as.numeric(orig_order)) %&amp;gt;% 
  bind_cols(simdat) %&amp;gt;% 
  ggplot(aes(x = x, y = y, shape = as.factor(cluster), color = sil_width)) + 
  geom_point() + 
  facet_wrap(~ class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, a silhouette plot with the n = 400 data points. The average silhouette width is a metric that we can use to summarize everything at a level of the full clustering process. Essentially, the closer that this average is to 0.5, then the more accurate our number of clusters &lt;em&gt;k&lt;/em&gt; is. This concept is further explored in the &lt;strong&gt;Part B&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sil, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part B&lt;/h2&gt;
&lt;p&gt;Question 5.1.b asks us to change the number of clusters &lt;em&gt;k&lt;/em&gt; and assess which &lt;em&gt;k&lt;/em&gt; value produces the best silhouette index.&lt;/p&gt;
&lt;p&gt;In this example, there are a couple of ways to assess which k gives the best silhouette index.One method would be trial and error and determining which k-value produces the highest silhouette index. This method works out for this example, but is impractical for much larger and complex datasets. Included below is the code for testing multiple different k-values and the resulting coefficient values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam2 = pam(simdatxy, 2)
sil2 = silhouette(pam2, 2)
plot(sil2, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam3 = pam(simdatxy, 3)
sil3 = silhouette(pam3, 3)
plot(sil3, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4)
plot(sil, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam12 = pam(simdatxy, 12)
sil12 = silhouette(pam12, 12)
plot(sil12, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam40 = pam(simdatxy, 40)
sil40 = silhouette(pam40, 40)
plot(sil40, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-18-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This trial and error method indicates that the highest silhouette index (that was tested) is achieved with k = 4.&lt;/p&gt;
&lt;p&gt;A different (seemingly more appropriate) method is to write a piece of code that will test a range of k-values automatically. This next piece of code is adapted from Amy Fox and the group that she worked with during class. This is a much more practical method that provides a clear answer of which &lt;em&gt;k&lt;/em&gt; gives the best silhouette index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- c(2:10)
df_test &amp;lt;- data.frame()
for (i in 2:10){
  
  pam_run &amp;lt;- pam(simdatxy, i)
  sil_run &amp;lt;- silhouette(pam_run, i)
  
  row_to_add &amp;lt;- data.frame(i, width = summary(sil_run)$avg.width)
  
  df_test &amp;lt;- rbind(df_test, row_to_add)
}
df_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    i     width
## 1  2 0.4067400
## 2  3 0.4000560
## 3  4 0.4985801
## 4  5 0.4401518
## 5  6 0.3957347
## 6  7 0.3717875
## 7  8 0.3699929
## 8  9 0.3670770
## 9 10 0.3516570&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_test, aes(i, width)) +
  geom_point() +
  geom_line() +
  xlab(&amp;quot;k&amp;quot;) +
  ylab(&amp;quot;Silhouette Index&amp;quot;) +
  ggtitle(&amp;quot;Testing different k values for Silhouette Index&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sil_run)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Silhouette of 400 units in 10 clusters from pam(x = simdatxy, k = i) :
##  Cluster sizes and average silhouette widths:
##        63        38        40        52        33        40        35        33 
## 0.3885059 0.3273800 0.3622990 0.3703291 0.3573781 0.3257945 0.4429236 0.2807700 
##        31        35 
## 0.3944945 0.2335738 
## Individual silhouette widths:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.1778  0.2389  0.3703  0.3517  0.4946  0.6623&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of &lt;code&gt;summary(sil_run)&lt;/code&gt; matches the trial and error method, but in a more efficient manner.&lt;/p&gt;
&lt;p&gt;In summary, k = 4 provides us with the best silhouette index value. This is because there truly are four groups in the dataset based on how we created it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part C&lt;/h2&gt;
&lt;p&gt;The last part of this exercise asks us to repeat by calculating the silhouette index on a uniform (unclustered) data distribution over a range of values.&lt;/p&gt;
&lt;p&gt;Here, a new data set is generated without clustering the randomly generated data. The 0 and 8 assignment values have been removed and replaced with a singular 1. This assigns all of the values to have the same class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

simdat1 = lapply(c(1), function(mx) {
  lapply(c(1), function(my) {
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = &amp;quot;:&amp;quot;))
   }) %&amp;gt;% bind_rows
}) %&amp;gt;% bind_rows

simdatxy1 = simdat1[, c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)]

ggplot(simdatxy1, aes(x = x, y = y)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pam4.1 = pam(simdatxy1, 4)
sil.1 = silhouette(pam4.1, 4)
plot(sil.1, col=c(&amp;quot;red&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;purple&amp;quot;), main=&amp;quot;Silhouette&amp;quot;, border = &amp;quot;NA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-12-exercise-solution-for-5-1_files/figure-html/unnamed-chunk-20-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average silhouette width is 0.33, which is much lower than the clustered value of 0.50 that we see with the first simulation. It should be pointed out that several of the points end up with negative silhouette widths. These observations were assigned to the wrong group entirely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/Chap-Clustering.html#ques:ques-WSSclusters&#34;&gt;Modern Statistics for Modern Biology - Chapter 5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Silhouette_(clustering)&#34;&gt;Silhouette Clustering - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@jyotiyadav99111/selecting-optimal-number-of-clusters-in-kmeans-algorithm-silhouette-score-c0d9ebb11308&#34;&gt;Blog on Selecting Optimal Number of Clusters&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 2, Part 2</title>
      <link>/post/ex-2-6/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/ex-2-6/</guid>
      <description>


&lt;div id=&#34;exercise-2.6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 2.6&lt;/h1&gt;
&lt;p&gt;The first part of the exercise asks you to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Choose your own prior for the parameters of the beta distribution. You can do this by sketching it here: &lt;a href=&#34;https://jhubiostatistics.shinyapps.io/drawyourprior&#34; class=&#34;uri&#34;&gt;https://jhubiostatistics.shinyapps.io/drawyourprior&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After sketching a plot, I chose the parameters to set up a prior: &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; = 2.47 and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; = 8.5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-this-prior&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using this prior&lt;/h1&gt;
&lt;p&gt;Next, the exercise asks you:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once you have set up a prior, re-analyse the data from Section 2.9.2, where we saw Y = 40 successes out of n = 300 trials.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To be able to use the &lt;code&gt;loglikelihood&lt;/code&gt; function from the text, I first needed to redefine it here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loglikelihood = function(theta, n = 300, k = 40) { ## Function definition from the textbook
  log(choose(n, k)) + k * log(theta) + (n - k) * log(1 - theta)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I created a vector of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values between 0 and 1, spaced 0.001 units wide. The plot below shows different possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and the log likelihood for each of these values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetas = seq(0, 1, by = 0.001)
plot(thetas, loglikelihood(thetas), xlab = expression(theta),
     ylab = expression(paste(&amp;quot;log f(&amp;quot;, theta, &amp;quot; | y)&amp;quot;)),type = &amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, I used &lt;code&gt;rbeta&lt;/code&gt; to draw 1,000,000 random samples from a beta distribution with my new picks for the parameters for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rtheta = rbeta(1000000, shape1 = 2.47, shape2 = 8.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running the above, for each of these &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values, we then generate a random sample of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as observed in the histogram (with orange bars):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = vapply(rtheta, function(th) {
  rbinom(1, prob = th, size = 300)
}, numeric(1))
hist(y, breaks = 50, col = &amp;quot;orange&amp;quot;, main = &amp;quot;&amp;quot;, xlab = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our next step is to use this information to generate a posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at a fixed &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; value. In this example they used &lt;span class=&#34;math inline&#34;&gt;\(Y=40\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After running the above, for each of these thetas, we generated simulated values for the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(Y=40\)&lt;/span&gt; as observed in this histogram (with green bars).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetaPostEmp = rtheta[ y == 40 ]
hist(thetaPostEmp, breaks = 40, col = &amp;quot;chartreuse4&amp;quot;, main = &amp;quot;&amp;quot;,
     probability = TRUE, xlab = expression(&amp;quot;posterior&amp;quot;~theta), ylim=c(0,40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;densPostTheory  =  dbeta(thetas, 42.47, 268.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check how this compares to the theoretical posterior distribution
for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(Y = 40\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(thetaPostEmp, breaks = 40, col = &amp;quot;chartreuse4&amp;quot;, main = &amp;quot;&amp;quot;,
  probability = TRUE, xlab = expression(&amp;quot;posterior&amp;quot;~theta))
lines(thetas, densPostTheory, type=&amp;quot;l&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also check the means of both distributions computed above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(thetaPostEmp) # Empirical&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1363246&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtheta = thetas[2]-thetas[1]
sum(thetas * densPostTheory * dtheta) # Theoretical&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365727&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monte-carlo-integration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo integration&lt;/h2&gt;
&lt;p&gt;We can use Monte Carlo integration instead and then check the agreement between our Monte Carlo sample &lt;code&gt;thetaPostMC&lt;/code&gt; and our sample &lt;code&gt;thetaPostEmp&lt;/code&gt; with a QQ plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetaPostMC = rbeta(n = 1e6, 42.47, 268.5)
mean(thetaPostMC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365813&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(thetaPostMC, thetaPostEmp, type = &amp;quot;l&amp;quot;, asp = 1)
abline(a = 0, b = 1, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-24-ex2-6_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;densPost2 = dbeta(thetas, 42.47, 268.5)
mcPost2   = rbeta(1e6, 42.47, 268.5)
sum(thetas * densPost2 * dtheta)  # mean, by numeric integration&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365727&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mcPost2)                     # mean, by MC&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1365487&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thetas[which.max(densPost2)]      # MAP estimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(mcPost2, c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2.5%     97.5% 
## 0.1007321 0.1767921&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exercise solution for Chapter 2, Part 1</title>
      <link>/post/exercise-solution-for-chapter-2/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-2/</guid>
      <description>


&lt;p&gt;As always, load libraries first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercise-2.3-from-modern-statistics-for-modern-biologists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 2.3 from Modern Statistics for Modern Biologists&lt;/h2&gt;
&lt;p&gt;A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus &lt;span class=&#34;math inline&#34;&gt;\(4^3\)&lt;/span&gt; would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.) The code is redundant. But is the redundancy even among codons that code for the same amino acid? In other words, if alanine is coded by 4 different codons, do these codons code for alanine equally (each 25%), or do some codons appear more often than others? Here we use the tuberculosis genome to explore codon bias.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-explore-the-data-mtb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;a) Explore the data, &lt;code&gt;mtb&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Use &lt;code&gt;table&lt;/code&gt; to tabulate the &lt;code&gt;AmAcid&lt;/code&gt; and &lt;code&gt;Codon&lt;/code&gt; variables.&lt;/p&gt;
&lt;p&gt;Each amino acid is encoded by 1–6 tri-nucleotide combinations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtb = read.table(&amp;quot;example_datasets/M_tuberculosis.txt&amp;quot;, header = TRUE)
codon_no &amp;lt;- rowSums(table(mtb))
codon_no&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ala Arg Asn Asp Cys End Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Thr Trp Tyr 
##   4   6   2   2   2   3   2   2   4   2   3   6   2   1   2   4   6   4   1   2 
## Val 
##   4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PerThousands&lt;/code&gt; of each codon can be visualized, where each plot represents an amino acid and each bar represents a different codon that codes for that amino acid. But what does the &lt;code&gt;PerThousands&lt;/code&gt; variable mean?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtb, aes(x=Codon, y=PerThous)) +
  geom_col()+
  facet_wrap(~AmAcid, scales=&amp;quot;free&amp;quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-19-exercise-solution-for-chapter-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-the-perthous-variable&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;b) The &lt;code&gt;PerThous&lt;/code&gt; variable&lt;/h1&gt;
&lt;p&gt;How was the &lt;code&gt;PerThous&lt;/code&gt; variable created?&lt;/p&gt;
&lt;p&gt;The sum of all of the numbers of codons gives you the total number of codons in the M. tuberculosis genome: &lt;code&gt;all_codons&lt;/code&gt;. Remember that this is not the size of the M. tuberculosis genome, but the number of codons in all M. tuberculosis genes. To get the size of the genome, multiply each codon by 3 (for each nucleotide) and add all non-coding nucleotides (which we do not know from this data set).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_codons = sum(mtb$Number)
all_codons&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1344223&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;PerThousands&lt;/code&gt; variable is derived by dividing the number of occurrences of the codon of interest by the total number of codons. Because this number is small and hard to interpret, multiplying it by 1000 gives a value that is easy to make sense of. Here is an example for proline. The four values returned align to the four codons that each code for proline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pro  =  mtb[mtb$AmAcid == &amp;quot;Pro&amp;quot;, &amp;quot;Number&amp;quot;]
pro / all_codons * 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.560240  6.121752  3.405685 17.032144&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;c-codon-bias&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;c) Codon bias&lt;/h1&gt;
&lt;p&gt;Write an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.&lt;/p&gt;
&lt;p&gt;First, let’s look at the expected frequencies of each codon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_expected &amp;lt;- data.frame(codon_no) %&amp;gt;%
  rownames_to_column(var = &amp;quot;AmAcid&amp;quot;) %&amp;gt;%
  mutate(prob_codon = 1/codon_no)
codon_expected&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    AmAcid codon_no prob_codon
## 1     Ala        4  0.2500000
## 2     Arg        6  0.1666667
## 3     Asn        2  0.5000000
## 4     Asp        2  0.5000000
## 5     Cys        2  0.5000000
## 6     End        3  0.3333333
## 7     Gln        2  0.5000000
## 8     Glu        2  0.5000000
## 9     Gly        4  0.2500000
## 10    His        2  0.5000000
## 11    Ile        3  0.3333333
## 12    Leu        6  0.1666667
## 13    Lys        2  0.5000000
## 14    Met        1  1.0000000
## 15    Phe        2  0.5000000
## 16    Pro        4  0.2500000
## 17    Ser        6  0.1666667
## 18    Thr        4  0.2500000
## 19    Trp        1  1.0000000
## 20    Tyr        2  0.5000000
## 21    Val        4  0.2500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, calculate the observed frequencies for each codon seen in the data set and use the chi-squared test statistic to determine if the difference between expected and observed codon frequencies is even or if some codon sequences are used more than others.&lt;/p&gt;
&lt;p&gt;To start, you can group the data by amino acid and then determine a few things about
the amino acid or the possible codons for it, including the total observations
across all codons for the amino acid (&lt;code&gt;total&lt;/code&gt;), the number of codons for that
amino acid (&lt;code&gt;n_codons&lt;/code&gt;), and the expected count for each codon for that amino acid
(the total number of observations for that amino acid divided by the number of
codons, giving an expected number that’s the same for all codons of an amino
acid; &lt;code&gt;expected&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_compared &amp;lt;- mtb %&amp;gt;% 
  group_by(AmAcid) %&amp;gt;% 
  mutate(total = sum(Number),
         n_codons = n(),
         expected = total / n_codons)
codon_compared&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 7
## # Groups:   AmAcid [21]
##    AmAcid Codon Number PerThous  total n_codons expected
##    &amp;lt;fct&amp;gt;  &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Gly    GGG    25874    19.2  132810        4   33202.
##  2 Gly    GGA    13306     9.9  132810        4   33202.
##  3 Gly    GGT    25320    18.8  132810        4   33202.
##  4 Gly    GGC    68310    50.8  132810        4   33202.
##  5 Glu    GAG    41103    30.6   62870        2   31435 
##  6 Glu    GAA    21767    16.2   62870        2   31435 
##  7 Asp    GAT    21165    15.8   77852        2   38926 
##  8 Asp    GAC    56687    42.2   77852        2   38926 
##  9 Val    GTG    53942    40.1  114991        4   28748.
## 10 Val    GTA     6372     4.74 114991        4   28748.
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;mutate&lt;/code&gt; function is used after &lt;code&gt;group_by&lt;/code&gt; to do all this
within each amino acid group of codons, but without collapsing to one row per
amino acid, as a &lt;code&gt;summarize&lt;/code&gt; call would.&lt;/p&gt;
&lt;p&gt;To convince yourself that this has worked out correctly, you can repeat
the plot we made before and see that the bars for the expected values are
always equal across all codons for an amino acid:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(codon_compared, aes(x=Codon, y=expected)) +
     geom_col()+
     facet_wrap(~AmAcid, scales=&amp;quot;free&amp;quot;) +
     theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-02-19-exercise-solution-for-chapter-2_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can calculate the chi-squared (&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;) statistic and compare it to the
chi-squared distribution to get the p-value when testing against the null hypothesis
that the amino acid observations are uniformly distributed across codons. The &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;
is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\chi^2 = \sum_i{\frac{(O_i-E_i)^2}{E_i}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(O_i\)&lt;/span&gt; is the observed value of data point &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;code&gt;Number&lt;/code&gt; in our data); and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E_i\)&lt;/span&gt; is the expected value of data point &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;code&gt;expected&lt;/code&gt; in our data)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our data, we can calculate the contribution to the total &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic
from each data point (in this case, each codon within an amino acid) using
&lt;code&gt;mutate&lt;/code&gt;, and then
add these values up using &lt;code&gt;group_by&lt;/code&gt; to group by amino acid followed by
&lt;code&gt;summarize&lt;/code&gt; to sum up across all the data points for an amino acid.
The other information we need to get is the number of codons for the
amino acid, because we’ll need this to determine the degrees of freedom
for the chi-squared distribution. Next, we used &lt;code&gt;mutate&lt;/code&gt; with
&lt;code&gt;pchisq&lt;/code&gt; to determine the p-values within each amino acid group for the
test against the null that the codons are uniformly distributed for that
amino acid (i.e., that there isn’t codon bias). These p-values turn out to
be super small, so we’re using a technique to get the log-transform versions of
them instead, which we explain a bit more later. Finally, we used &lt;code&gt;arrange&lt;/code&gt; to
list the amino acids by evidence against uniform distribution of the codons,
from most evidence against (smallest p-value so most negative log(p-value))
to least evidence against (although still plenty of evidence against) and added
an &lt;code&gt;index&lt;/code&gt; with the ranking for each codon by adding a column with the sequence
of numbers from 1 to the number of rows in the data (&lt;code&gt;n()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codon_compared %&amp;gt;% 
  filter(n_codons &amp;gt; 1) %&amp;gt;% 
  group_by(AmAcid) %&amp;gt;% 
  mutate(chi_squared = ((Number - expected)^2/expected)) %&amp;gt;% 
  summarise(chi_squared = sum(chi_squared),
            n = n()) %&amp;gt;% 
  mutate(p_value = pchisq(chi_squared, df = n-1, log = TRUE, lower.tail = FALSE)) %&amp;gt;% 
  arrange(p_value) %&amp;gt;% 
  mutate(rank = 1:n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 5
##    AmAcid chi_squared     n p_value  rank
##    &amp;lt;fct&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 Leu        135432.     6 -67700.     1
##  2 Ala         75620.     4 -37805.     2
##  3 Arg         72183.     6 -36076.     3
##  4 Thr         58767.     4 -29378.     4
##  5 Val         58737.     4 -29363.     5
##  6 Ile         56070.     3 -28035.     6
##  7 Gly         52534.     4 -26262.     7
##  8 Pro         45400.     4 -22695.     8
##  9 Ser         36742.     6 -18357.     9
## 10 Asp         16208.     2  -8109.    10
## 11 Phe         13444.     2  -6727.    11
## 12 Asn         11404.     2  -5707.    12
## 13 Gln          9376.     2  -4693.    13
## 14 Lys          6382.     2  -3195.    14
## 15 Glu          5947.     2  -2978.    15
## 16 His          5346.     2  -2678.    16
## 17 Tyr          4738.     2  -2373.    17
## 18 Cys          2958.     2  -1483.    18
## 19 End           928.     3   -464.    19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you may notice, these log transforms of the p-values (which we got rather than untransformed p-values in the &lt;code&gt;pchisq&lt;/code&gt; call because we used the option &lt;code&gt;log = TRUE&lt;/code&gt;) are large in magnitude and negative (so very tiny once you take the exponent if you re-transformed them to p-values) values. If you tried to calculate the untransformed p-values (and we did!), this number is so small (0.00000000e+00) that it is too small for R—it shows up as exactly zero in R, even though it actually is a very tiny, but still non-zero, number. To get around this issue, we told &lt;code&gt;pchisq&lt;/code&gt; to work on these p-values as log transforms, and then we left the p-value as that log-transformed value. A group of numbers that are log transformed will be in the same order as their untransformed versions, so we don’t need to convert back to figure out which amino acid had that smallest p-value. We can just sort the amino acids from most negative to less negative using these log-transformed versions of the p-values. We now have the amino acids ranked from most biased codons (1) to least (19).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to create an exercise solution blog post</title>
      <link>/post/how-to-create-an-exercise-solution-blog-post/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/how-to-create-an-exercise-solution-blog-post/</guid>
      <description>


&lt;p&gt;Each of you will be responsible once or twice over the semester to create
a blog post that provides a clean, clearly-presented solution to the
in-class exercise for the week. This blog post provides the technical
instructions for writing and submitting that exercise.&lt;/p&gt;
&lt;p&gt;Your exercise solution should be posted &lt;strong&gt;before&lt;/strong&gt; the next class meeting.
Since it will need to be reviewed by the faculty before it can be officially
posted, please plan to submit it by the &lt;strong&gt;Tuesday after&lt;/strong&gt; the class for your
exercise. Student assignments for the exercises are given in the
Schedule section of our course website.&lt;/p&gt;
&lt;div id=&#34;overview-of-creating-a-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview of creating a post&lt;/h2&gt;
&lt;p&gt;You will be submitting your exercise solution as a blog post. Creating
one for our website will follow all the same steps as creating a blog
post for a vocabulary list, just with different content. Please read
the &lt;a href=&#34;https://kind-neumann-789611.netlify.com/post/creating-a-vocabulary-list-blog-post/&#34;&gt;post on creating a vocabulary list&lt;/a&gt;
and follow the steps there to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update your fork of the website&lt;/li&gt;
&lt;li&gt;Make a new blog post&lt;/li&gt;
&lt;li&gt;Use RMarkdown syntax to write the blog post&lt;/li&gt;
&lt;li&gt;Submit the blog post&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;content-for-the-blog-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Content for the blog post&lt;/h2&gt;
&lt;p&gt;The blog post should provide a walk-through of the solution to that week’s in-course
exercise. We have posted an example for &lt;a href=&#34;https://kind-neumann-789611.netlify.com/post/exercise-solution-for-chapter-1/&#34;&gt;the exercise for Chapter 1&lt;/a&gt;
to give you an idea of what you should aim to write.&lt;/p&gt;
&lt;p&gt;Generally, this exercise will be a resource for everyone in the class, to make sure
they’ve understood the exercise, as well as to see how someone else tackled the problem.
Your solution should cover all parts of the exercise (for example, if there’s a
part A and B, you should cover both). You can start by writing it as you would if you
were assigned the exercise as a homework problem, but then you should do a second step
of revision to provide some context and dig a bit deeper into how you tackled
the question. Since we are only requiring you to write up exercise answers once
or twice over the semester (rather than submitting homework for exercises every
week), we expect this product to be more in-depth and polished than a typical
homework solution.&lt;/p&gt;
&lt;p&gt;First, make sure that you have provided text explaining what the
exercise asks for, in case the reader hasn’t recently read the exercise prompt.
Second, please add a few details either about how you tackled the problem through code
or how the statistical principles covered in the exercise could apply to other problems
you’ve come across in your research or coursework.&lt;/p&gt;
&lt;p&gt;To help in preparing your post, plan to spend the exercise time in class during the
week of your exercise visiting the different groups of students working on the
exercise. You can talk to them about how they’re approaching the problem, how they
interpret it, etc., to help you develop your own answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tips&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be sure to refresh yourself on all the Markdown formatting tags you can use to improve
the appearance of your post. Be sure to include things like section headings and
italics or bold as appropriate. RStudio’s website has some nice cheatsheets on
RMarkdown that can help.&lt;/li&gt;
&lt;li&gt;Make sure you include R code if appropriate. If you put parentheses around an
assignment expression in R, it will print out the assigned object and make the
assignment in the same call—you might find this useful in writing concise code
while still showing what’s in the objects you create.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;$&lt;/code&gt; and &lt;code&gt;$$&lt;/code&gt; tags in RMarkdown to include mathematical equations in your blog post
when appropriate.&lt;/li&gt;
&lt;li&gt;If you need to read in a dataset for R code in your blog post, save it in the
website directory’s “content/post/example_datasets” subdirectory. If your data
comes from an online source or from an R library, you won’t need to do this,
only if you need a “local” copy of the datafile to run your RMarkdown code.&lt;/li&gt;
&lt;li&gt;You are welcome to draw from (and cite) other statistics textbooks or dictionaries
if you’d like to in explaining the problem and your approach to it.&lt;/li&gt;
&lt;li&gt;For the code, look at vignettes and helpfiles, especially for packages you are not
familiar with.&lt;/li&gt;
&lt;li&gt;For a lot of Bioconductor packages, object-oriented programming is used pretty
heavily. This means that associated data in R packages will often be stored in a
format that you haven’t used yet. Look up more information on data classes used in
your exercise if you aren’t familiar with them. You can use the &lt;code&gt;class&lt;/code&gt; function
to determine the class of an object as well as the name of the package that defines
that class. The &lt;code&gt;str&lt;/code&gt; function is often helpful for exploring a data object class, as well.
Many of the Bioconductor object classes will have special &lt;em&gt;accessor methods&lt;/em&gt;, which are
functions that allow you to extract certain elements from the object—check the helpfile
for the object class, as these methods are often listed there with examples.&lt;/li&gt;
&lt;li&gt;Googling can also be very helpful for learning more about functions, packages, and
datasets in R, especially if you don’t yet know what package the item is from.&lt;/li&gt;
&lt;li&gt;Most Bioconductor packages have very nice vignettes available online and from your
R session once you have installed the package. These are a great place to start to find
out more about how to use the functions and object classes that come with the package.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chapter 1 exercise setup</title>
      <link>/post/chapter-1-exercise-setup/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/chapter-1-exercise-setup/</guid>
      <description>


&lt;p&gt;The code instructions in the exercise statement appear to be outdated. The code below worked on my machine. Note that when asked whether I would like to update packages from the binary version, I said no. (When I said yes, &lt;code&gt;R&lt;/code&gt; gave an error.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!requireNamespace(&amp;quot;BiocManager&amp;quot;, quietly = TRUE))
  install.packages(&amp;quot;BiocManager&amp;quot;)
BiocManager::install(c(&amp;quot;Biostrings&amp;quot;, &amp;quot;BSgenome.Celegans.UCSC.ce2&amp;quot;,&amp;quot;BSgenome&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the various data genome data sets available by loading the &lt;code&gt;BSgenome&lt;/code&gt; library and typing &lt;code&gt;available.genomes()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you have the needed packages installed, you can access the sequence data for this exercise via the following commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(&amp;quot;BSgenome.Celegans.UCSC.ce2&amp;quot;))
Celegans&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Worm genome:
## # organism: Caenorhabditis elegans (Worm)
## # provider: UCSC
## # provider version: ce2
## # release date: Mar. 2004
## # release name: WormBase v. WS120
## # 7 sequences:
## #   chrI   chrII  chrIII chrIV  chrV   chrX   chrM                              
## # (use &amp;#39;seqnames()&amp;#39; to see all the sequence names, use the &amp;#39;$&amp;#39; or &amp;#39;[[&amp;#39; operator
## # to access a given sequence)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seqnames(Celegans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;chrI&amp;quot;   &amp;quot;chrII&amp;quot;  &amp;quot;chrIII&amp;quot; &amp;quot;chrIV&amp;quot;  &amp;quot;chrV&amp;quot;   &amp;quot;chrX&amp;quot;   &amp;quot;chrM&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Celegans$chrM&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   13794-letter &amp;quot;DNAString&amp;quot; instance
## seq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(Celegans$chrM)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;DNAString&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;Biostrings&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(Celegans$chrM)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Biostrings packages provides functions to summarize the sequence. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Biostrings&amp;quot;)
lfM = letterFrequency(Celegans$chrM, letters=c(&amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;T&amp;quot;))
lfM&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    A    C    G    T 
## 4335 1225 2055 6179&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(lfM)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13794&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
