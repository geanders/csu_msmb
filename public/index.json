[{"authors":null,"categories":null,"content":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.\n","date":1588032000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588107139,"objectID":"a99ca1d34add3871e4b8e2225c007a06","permalink":"/authors/bailey-fosdick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bailey-fosdick/","section":"authors","summary":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.","tags":null,"title":"Bailey Fosdick","type":"authors"},{"authors":null,"categories":null,"content":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.\n","date":1587686400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587767890,"objectID":"d7248a08ab17e3d5c2bea65700d68a9b","permalink":"/authors/camron-pearce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/camron-pearce/","section":"authors","summary":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.","tags":null,"title":"Camron Pearce","type":"authors"},{"authors":null,"categories":null,"content":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.\n","date":1587513600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586917751,"objectID":"a2f4c4d8a79a270a1ff69d28937404db","permalink":"/authors/sarah-cooper/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sarah-cooper/","section":"authors","summary":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.","tags":null,"title":"Sarah Cooper","type":"authors"},{"authors":null,"categories":null,"content":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She\u0026rsquo;s excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.\n","date":1586476800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586551291,"objectID":"359ce304229cff7b3cd9e06860d9e8f8","permalink":"/authors/sere-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sere-williams/","section":"authors","summary":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She\u0026rsquo;s excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.","tags":null,"title":"Sere Williams","type":"authors"},{"authors":null,"categories":null,"content":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.\n","date":1586131200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586211407,"objectID":"2c23a0a0af5e97a1815648da4252c925","permalink":"/authors/mikaela-elder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mikaela-elder/","section":"authors","summary":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.","tags":null,"title":"Mikaela Elder","type":"authors"},{"authors":null,"categories":null,"content":"Zach Laubach\u0026rsquo;s research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data. He has carried out his research in birds, hyenas, and humans.\n","date":1585526400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585625857,"objectID":"995dbc8c49ceaac8348f9682a5aeed3c","permalink":"/authors/zach_laubach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zach_laubach/","section":"authors","summary":"Zach Laubach\u0026rsquo;s research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data.","tags":null,"title":"Zach Laubach","type":"authors"},{"authors":null,"categories":null,"content":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.\n","date":1583971200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1584040643,"objectID":"9eeff3b66d9db9aba8ad7e1cd7ebf977","permalink":"/authors/brooke-anderson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brooke-anderson/","section":"authors","summary":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.","tags":null,"title":"Brooke Anderson","type":"authors"},{"authors":null,"categories":null,"content":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.\n","date":1583452800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1583452800,"objectID":"05bf39477bf7e4b59f2e5aed40d43fa8","permalink":"/authors/daniel-dean/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel-dean/","section":"authors","summary":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.","tags":null,"title":"Daniel Dean","type":"authors"},{"authors":null,"categories":null,"content":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.\n","date":1582243200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582321282,"objectID":"7979d3030119d29653a50463c423f481","permalink":"/authors/burton-karger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/burton-karger/","section":"authors","summary":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.","tags":null,"title":"Burton Karger","type":"authors"},{"authors":null,"categories":null,"content":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She\u0026rsquo;s currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she\u0026rsquo;s not in the lab, she enjoys traveling and experiencing new food and cultures.\n","date":1582156800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582156800,"objectID":"7208808b98af56edaf6697e234c3877b","permalink":"/authors/amy-fox/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/amy-fox/","section":"authors","summary":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She\u0026rsquo;s currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she\u0026rsquo;s not in the lab, she enjoys traveling and experiencing new food and cultures.","tags":null,"title":"Amy Fox","type":"authors"},{"authors":null,"categories":null,"content":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.\n","date":1581552000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1581691815,"objectID":"a0b6f7b6e4c0c6781dd1b5bb9f45c4be","permalink":"/authors/sierra-pugh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sierra-pugh/","section":"authors","summary":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.","tags":null,"title":"Sierra Pugh","type":"authors"},{"authors":null,"categories":null,"content":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"84c043c7c8c0042274f75edaccfe9957","permalink":"/authors/james_dilisio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/james_dilisio/","section":"authors","summary":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.","tags":null,"title":"James DiLisio","type":"authors"},{"authors":null,"categories":null,"content":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f8f117d42fb07fcd0da9af3c5821003","permalink":"/authors/mike-lyons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mike-lyons/","section":"authors","summary":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.","tags":null,"title":"Mike Lyons","type":"authors"},{"authors":null,"categories":null,"content":"Sherry WeMott is a research associate and graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"93e55912b52bf4b4c2aca0826be7d218","permalink":"/authors/sherry-wemott/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sherry-wemott/","section":"authors","summary":"Sherry WeMott is a research associate and graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.","tags":null,"title":"Sherry WeMott","type":"authors"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 30 Large group: meeting link\n Group 1 (Sierra, Camron, Sere): meeting link Group 2 (Burton, Mikaela, Sherry, Amy): meeting link Group 3 (Daniel, James, Sarah): meeting link   Vocabulary quiz for April 30 Loading…   ","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588107139,"objectID":"0904a5238979fa30d069384bec6a2b15","permalink":"/post/details-for-class-on-april-30/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/post/details-for-class-on-april-30/","section":"post","summary":" Links for April 30 Large group: meeting link\n Group 1 (Sierra, Camron, Sere): meeting link Group 2 (Burton, Mikaela, Sherry, Amy): meeting link Group 3 (Daniel, James, Sarah): meeting link   Vocabulary quiz for April 30 Loading…   ","tags":["class meeting"],"title":"Details for class on April 30","type":"post"},{"authors":["Camron Pearce"],"categories":["vocabulary","Chapter 11"],"content":"  Chapter 11 is focused on learning how to read, write, and manipulate images in R. The first sections are helping the reader understand when to apply different filters and transformations to an image and why it is necessary. It then touches on segmentation and feature extraction, two components that are utilized to simplify an image for machine learning and recognition. Finally, statistal methods are introduced to analyze spacial distributions and spatial point process is introduced on a basic level.\nThe vocabulary words for Chapter 11 are:\n        segmentation  partitioning an image to assign a label to every pixel or group of pixels with similar characteristics    slot  a part, element, or “property” of an object in the context of object-oriented programming in R    classification  the process of grouping observations in a dataset by their similarities in terms of measured characteristics    feature extraction  the process of building derived values to describe observations or features from the initial set of measured data, with the aim of creating a new set of characteristics that is informative.    spatial point process  mechanism that generates a random collection of coordinates or points randomly located along an underlying mathematical space. There is at most one point observed at any location.    Poisson process  mechanism that generates instantaneous events (in time and/or space) based on the Poisson distribution    Ripley’s K function  a descriptive statistic for detecting the deviations from spatial homogeneity that can help determine if points are random, dispersed, or clustered    pair correlation function  a description of how the point density varies as a function of distance from the point of reference    spatial transformation  changes to a coordinate system that provides a new approach to defining variation of material parameters    linear filter  a tool for refining an image such that the output pixel is a combination of the time-varying input pixels subject to the constraint of linearity    dynamic range  the ratio or logarithmic value of the difference between the largest and smallest values    noise reduction  the process of removing the undesirable variation in image pixelation    adaptive thresholding  segmenting an image using smaller regions that are defined by the range of local intensity values    binary image  an image consisting of pixels that can only have one of exactly two values, usually black and white    morphological operation  image processing in which each individual pixel in the image is adjusted based on the other pixels in the neighborhood    Voronoi tessellation  partitioning an image plane into regions closest to each set of points. Line segments are formed equidistant to the two nearest points    convex hull  the smallest polygon that encloses all of the points of interest in a set    spatial dependence  the propensity for nearby points to influence each other and possess similar attributes    virion  infectious nucleic acid surrounded by a protective protein capsid    actin  globular proteins that form the microfilaments essential for cell mobility and division    macrophages  immune cells responsible for engulfing potential pathogens and other lymphatic particles    dendritic cells  immune cells responsible for processing foreign material and presenting it to other cells in the immune system    light sheet microscopy  a method that illuminates a specimen in a single plane and detected from the perpendicular direction     Sources Consulted https://www.cso.ie/en/methods/classifications/classificationsexplained/ http://www.stat.umn.edu/geyer/8501/points.pdf\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528837/\nhttps://www.e-education.psu.edu/natureofgeoinfo/c1_p18.html\nhttps://www.leica-microsystems.com/science-lab/topics/light-sheet-microscopy/\nhttps://www.mathworks.com/help/images/morphological-filtering.html\n Practice   ","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587767890,"objectID":"72bafb87518d67654ebda7aee740e4ff","permalink":"/post/vocabulary-for-chapter-11/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-11/","section":"post","summary":"Chapter 11 is focused on learning how to read, write, and manipulate images in R. The first sections are helping the reader understand when to apply different filters and transformations to an image and why it is necessary. It then touches on segmentation and feature extraction, two components that are utilized to simplify an image for machine learning and recognition. Finally, statistal methods are introduced to analyze spacial distributions and spatial point process is introduced on a basic level.","tags":["vocabulary"],"title":"Vocabulary for Chapter 11","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 23 Large group: meeting link\n Group 1 (Sierra, Sarah, Daniel, Sere): meeting link Group 2 (Mikaela, Sherry, James): meeting link Group 3 (Burton, Camron, Amy): meeting link   Additional links  Link for HIV amino acid sequences: https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html Online book on ggtree: https://yulab-smu.github.io/treedata-book/ Introductory vignette for phangorn: https://cran.r-project.org/web/packages/phangorn/vignettes/Trees.pdf Vignette for ape: https://cran.r-project.org/web/packages/ape/vignettes/MoranI.pdf Online book with (much!) more on studying networks: http://networksciencebook.com/ Relevant COVID-19 article: https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30273-5/fulltext#.Xp_6SyUko0Y.twitter   Slido comments  https://admin.sli.do/event/lkcufqsv/questions   Vocabulary quiz for April 23 Loading…   ","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587606297,"objectID":"9dd7feb31b35870ea662ccd1a51a32b8","permalink":"/post/details-for-class-on-april-23/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/details-for-class-on-april-23/","section":"post","summary":"Links for April 23 Large group: meeting link\n Group 1 (Sierra, Sarah, Daniel, Sere): meeting link Group 2 (Mikaela, Sherry, James): meeting link Group 3 (Burton, Camron, Amy): meeting link   Additional links  Link for HIV amino acid sequences: https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html Online book on ggtree: https://yulab-smu.github.io/treedata-book/ Introductory vignette for phangorn: https://cran.r-project.org/web/packages/phangorn/vignettes/Trees.pdf Vignette for ape: https://cran.r-project.org/web/packages/ape/vignettes/MoranI.pdf Online book with (much!) more on studying networks: http://networksciencebook.com/ Relevant COVID-19 article: https://www.","tags":["class meeting"],"title":"Details for class on April 23","type":"post"},{"authors":["Sarah Cooper"],"categories":["vocabulary","Chapter 10"],"content":"  Chapter 10 discusses the use of networks and trees to visualize biological data. It covers the main components of each and how different data sets can be appropriately transformed into specific networks and trees based on what you are trying to present. The vocabulary words for Chapter 10 are:\n        Graph  A structure formed by a set of nodes or vertices and a set of edges between these vertices    Adjacency matrix  The matrix representation of edges of a graph with as many rows as nodes in the graph    Network  A weighted, directed graph    Sparse  In the context of graphs, a term to describe a graph when the number of edges is similar to the number of nodes    Dense  In the context of graphs, a term to describe a graph when the number of edges is (approximately) a quadratic function of the nodes    Arrows/directed edges  Graph edges that directionally connect nodes    Annotation variables  Graph visualization characteristics that help to demonstrate strength of a link in a graph by changing the width of the edge or covariates associated to the size or color of the node    Graph layouts  Different ways to plot a graph, either for aesthetic or practical reasons    Binary data  Data in which each observation can take only one of two values (e.g., 0 or 1)    Differentially expressed genes  A term to describe changes in gene expression levels between different experimental groups    Bipartite graph  A graph where each edge connects a node    Overrepresented or enriched  In the context of gene expresion, a term to describe increased expression of a gene / set of genes of interest    Gene Ontology (GO)  A resource aimed to unify the vocabulary to describe genes and gene product attributes across all species    Fisher’s exact test / Hypergeometric testing  Two-way table testing used to account for the fact that some categories are extremely numerous and others are rarer    Known skeleton graph  A graph that projects significance scores such as p-values    Perturbation  In the context of a network, an alteration of the function of a biological system, induced by external or internal mechanisms    Hotspots  In the context of a graph, areas with high event density    Rooted binary tree  A data tree in which each node has at most two children    Cycles  In the context of graphs, another word for loops: either self-loops or ones that go through several vertices    Ancestral taxa  Correspond to inner nodes and are inferred from the contemporaneous data on the tips    Contemporaneous  In the context of data for phylogenetic tress, organisms at terminal nodes that exist at the same time and are related to each other, thus revealing information about their common ancestors    OTUs (Operational Taxonomic Units)  A method of clustering organisms based on DNA similarity of a certain taxonomic marker gene (Tips of the tree)    Parameter  In the context of statistics, numerical value that describes a population    Gene trees  Structures produced when different genes show differences in their evolutionary histories    Markov chain  A sequence where given the current state, the next state is conditionally independent of all previous states    Molecular clock hypothesis  A term that describes a technique that uses the mutation rate to infer what happened to a species historically    Non-identifiability  The inability to distinguish a parameterization of a model based on observed data    Time homogeneity  In the context of Markov chains, the state of the mutation rate being constant across history    Generator  In the context of Markov chains, the instantaneous change probability matrix describing transitions between steps of the chain    Transition matrix  A matrix that contains all probabilities of any state changes for a Markov chain    Parsimony tree  A structure created using nonparametric method that minimizes the number of changes necessary to explain the data    Maximum likelihood tree  A structure created using a parametric method that uses efficient optimization algorithms to maximize the likelihood of a tree under the model assumptions.    Bayesian posterior distributions for trees  A method that uses MCMC to find posterior distributions of the phylogenies    Distance-based methods  Semi-parametric methods similar to the hierarchical clustering algorithms but use the parametric evolutionary models    Aligning  Arranging different sequences of DNA, RNA, or protein together to identify similarities or differences between them    indel (inserted-deleted) event  Insertion or deletion of bases in the genome of an organism    Filtering operations  In the context of low-quality rRNA reads, the removal of low-quality reads and trimming of remaining reads to a consistent length    Interactive  In the context of a plot, enabling direct actions on a graphical plot to change elements and link multiple plots    Spanning tree  A tree that goes through all points at least once    Minimum spanning tree (MST)  Given distances between vertices, a tree that spans all the points and has the minimum total length    Jitter  To slightly move coordinates on a graph to avoid too much overlapping    Undirected network  A term describing a graph without arrows between nodes    Associated  A term indicating that two variables are related    Friedman-Rafsky tests  Tests for two/multiple sample segregation on a minimum spanning tree    Pure edges  In the context of graphs, edges whose two nodes have the same level of the factor variable    Microbiome  The aggregate of all microbiota that reside on or within an organism tissues and biofluids along with the corresponding anatomical sites in which they reside    Exponential random graph models (ERGMs)  Models that can be used to predict vertex covariates    Protein interaction networks  In the context of graphing, a way to visualize observed protein-protein interactions    Phylogenetic tree  A tree used to visualize evolutionary relationships among species    Strain  In the context of a virus, a genetic variant or subtype    Taxa  A group of one or more populations of an organism making up a single unit, typically disected to the level of genus and species    Protein  A compound made up of amino acids; one of the four types of macromolecules that make up living organisms.     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page   Practice   ","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586917751,"objectID":"3f55cfe386c5e28463c916e16a0504a2","permalink":"/post/vocabulary-for-chapter-10/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-10/","section":"post","summary":"Chapter 10 discusses the use of networks and trees to visualize biological data. It covers the main components of each and how different data sets can be appropriately transformed into specific networks and trees based on what you are trying to present. The vocabulary words for Chapter 10 are:\n        Graph  A structure formed by a set of nodes or vertices and a set of edges between these vertices    Adjacency matrix  The matrix representation of edges of a graph with as many rows as nodes in the graph    Network  A weighted, directed graph    Sparse  In the context of graphs, a term to describe a graph when the number of edges is similar to the number of nodes    Dense  In the context of graphs, a term to describe a graph when the number of edges is (approximately) a quadratic function of the nodes    Arrows/directed edges  Graph edges that directionally connect nodes    Annotation variables  Graph visualization characteristics that help to demonstrate strength of a link in a graph by changing the width of the edge or covariates associated to the size or color of the node    Graph layouts  Different ways to plot a graph, either for aesthetic or practical reasons    Binary data  Data in which each observation can take only one of two values (e.","tags":["vocabulary","Chapter 10"],"title":"Vocabulary for Chapter 10","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 16 Large group: meeting link\n Group 1 (Sarah, James, Burton, Mikeala): meeting link Group 2 (Daniel, Amy, Sierra): meeting link Group 3 (Sere, Camron, Sherry): meeting link   Space for comments, questions  For Chapter 9: https://www.sli.do/ Event code: #MSMB_CH9 For Chapter 10 (will be live starting this weekend): https://www.sli.do/ Event code: #MSMB_CH10   Vocabulary quiz for April 16 Loading…   ","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587065300,"objectID":"f84c67b395e7a50d1ff7e669661c0f1a","permalink":"/post/details-for-class-april-16/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/details-for-class-april-16/","section":"post","summary":" Links for April 16 Large group: meeting link\n Group 1 (Sarah, James, Burton, Mikeala): meeting link Group 2 (Daniel, Amy, Sierra): meeting link Group 3 (Sere, Camron, Sherry): meeting link   Space for comments, questions  For Chapter 9: https://www.sli.do/ Event code: #MSMB_CH9 For Chapter 10 (will be live starting this weekend): https://www.sli.do/ Event code: #MSMB_CH10   Vocabulary quiz for April 16 Loading…   ","tags":["class meeting"],"title":"Details for class April 16","type":"post"},{"authors":["Sere Williams"],"categories":["vocabulary","Chapter 9"],"content":"  Chapter 9 covers multivariate methods for heterogenous data. It builds on methods covered in Chapter 7, like dimension reduction, by extending these ideas to more complex, heterogenous data.\nThe vocabulary words for Chapter 9 are:\n        multidimensional scaling (MDS)  a linear dimension reduction method applied in cases where distances between observations are available    clusters  in the context of data analysis, data points that group together    robust  in the context of a statistical method, a ‘sturdy’ estimator that is not heavily influenced by outliers    outlier  a single data point with large distances to other data points, thus potentially dominating and skewing the analysis    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    non-metric multidimensional scaling (NMDS)  a robust ordination method which attempts to embed data points in a new space while maintaining their respective order to one another    metadata  information, data, or descriptions that characterize other data    batch effects  hidden factors that affect the data but are not documented; e.g. running samples at the same time have a degree of similarity from being run in the same batch    confounded effects  a term describing when there is uncertainty in the source of variation impacting data    supplementary  in the context of variables for a statistical model, categorical variables added to continuous variables in heterogenous data    supplementary points  points created using the group-means of points in each of the groups    interactive  in the context of plots, data visualizations that can be manipulated in real time by the observer    contingency table  the result of counting the co-occurrence of any pair of categorical variables measured in a set of observations; for example, two phenotypes    chi-square distance  weighted Eucledian distance using relative counts and standardized by the mean, not the variance    biplots  a type of exploratory graph that displays information on both the observations and the variables of a data matrix    co-occurence matrix  a matrix that captures the extent to which variables are jointly observed in observations    correspondence analysis (CA) / dual scaling  a method for computing low dimensional projections that explain dependencies in categorical data    ordination method  a method which enables one to detect and interpret a hidden ordering, gradient or latent variable in the data    clustering  in the context of statistical methods, a way to detect and interpret a hidden factor/categorical variable    kernel  a linear algorithm designed to determine a non-linear decision boundary; used in pattern analysis to better understand general types of relations like clusters, rankings, principal components, or correlations    local linear embedding (LLE)  a nonlinear method for estimating nonlinear trajectories by points in the relevant state spaces    isomap  a nonlinear method for estimating nonlinear trajectories by points in the relevant state spaces    inertia  in the context of counts in a contingency table, the weighted sum of the squares of distances between observed and expected frequencies    covariance  measure of the joint variability of two random variables    matrix association  correlation of vectors derived from matrices based on dissimilarity    RV coefficient  the global measure of similarity of two data tables as opposed to two vectors; correlation coefficient for tables    penalty  a method to constrain the typical optimization algorithm, added to interpret correlation when there are too many degrees of freedom    sparsity penalty  an approach to maintain the number of non-zero coefficients to a minimum    heterogenous data  a mixture of many continuous and a few categorical variables    canonical correlation  a method for finding a few linear combinations of variables from each table that are as correlated as possible    nonlinear  a regression equation where the equation is not ‘linear in the parameters,’ meaning the relationship between parameters cannot be calulated by multiplying, exponentiating, or transforming independent variables    species tree  a simplified term for a diagram showing the relatedness of organisms based on biological, often genetic sequence, information    assay  an investigative (analytic) procedure in laboratory medicine, pharmacology, environmental biology and molecular biology for qualitatively assessing or quantitatively measuring the presence, amount, or functional activity of a target entity (the analyte)    protocol  a predefined written procedural method of conducting experiments    microarray  a ‘lab-on-a-chip’ method to assess many samples at once, often used in gene expression studies    taxon  a group of one or more populations of an organism making up a single unit, typically disected to the level of genus and species    mutation  an alteration in the nucleotide sequence of the genome of an organism or virus    phenotype  a visually observed genetic trait or characteristic    cell development  the process of a cell transitioning from one state to another, such as in the case of a cell transitioning from growth to division in mitosis    metabolite  an intermediate or end product of metabolism; typically a small, organic molecule     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. http://www.econ.upf.edu/~michael/stanford/maeb4.pdf https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/ https://www.wikipedia.org   Practice   ","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586551291,"objectID":"b8ab79ced61153083dce2875f84e1d4b","permalink":"/post/vocabulary-for-chapter-9/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-9/","section":"post","summary":"Chapter 9 covers multivariate methods for heterogenous data. It builds on methods covered in Chapter 7, like dimension reduction, by extending these ideas to more complex, heterogenous data.\nThe vocabulary words for Chapter 9 are:\n        multidimensional scaling (MDS)  a linear dimension reduction method applied in cases where distances between observations are available    clusters  in the context of data analysis, data points that group together    robust  in the context of a statistical method, a ‘sturdy’ estimator that is not heavily influenced by outliers    outlier  a single data point with large distances to other data points, thus potentially dominating and skewing the analysis    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    non-metric multidimensional scaling (NMDS)  a robust ordination method which attempts to embed data points in a new space while maintaining their respective order to one another    metadata  information, data, or descriptions that characterize other data    batch effects  hidden factors that affect the data but are not documented; e.","tags":["vocabulary","Chapter 9"],"title":"Vocabulary for Chapter 9","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 9 Large group: meeting link\n Group 1 (Sierra, Daniel, Burton): meeting link Group 2 (Camron, Sarah, Mikaela): meeting link Group 3 (Sere, James, Sherry, Amy): meeting link   Additional links  DESeq2 vignette edgeR vignette   Vocabulary quiz for April 9 Loading…   ","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586364432,"objectID":"3aee6c58c71f4e9cbdf1b625d5ec25ad","permalink":"/post/details-for-class-on-april-9/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/details-for-class-on-april-9/","section":"post","summary":" Links for April 9 Large group: meeting link\n Group 1 (Sierra, Daniel, Burton): meeting link Group 2 (Camron, Sarah, Mikaela): meeting link Group 3 (Sere, James, Sherry, Amy): meeting link   Additional links  DESeq2 vignette edgeR vignette   Vocabulary quiz for April 9 Loading…   ","tags":["class meeting"],"title":"Details for class on April 9","type":"post"},{"authors":["Mikaela Elder"],"categories":["vocabulary","chapter 8"],"content":"  Chapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n        RNA-Seq  sequencing of RNA molecules found in a population of cells or in a tissue    ChIP-Seq  sequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)    RIP-Seq  sequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein    DNA-Seq  sequencing of genomic DNA    HiC  high-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA    cDNA  complementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq    genetic screens  a technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification    read  the sequence obtained from a fragment    sequencing library  the collection of DNA molecules used as input for the sequencing machine    fragments  molecules being sequenced during a sequencing analysis    count table  a matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample    dynamic range  a ratio between the maximum and minimum values    heteroskedasticity  a phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different    normalization  a technique that adjusts for the nature and magnitude of systematic sampling biases    rare events  occurrences in the tail(s) of a distribution; observations that are extraordinarily high or low    dispersion  a measure of the spread of the data; a common measure is the standard deviation or variance    gamma-Poisson  negative binomial distribution with 2 parameters; 𝛼 and 𝛽    systematic biases  systematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment    metadata  a set of data that describes or gives information about other data    multifactorial design  an experimental design with more than one independent variable    balanced  in the context of study design, these are where there is an equal number of observations of all combinations of factors being tested    differential expression analysis  a type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups    intercept  a coefficient representing the base level of the measurement in the negative control    design factors  binary indicator variables    interaction effect  a parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation    design matrix  a matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions    residuals  a term in a model that reflects the experimental fluctuations (i.e. random noise)    least sum-of-squares fitting  a type of model fitting that minimizes the sum of the squared residuals    linear model  a model that is a linear function of parameters, i.e. takes the form: y_j = sum_k (x_jk * beta_k + e_j)    analysis of variance (ANOVA)  an analysis that decomposes patterns in the data into systematic variability and noise    noise  variability unaccounted for by model parameters    systematic variability  variability accounted for by model parameters    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    robust  a “sturdy” estimator that is not heavily influenced by outliers    least absolute deviations  minimization of the sum of the absolute values of the residuals    least quantile of squares  a type of regression where the difference between the model quantile and empirical quantile is minimized    least trimmed sum of squares  a type of regression that minimized the sum of squared residuals, where the sum is over only a fraction of the smallest residuals    logistic regression  a type of generalized linear regression for binary data where the outcome is transformed by the logistic function and bounded between 0 and 1    maximum likelihood  a method for parameter estimation that finds the parameter value that maximizes the probablity of the observed data under the model    likelihood  a function of a model parameter which is equal to the probability of the observed data under the model    maximum-likelihood estimates  model parameters that are estimated by maximizing the probability of the observed data under the model    nuisance factor / blocking factor  a factor that has some effect on the response but is of no interest to the experiment    batch effects  hidden factors that affect the data but are not documented; e.g. running samples at the same time have a degree of similarity from being run in the same batch    pseudocounts  transformations that take the form y = log2(n + n_0) where n is the count and n_0 is a chosen positive constant    variance stabilizing transformation  a transformation that has finite values and finite slope, even for counts close to zero    regularized logarithm (rlog) transformation  a technique that transforms the original count data to a log2-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data    Cook’s distance  a measure of how much a single sample is influencing the coefficients in a model; large values indicate an outlier count    sampling without replacement  a random sample in which no observation occurs more than one time in the sample    null hypothesis  often, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.    variability  in statistics, the amount by which a set of observations deviate from their mean    outlier  a data point that does not follow the pattern of the rest of the data; often this data point will have a large residual    M-estimation  a type of regression analysis that is more robust than OLS to outliers or data that does not follow a normal distribution; it minimizes the sum of the penalization function applied to the residuals    conservative  an approach that prioritizes reducing false positives    splicing  a process in eukaryotic organisms where mRNA is cut down from the full-length gene to just the exons before being translated    exons  segments of a gene that actually get used during translation or encode for a protein    isoforms  different forms of the same gene that result from splicing events that combine different exons in an mRNA script    upregulated  a term used to describe the increased expression of a gene    gene knockdown  a way of inactivating a gene by targeting its mRNA transcript for inactivation or degradation    gene knockout  deletion of a gene from the genome    transcriptome  the total of all of the mRNA expressed from genes in an organism    polymorphism  genetic variation within a population     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Lexico: https://www.lexico.com Statistics How To: https://www.statisticshowto.com Lavrakas, 2008. Sampling without replacement. Encyclopedia of Survey Research Methods. https://dx.doi.org/10.4135/9781412963947.n516   Practice   ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586211407,"objectID":"faf087f699e0eef13cfb4f9ef67d569b","permalink":"/post/chapter-8-vocabulary-list/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/chapter-8-vocabulary-list/","section":"post","summary":"Chapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n        RNA-Seq  sequencing of RNA molecules found in a population of cells or in a tissue    ChIP-Seq  sequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)    RIP-Seq  sequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein    DNA-Seq  sequencing of genomic DNA    HiC  high-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA    cDNA  complementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq    genetic screens  a technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification    read  the sequence obtained from a fragment    sequencing library  the collection of DNA molecules used as input for the sequencing machine    fragments  molecules being sequenced during a sequencing analysis    count table  a matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample    dynamic range  a ratio between the maximum and minimum values    heteroskedasticity  a phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different    normalization  a technique that adjusts for the nature and magnitude of systematic sampling biases    rare events  occurrences in the tail(s) of a distribution; observations that are extraordinarily high or low    dispersion  a measure of the spread of the data; a common measure is the standard deviation or variance    gamma-Poisson  negative binomial distribution with 2 parameters; 𝛼 and 𝛽    systematic biases  systematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment    metadata  a set of data that describes or gives information about other data    multifactorial design  an experimental design with more than one independent variable    balanced  in the context of study design, these are where there is an equal number of observations of all combinations of factors being tested    differential expression analysis  a type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups    intercept  a coefficient representing the base level of the measurement in the negative control    design factors  binary indicator variables    interaction effect  a parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation    design matrix  a matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions    residuals  a term in a model that reflects the experimental fluctuations (i.","tags":["vocabulary","chapter 8"],"title":"Chapter 8 Vocabulary List","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" General course information We are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post. 3:10-4:00 pm: Large group meeting for group discussion. 4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n Links for April 2 Large group: meeting link\n Group 1 (Sierra, Camron, James): meeting link Group 2 (Sherry, Amy, Mikaela): meeting link Group 3 (Daniel, Burton, Sarah, Sere): meeting link   Vocabulary quiz for April 2 Loading…   ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585665132,"objectID":"a3086d5472c57c6fd375723b87696627","permalink":"/post/details-for-class-on-april-2/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/details-for-class-on-april-2/","section":"post","summary":"General course information We are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post. 3:10-4:00 pm: Large group meeting for group discussion. 4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n Links for April 2 Large group: meeting link","tags":["class meeting"],"title":"Details for class on April 2","type":"post"},{"authors":["Zach Laubach"],"categories":["Chapter 7","vocabulary"],"content":"  Chapter 7 covers multivariate analysis, with a focus on principal component analysis and dimension reduction in general.\n        principal component analysis (PCA)  an unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data    matrix  a two dimensional arrangement of rows and columns used to store data    mass spectroscopy  a measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance    correlation coefficient  a measure of how two variables co-vary, reported as a single summary value    centering  subtracting the mean of the data so the new mean is 0    scaling / standardizing  dividing data values by the data’s standard deviation so the new standard deviation is 1    data simplification  a broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data    dimension reduction  summarizing data to reduce the number of variables for downstream analyses    principal scores  a normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component    unsupervised learning  a machine learning method used to find patterns in the data without a priori variable ranking or labeling    status  in the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.g., to consider one variable as the outcome or goal and the rest as potential predictive variables)    projection  a representation of data from a higher dimensional space to a lower dimensional space    linear  in the context of a statistical technique, a description that describes the search for relationships between variables that can be expressed as a linear combination of predictors    regression line  a linear function of the form y = mx + b which is used to project two-dimensional data onto a 1 dimensional line    linear regression  a supervised method that models the relationship between explanatory and response variables by minimizing the residual sum of squares with respect to the response variable    supervised learning  in the context of a statistical learning technique, a machine learning method that uses specified, user defined inputs to map patterns (input/output associations) in data    predictor  an independent, explanatory, or ‘x’ variable in a model    response  an outcome or ‘y’ variable in a model that is thought to be affected by a predictor    principal components  uncorrelated latent variables created by the PCA procedure, of which there are as many as there are original variables entered into the procedure    inertia  in the context of variability of points, the total variance of a point cloud based on the sum of squares of the projection of points    linear combination  mathematical expression in which terms are scaled by constants and then added together    loadings  in the context of principal components, these values quantify the weight of each original variable in a principal component    singular value decomposition (SVD)  a way to decompose a rectangular matrix by factoring it into three different matrices in a way that has some useful mathematical applications    rank  in the context of a matrix, the maximum number of linearly independent column or row vectors    norm  in the context of a vector, a positive scalar quantity reflecting its size/magnitude    singular value  a non-negative, normalizing value from a singular value decomposition quantifying the relative importance of the corresponding singular vectors    orthonormal  the characteristic of a set of vectors that are both orthogonal (uncorrelated) and normalized    principal plane  a 2-dimensional space across which the data are most spread out or variable    trace  in the context of matrices, the sum of the diagonal elements of a square matrix    supplementary information  extra information or instruction to help clarify research question, procedure or results    metadata  information, data, or descriptions that characterize other data    biplot  a type of exploratory graph that displays information on both the observations and the variables of a data matrix    biometric characteristics  physical, physiological, demographic, or behavioral features of an organism that can be measured and quantified    proliferation rate  speed at which the number of cells increase through the process of cellular division    gene expression profile  a snapshot measure of the level of activity/expression (transcription) of a collection (thousands) of genes, representing a global measure of gene function    T-cell populations  groups of differentiated white blood cells that function in immune response    operational taxonomic units (OTUs)  clusters of closely related species of bacteria based on sequence similarity    transcriptome data  the complete set of all RNA molecules measured from a biological sample generated from genome-wide sequencing methods, like RNA-seq    sequence read  an inferred sequence of base pairs, or fragments of the genome, generated from one of many genomics methods    proteomic profile  a snapshot measure of the levels of all proteins measured in a biological sample    molecule  two or more chemically bond atoms that lack a charge    m/z ratio  mass to charge ratio used in mass spectrometry to differentiation molecules    wild-type  a normal allele or phenotype that occurs under natural conditions     Source Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following source:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page   Practice   ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585625857,"objectID":"599af682b514a006f13ca6642d76980a","permalink":"/post/vocabulary-for-chapter-7/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-7/","section":"post","summary":"Chapter 7 covers multivariate analysis, with a focus on principal component analysis and dimension reduction in general.\n        principal component analysis (PCA)  an unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data    matrix  a two dimensional arrangement of rows and columns used to store data    mass spectroscopy  a measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance    correlation coefficient  a measure of how two variables co-vary, reported as a single summary value    centering  subtracting the mean of the data so the new mean is 0    scaling / standardizing  dividing data values by the data’s standard deviation so the new standard deviation is 1    data simplification  a broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data    dimension reduction  summarizing data to reduce the number of variables for downstream analyses    principal scores  a normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component    unsupervised learning  a machine learning method used to find patterns in the data without a priori variable ranking or labeling    status  in the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.","tags":["Chapter 7","vocabulary"],"title":"Vocabulary for Chapter 7","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 6","quiz"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584040643,"objectID":"4bf4bf373f3cd7c717411b606027c93c","permalink":"/post/chapter-6-vocabular-zuiz/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/chapter-6-vocabular-zuiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["Chapter 6","quiz"],"title":"Chapter 6 vocabulary quiz","type":"post"},{"authors":["Daniel Dean"],"categories":["Chapter 6","vocabulary"],"content":"  Chapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n        Occam’s razor  Heuristic stating that the simplest explanation for a phenomenon is often the best    rejection region  Subset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.g. outcomes with a null-distribution probability \u0026lt; 0.05; if an outcome falls within this region (e.g., p \u0026lt; 0.05), it suggests that the null hypothesis is not true.    test statistic  Metric for measuring how well a null hypothesis fits the data    null hypothesis  Hypothesis describing some ‘uninteresting’ outcome (e.g., that no difference exists between certain groups of events/outcomes)    null distribution  Distribution of possible outcomes, given that the null hypothesis is true    alternative hypothesis  A hypothesis providing a different probability distribution than the null hypothesis; conceptually, holds that some difference from the null hypothesis exists (e.g. different means, frequencies, trends)    significance level/false positive rate/Type I error  Probability of incorrectly rejecting the null hypothesis due to outcomes falling within the rejection region by chance; in terms of the null distribution, total probability that the outcome could fall within the rejection region given that the null hypothesis is true.    power  True positive rate of a test (i.e., probability that an outcome falls in the rejection region of the null distribution, given that the alternative hypothesis is true)    false negative rate/Type II error  Probability of incorrectly failing to reject the null hypothesis when an outcome from the alternative hypothesis distribution fails to fall within the rejection region of the null hypothesis.    specificity  Complement of false positive rate (Type I error); probability of a test failing to reject the null hypothesis when it is true.    power/sensitivity/true negative rate  Complement of false negative rate (Type II error); probability of correctly rejecting null hypothesis if the alternative hypothesis is true.    assumption of independence  Treating every observation in a dataset as if it has no influence on the outcomes of other observations (or at least none unaccounted-for in the model).    p-value hacking  Fallaciously ‘fishing’ for significant results by running tests until a small p-value is obtained by chance; this can be deliberate or inadvertently caused by a scattershot approach to testing.    hypothesis switching  Fallacy of generating and/or changing hypotheses for a set of known results until a significant result is obtained by chance.    family-wide error rate (FWER)  Probability of at least one false positive occurring in repeated tests. Assuming independent tests, this is the complement of the probability of only true positives occurring, and approaches 1.0 as the number of tests approaches infinity.    p-value histogram  Visualization to get a quick sense of p-value distribution of possible test outcomes for a null hypothesis. The distribution is a mixture of cases where the null hypothesis is rejected (small p-values) or retained (larger p-values).    false discovery rate (FDR)  The proportion of false positives among all cases where the null hypothesis is rejected across an entire distribution.    local false discovery rate (fdr)  The probability of Type I Error at a given p-value when the distribution of the p-values is treated as a mixture model of the null distribution and alternative hypothesis distribution. This varies based on the p-value, rather than being a property of the entire distribution.    tail-area false disovery rate (Fdr)  Integration-based extension of the local false discovery rate to obtain a false discovery rate for the entire distribution.    independent filtering  Method to increase test power by filtering variables with criteria that are independent under the null hypothesis, but correlated under the alternative    independent hypothesis weighting  A method of improving power of multiple testing by weighting hypotheses based on their power    Bonferroni adjustment  Method used to compensate for inflated Type I (false positive) error in multiple testing by dividing the test significance level/hypothesis threshold (e.g., alpha = 0.05) by the number of tests performed    whole genome sequencing  Method used to determine and record the DNA base values and order across all of an organism’s genes    marker gene  A gene used to determine membership in a group of interest (e.g., a taxon, genotype within a population, or possessing a certain metabolic trait)    expression level  The realtive abundance of transcriptions of a gene of interest present in, e.g., a cell or environment    reagent  a compound used in creating a chemical reaction like an assay    hypothesis testing  Evaluating whether outcomes are sufficently unlikely under the null hypothesis (holding that outcomes are determined fully by chance) that it can be rejected in favor of an alternative hypothesis    workflow  A sequence of steps used in carrying out a larger operation or process    two-sided test  A statistical test which rejects the null hypothesis if an observed test statistic is either too large or too small compared to that expected under the null hypothesis    one-sided test  A statistical test which rejects the null hypothesis if an observed test statistic departs from the expected range in a single, predetermined direction (i.e. larger or smaller)    two-sample  In the context of statistical testing, a situation whether the data belong to two known groups.    unpaired  In the context of statistical tests, these are used when comparing groups with independent measurements (e.g. the observations for one group have no association with observations from the other group)    equal variances  When groups being compared have (substantially) equivalent levels of variability.    dependence  When the outcomes of two variables are associated with one another.    expected value  For a random quantity, this is the value of the mean, i.e. “average value”.     Sources Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page Bourgon, R., Gentleman, R. \u0026amp; Huber, W. Independent filtering increases detection power for high-throughput experiments. Proceedings of the National Academy of Sciences 107, 9546–9551 (2010). Ignatiadis, N., Klaus, B., Zaugg, J. et al. Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nat Methods 13, 577–580 (2016). https://doi.org/10.1038/nmeth.3885 https://www.statisticssolutions.com/bonferroni-correction/ https://bioconductor.org/packages/release/bioc/vignettes/IHW/inst/doc/introduction_to_ihw.html https://www.statisticshowto.datasciencecentral.com/familywise-error-rate/ https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/ https://www.biostars.org/p/273537/\nPractice    ","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"daf9e88176b809bdbed22af1b1b580a4","permalink":"/post/vocabulary-for-chapter-6/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-6/","section":"post","summary":"Chapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n        Occam’s razor  Heuristic stating that the simplest explanation for a phenomenon is often the best    rejection region  Subset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.","tags":["Chapter 6","vocabulary"],"title":"Vocabulary for Chapter 6","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 5","quiz"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583445540,"objectID":"4ca9d7604bf3c82206fd30c69052f4e2","permalink":"/post/chapter-5-vocabulary-quiz/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/chapter-5-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["Chapter 5","quiz"],"title":"Chapter 5 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 4"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582825339,"objectID":"47162ba198ed97a383b0f7af77226d51","permalink":"/post/chapter-4-vocabulary-quiz/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/chapter-4-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 4"],"title":"Chapter 4 vocabulary quiz","type":"post"},{"authors":["Burton Karger"],"categories":["Chapter 5","vocabulary"],"content":"  Chapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n        unsupervised method  A learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.    status  A variable’s classification as an outcome/predictor (e.g. independent/dependent) in an analysis.    distance  A measure of the difference between two random variables.    The Euclidean distance  A distance metric equal to the “ordinary” straight-line distance between two points.    Manhattan distance  A distance metric equal to the sum of the absolute differences between the coordinate values for two points.    Maximum distance  A distance metric equal to the largest absolute difference between the coordinate values for two points.    Weighted Euclidean distance  A distance metric, which is a generalization of the ordinary Euclidean distance, that differentially weights the differences between the coordinate values for two points.    Minkowski distance  A distance metric equal to the mth root of the sum of the absolute differences between the coordinate values each raised to the mth power.    Edit or Hamming distance  A distance metric for comparing character sequences that counts the number of differences between two character strings.    Binary distance  A distance metric for binary strings based on the proportion of features having only one bit on amongst those features that have at least one bit on.    Jaccard distance  A distance metric that quantifies how dissimilar two sets are.    co-occurrence  The fact of two or more things occurring together or simultaneously.    Jaccard index  A statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets.    Jaccard dissimilarity  1 - the Jaccard index.    Correlation-based distance  A distance metric that measures two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.    Clusters of Differentiation (CDs)  At different stages of their development, immune cells express unique combinations of proteins on their surfaces.    Rectangular gating  A method of identifying groups of cells from a flow cytometry experiment using either a line (one-dimensional) or the quadrants created by two perpendicular lines (two-dimensional)    Hyperbolic Arcsine (asinh)  A transform function often preferred over the log tranform for flow cytometry data because it can be applied to negative values.    density-based clustering (dbscan)  The dbscan method clusters points in dense regions according to the density-connectedness criterion. It looks at small neighborhood spheres to see if points are connected.    curse of dimensionality  When the dimensionality increases, the volume of the space increases so fast that the available data become sparse    density-reachability  A fundamental criterion in dbscan that quantifies whether two points are close enough together and surrounded by sufficiently many other points.    recursive partitioning methods  A class of methods for dividing heterogeneous populations into more homogeneous subgroups, often used to make decision trees, that starts by separating the whole population into a few groups and iteratively continues separating each into subgroups.    minimal jump method/single linkage method/nearest neighbor method  A clustering method that computes the distance between clusters as the smallest distance between any two points in the two clusters.    maximum jump method/complete linkage method  A clustering method that defines the distance between clusters as the largest distance between any two objects in the two clusters.    average linkage method  A clustering method that defines the distance between clusters as the average distance between a point in one cluster and another point in the other cluster.    Ward’s method  A clustering method that takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to break the clusters up into ones of smaller sizes.    Within-groups sum of squares (WSS)  A measure of the variability among data points within an identified cluster.    Calinski-Harabasz index  Quantifies the relative variability between groups (between group sum of squares) and within groups (within-groups sum of sqaures), similar to the F statistic used in analysis of variance.    Between-groups sum of squares (BSS)  A measure of the variability between clusters.    gap statistic  A metric used to perform model selection which quantifies the amount of model fit improvement when using a more complex model. These can be used to select the number of clusters for a data set.    technical / batch effects  Depedence in data observations that results from technical differences between samples, such as the type of sequencing machine or the technician that ran the sample, rather than from scientifically interesting causes.    computational complexity  A measure of the computational resources needed to run an algorithm.    noise  Unexplained variability within a data sample.    operational taxonomic unit (OTU)  A method of clustering organisms based on DNA sequence similarity of a certain taxonomic marker gene.    bias  The tendency of a statistic to overestimate or underestimate a parameter.    representativeness heuristic  A method of learning or discovery that assesses similarity of objects and organizes them based around a category prototype (e.g., like goes with like, and causes and effects should resemble each other).    rare variants  An alternative form of a gene that occur just once or twice in an individual sample but more often across all samples.    insertion-deletion (indel)  insertion or deletion of bases in the genome of an organism.    neighboring cluster  The cluster with the lowest average dissimilarity to a given cluster.    silhouette index  A metric quantifying the degree to which a given data point belongs to its designated cluster.    Microbiome  The aggregate of all microbiota that reside on or within an organim’s tissues and biofluids along with the corresponding anatomical sites in which they reside.    filtering  in the context of low-quality rRNA reads removal of low-quality reads and trimming them to a consistent length    Histopathology  The microscopic examination of tissue in order to study the manifestations of disease.    Molecular signature  Sets of genes, proteins, genetic variants or other variables that can be used as markers for a particular phenotype    Gene expression data  Gene expression measurements : from gene¬scale to genome¬scale    Single-cell RNA-Seq experiment  a measurement of the gene expression profiles of individual cells.    gene transcript  An RNA molecule of defined size over the length of a gene.    cell lineage dynamics  Visualized with tools such as scRNA-seq to track individual cells through their natural progression.    Flow cytometry  A technique for identifying and sorting cells and their components (such as DNA) by staining with a fluorescent dye and detecting the fluorescence usually by laser beam illumination    Mass cytometry  A variation of flow cytometry in which antibodies are labeled with heavy metal ion tags rather than fluorochromes. Readout is by time-of-flight mass spectrometry.    Immune cells  cells that are part of the immune system and help the body fight infections and other diseases    CD marker / antigen marker  are specific types of molecules found on the surface of cells that help differentiate one cell type from another.    CD4  A glycoprotein found on the surface of immune cells such as T helper cells, monocytes, macrophages, and dendritic cells.    helper T cells  A type of T cell that provides help to other cells in the immune response by recognizing foreign antigens and secreting substances called cytokines that activate T and B cells    Isotope  Two or more forms of the same element that contain equal numbers of protons but different numbers of neutrons in their nuclei, and hence differ in relative atomic mass but not in chemical properties;    Inner cell mass (ICM)  Pluripotent cell lineage in the blastocyst. forms within the blastocyst, prior to its implantation within the uterus.    Blastocyst  A thin-walled hollow structure in early embryonic development that contains a cluster of cells called the inner cell mass from which the embryo arises.    Pluripotent epiblast (EPI)  The functional progenitors of soma and germ cells which later differentiate into three layers: definitive endoderm, mesoderm and ectoderm    primitive endoderm (PE)  The second extraembryonic tissue to form during embryogenesis in mammals. The PE develops from pluripotent cells of the blastocyst inner cell mass    variable regions  in the context of taxon identification of bacteria bacterial 16S ribosomal RNA (rRNA) genes contain nine “hypervariable regions” (V1 – V9) that demonstrate considerable sequence diversity among different bacteria.    Chimera  An organism or tissue that contains at least two different sets of DNA, most often originating from the fusion of as many different zygotes (fertilized eggs).     Sources Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page https://academic.oup.com/biolreprod/article/85/5/946/2530522 https://discovery.lifemapsc.com/in-vivo-development/inner-cell-mass/inner-cell-mass https://study.com/academy/lesson/inner-cell-mass-icm-definition-function-quiz.html https://www.sciencedirect.com/ https://www.medicinenet.com/ https://www.niaid.nih.gov/ https://iti.stanford.edu/ https://sysbiowiki.soe.ucsc.edu/node/323 https://www.statisticshowto.datasciencecentral.com/between-group-variation/ https://vsoch.github.io/2013/the-gap-statistic/   Practice   ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582321282,"objectID":"e98258856605bd61f466b24e8c688fd1","permalink":"/post/vocabulary-for-chapter-5/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-5/","section":"post","summary":"Chapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n        unsupervised method  A learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.    status  A variable’s classification as an outcome/predictor (e.","tags":["Chapter 5","vocabulary"],"title":"Vocabulary for Chapter 5","type":"post"},{"authors":["Amy Fox"],"categories":["vocabulary","Chapter 4"],"content":"  Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n        finite mixture  in the context of statistic, when the distribution of interest is a combination of a few different probability distributions    infinite mixture  in the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)    mixture model  a model for a combination of two or more different probability distributions    probability density function  a function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.    bimodal distribution  a distribution comprised of two modes    expectation-maximization (EM) algorithm  an algorithm that allows for parameter estimation in probabilistic models with incomplete data    data augmentation  adding variables that are not measured (latent variables) to the data    latent variables  variables not measured in the data    bivariate distribution  a combined distribution made of two random variables    mixture fraction  a fraction used to describe the inhomogeneity in the mixture composition    identifiability  an issue where there can be several explanations for the same observed values; occurs when there are too many degrees of freedom in parameters    marginal likelihood  the sum of the marginal distributions    expectation function  a function that calculates the average of all possible values of the group that an observation belongs to    maximization step  a step to optimize the parameters of a model    soft averaging  the process in which observations are not assigned to groups, rather they are added to multiple groups by using probabilities of memberships as weights    model averaging  the process of using several models and combining them together into a weighted model    zero-inflated data  data that contains a large number of zero counts    ChIP-Seq data  sequencing data that identifies DNA binding sites for proteins    chromosome  a DNA molecule that contains the genetic material of an organism    binding site  in the context of molecular biology, a specific region to which a macromolecule binds    deoxyribonucleotide monophosphate  a single phosphate group in a unit of DNA    gene expression measurement  the measurement of a functional gene product (i.e., protein or RNA)    microarray  a laboratory tool used to detect gene expression    promoter  in the context of genetics, a region of DNA that initiates transcription of a gene    point mass  a finite probabiliity concentrated at a point in the proability mass distribution at which there is a discontinuous segment in probability density function    sampling distribution  the probability distribution calculated from a random sample    empirical cumulative distribution function (ECDF)  a step distribution function based on empirical data measurements    density  in the context of probability distributions, the derivitive of the distribution function    bootstrap  an approximation of the true sampling distribution; created by drawing new samples from the empirical distribution of the original sample    non-parametric method  a statistical method that does not make assumptions about population distribution or sample size    nonparametric bootstrap  an approximation of the true sampling distribution not based off of a specific assumption or a particular model    Laplace distribution  a distribution that shows differences between two independent variates with identical exponential distributions    gamma distribution  a distribution that is positively valued and continuous with two parameters: shape and scale    negative binomial distribution/ gamma-Poisson distubtion  the probability distribution of the number of failures before the kth success in a sequence of Bernoulli trials    dispersion  the amount by which a set of observations deviate from their mean    variance-stabilizing transformations  transformations designed to give approximate independence between mean and variance    heteroscedasticity  the variance of the data is different in different regions of the data    delta method  a calculus procedure that uses random variables to approximate the expected value and variance of a function     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Zero-Inflated Poisson Regression. Institute for Digital Research and Education Statistical Consulting. https://stats.idre.ucla.edu/r/dae/zip/. Berrar, 2019. Introduction to Non-parametric Bootstrap. Research Gate. https://www.researchgate.net/ Do and Batzoglou, 2008. What is the expectaion maximization algorithm?. Nature Biotechnology. Wikipedia: The Free Encylcopedia. https://en.wikipedia.org/wiki/Main_Page Google Oxford American Dictionary. https://www.google.com d’Auzay, et al., 2019. Statistics of progress variable and mixture fraction gradients in an open turbulent jet spray flame. Fuel. Brownlee, 2019. A Gentle Introduction to Expectation-Maximization (EM Algorithm). Machine Learning Mastery. https://www.machinelearningmastery.com Non-parametric Methods. R tutorial. https://www.r-tutor.com Precise analysis of DNA–protein binding sequences. Illumina. https://www.illumina.com Microarray. Nature. https://www.nature.com   Practice   ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"379ab172062419319ec4efa5c87e334a","permalink":"/post/vocabulary-for-chapter-4/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-4/","section":"post","summary":"Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n        finite mixture  in the context of statistic, when the distribution of interest is a combination of a few different probability distributions    infinite mixture  in the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)    mixture model  a model for a combination of two or more different probability distributions    probability density function  a function giving the relative likelihood that a continuous random variable is equal to a given value.","tags":["vocabulary","Chapter 4"],"title":"Vocabulary for Chapter 4","type":"post"},{"authors":[],"categories":["quiz","Chapter 2"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582161527,"objectID":"ef44a84c1820dfdedb0a09c0a4ae4fb1","permalink":"/post/chapter-2-part-2-vocabulary-quiz/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/chapter-2-part-2-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 2"],"title":"Chapter 2, part 2, vocabulary quiz","type":"post"},{"authors":["Sere Williams"],"categories":["exercises","Chapter 2"],"content":" As always, load libraries first.\nlibrary(ggplot2) library(tidyverse) library(dplyr) Exercise 2.3 from Modern Statistics for Modern Biologists A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.) The code is redundant. But is the redundancy even among codons that code for the same amino acid? In other words, if alanine is coded by 4 different codons, do these codons code for alanine equally (each 25%), or do some codons appear more often than others? Here we use the tuberculosis genome to explore codon bias.\n a) Explore the data, mtb Use table to tabulate the AmAcid and Codon variables.\nEach amino acid is encoded by 1–6 tri-nucleotide combinations.\nmtb = read.table(\u0026quot;example_datasets/M_tuberculosis.txt\u0026quot;, header = TRUE) codon_no \u0026lt;- rowSums(table(mtb)) codon_no ## Ala Arg Asn Asp Cys End Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Thr Trp Tyr ## 4 6 2 2 2 3 2 2 4 2 3 6 2 1 2 4 6 4 1 2 ## Val ## 4 The PerThousands of each codon can be visualized, where each plot represents an amino acid and each bar represents a different codon that codes for that amino acid. But what does the PerThousands variable mean?\nggplot(mtb, aes(x=Codon, y=PerThous)) + geom_col()+ facet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1))  b) The PerThous variable How was the PerThous variable created?\nThe sum of all of the numbers of codons gives you the total number of codons in the M. tuberculosis genome: all_codons. Remember that this is not the size of the M. tuberculosis genome, but the number of codons in all M. tuberculosis genes. To get the size of the genome, multiply each codon by 3 (for each nucleotide) and add all non-coding nucleotides (which we do not know from this data set).\nall_codons = sum(mtb$Number) all_codons ## [1] 1344223 The PerThousands variable is derived by dividing the number of occurrences of the codon of interest by the total number of codons. Because this number is small and hard to interpret, multiplying it by 1000 gives a value that is easy to make sense of. Here is an example for proline. The four values returned align to the four codons that each code for proline.\npro = mtb[mtb$AmAcid == \u0026quot;Pro\u0026quot;, \u0026quot;Number\u0026quot;] pro / all_codons * 1000 ## [1] 31.560240 6.121752 3.405685 17.032144  c) Codon bias Write an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.\nFirst, let’s look at the expected frequencies of each codon.\ncodon_expected \u0026lt;- data.frame(codon_no) %\u0026gt;% rownames_to_column(var = \u0026quot;AmAcid\u0026quot;) %\u0026gt;% mutate(prob_codon = 1/codon_no) codon_expected ## AmAcid codon_no prob_codon ## 1 Ala 4 0.2500000 ## 2 Arg 6 0.1666667 ## 3 Asn 2 0.5000000 ## 4 Asp 2 0.5000000 ## 5 Cys 2 0.5000000 ## 6 End 3 0.3333333 ## 7 Gln 2 0.5000000 ## 8 Glu 2 0.5000000 ## 9 Gly 4 0.2500000 ## 10 His 2 0.5000000 ## 11 Ile 3 0.3333333 ## 12 Leu 6 0.1666667 ## 13 Lys 2 0.5000000 ## 14 Met 1 1.0000000 ## 15 Phe 2 0.5000000 ## 16 Pro 4 0.2500000 ## 17 Ser 6 0.1666667 ## 18 Thr 4 0.2500000 ## 19 Trp 1 1.0000000 ## 20 Tyr 2 0.5000000 ## 21 Val 4 0.2500000 Next, calculate the observed frequencies for each codon seen in the data set and use the chi-squared test statistic to determine if the difference between expected and observed codon frequencies is even or if some codon sequences are used more than others.\nTo start, you can group the data by amino acid and then determine a few things about the amino acid or the possible codons for it, including the total observations across all codons for the amino acid (total), the number of codons for that amino acid (n_codons), and the expected count for each codon for that amino acid (the total number of observations for that amino acid divided by the number of codons, giving an expected number that’s the same for all codons of an amino acid; expected).\ncodon_compared \u0026lt;- mtb %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(total = sum(Number), n_codons = n(), expected = total / n_codons) codon_compared ## # A tibble: 64 x 7 ## # Groups: AmAcid [21] ## AmAcid Codon Number PerThous total n_codons expected ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Gly GGG 25874 19.2 132810 4 33202. ## 2 Gly GGA 13306 9.9 132810 4 33202. ## 3 Gly GGT 25320 18.8 132810 4 33202. ## 4 Gly GGC 68310 50.8 132810 4 33202. ## 5 Glu GAG 41103 30.6 62870 2 31435 ## 6 Glu GAA 21767 16.2 62870 2 31435 ## 7 Asp GAT 21165 15.8 77852 2 38926 ## 8 Asp GAC 56687 42.2 77852 2 38926 ## 9 Val GTG 53942 40.1 114991 4 28748. ## 10 Val GTA 6372 4.74 114991 4 28748. ## # … with 54 more rows The mutate function is used after group_by to do all this within each amino acid group of codons, but without collapsing to one row per amino acid, as a summarize call would.\nTo convince yourself that this has worked out correctly, you can repeat the plot we made before and see that the bars for the expected values are always equal across all codons for an amino acid:\nggplot(codon_compared, aes(x=Codon, y=expected)) + geom_col()+ facet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Finally, we can calculate the chi-squared (\\(\\chi^2\\)) statistic and compare it to the chi-squared distribution to get the p-value when testing against the null hypothesis that the amino acid observations are uniformly distributed across codons. The \\(\\chi^2\\) is calculated as:\n\\[ \\chi^2 = \\sum_i{\\frac{(O_i-E_i)^2}{E_i}} \\]\nwhere:\n \\(O_i\\) is the observed value of data point \\(i\\) (Number in our data); and \\(E_i\\) is the expected value of data point \\(i\\) (expected in our data)  In our data, we can calculate the contribution to the total \\(\\chi^2\\) statistic from each data point (in this case, each codon within an amino acid) using mutate, and then add these values up using group_by to group by amino acid followed by summarize to sum up across all the data points for an amino acid. The other information we need to get is the number of codons for the amino acid, because we’ll need this to determine the degrees of freedom for the chi-squared distribution. Next, we used mutate with pchisq to determine the p-values within each amino acid group for the test against the null that the codons are uniformly distributed for that amino acid (i.e., that there isn’t codon bias). These p-values turn out to be super small, so we’re using a technique to get the log-transform versions of them instead, which we explain a bit more later. Finally, we used arrange to list the amino acids by evidence against uniform distribution of the codons, from most evidence against (smallest p-value so most negative log(p-value)) to least evidence against (although still plenty of evidence against) and added an index with the ranking for each codon by adding a column with the sequence of numbers from 1 to the number of rows in the data (n()).\ncodon_compared %\u0026gt;% filter(n_codons \u0026gt; 1) %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(chi_squared = ((Number - expected)^2/expected)) %\u0026gt;% summarise(chi_squared = sum(chi_squared), n = n()) %\u0026gt;% mutate(p_value = pchisq(chi_squared, df = n-1, log = TRUE, lower.tail = FALSE)) %\u0026gt;% arrange(p_value) %\u0026gt;% mutate(rank = 1:n()) ## # A tibble: 19 x 5 ## AmAcid chi_squared n p_value rank ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Leu 135432. 6 -67700. 1 ## 2 Ala 75620. 4 -37805. 2 ## 3 Arg 72183. 6 -36076. 3 ## 4 Thr 58767. 4 -29378. 4 ## 5 Val 58737. 4 -29363. 5 ## 6 Ile 56070. 3 -28035. 6 ## 7 Gly 52534. 4 -26262. 7 ## 8 Pro 45400. 4 -22695. 8 ## 9 Ser 36742. 6 -18357. 9 ## 10 Asp 16208. 2 -8109. 10 ## 11 Phe 13444. 2 -6727. 11 ## 12 Asn 11404. 2 -5707. 12 ## 13 Gln 9376. 2 -4693. 13 ## 14 Lys 6382. 2 -3195. 14 ## 15 Glu 5947. 2 -2978. 15 ## 16 His 5346. 2 -2678. 16 ## 17 Tyr 4738. 2 -2373. 17 ## 18 Cys 2958. 2 -1483. 18 ## 19 End 928. 3 -464. 19 As you may notice, these log transforms of the p-values (which we got rather than untransformed p-values in the pchisq call because we used the option log = TRUE) are large in magnitude and negative (so very tiny once you take the exponent if you re-transformed them to p-values) values. If you tried to calculate the untransformed p-values (and we did!), this number is so small (0.00000000e+00) that it is too small for R—it shows up as exactly zero in R, even though it actually is a very tiny, but still non-zero, number. To get around this issue, we told pchisq to work on these p-values as log transforms, and then we left the p-value as that log-transformed value. A group of numbers that are log transformed will be in the same order as their untransformed versions, so we don’t need to convert back to figure out which amino acid had that smallest p-value. We can just sort the amino acids from most negative to less negative using these log-transformed versions of the p-values. We now have the amino acids ranked from most biased codons (1) to least (19).\n ","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582151148,"objectID":"40f5d68248e48973d3f72a984ce57de5","permalink":"/post/exercise-solution-for-chapter-2/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-2/","section":"post","summary":"As always, load libraries first.\nlibrary(ggplot2) library(tidyverse) library(dplyr) Exercise 2.3 from Modern Statistics for Modern Biologists A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.","tags":["exercises","Chapter 2"],"title":"Exercise solution for Chapter 2, Part 1","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 2"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581612248,"objectID":"7c8103a2670fda3b5cb4a7557b69a29c","permalink":"/post/chapter-2-vocabulary-quiz/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/chapter-2-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 2"],"title":"Chapter 2 Part 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","blogdown","exercises","github"],"content":" Each of you will be responsible once or twice over the semester to create a blog post that provides a clean, clearly-presented solution to the in-class exercise for the week. This blog post provides the technical instructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting. Since it will need to be reviewed by the faculty before it can be officially posted, please plan to submit it by the Tuesday after the class for your exercise. Student assignments for the exercises are given in the Schedule section of our course website.\nOverview of creating a post You will be submitting your exercise solution as a blog post. Creating one for our website will follow all the same steps as creating a blog post for a vocabulary list, just with different content. Please read the post on creating a vocabulary list and follow the steps there to:\n Update your fork of the website Make a new blog post Use RMarkdown syntax to write the blog post Submit the blog post   Content for the blog post The blog post should provide a walk-through of the solution to that week’s in-course exercise. We have posted an example for the exercise for Chapter 1 to give you an idea of what you should aim to write.\nGenerally, this exercise will be a resource for everyone in the class, to make sure they’ve understood the exercise, as well as to see how someone else tackled the problem. Your solution should cover all parts of the exercise (for example, if there’s a part A and B, you should cover both). You can start by writing it as you would if you were assigned the exercise as a homework problem, but then you should do a second step of revision to provide some context and dig a bit deeper into how you tackled the question. Since we are only requiring you to write up exercise answers once or twice over the semester (rather than submitting homework for exercises every week), we expect this product to be more in-depth and polished than a typical homework solution.\nFirst, make sure that you have provided text explaining what the exercise asks for, in case the reader hasn’t recently read the exercise prompt. Second, please add a few details either about how you tackled the problem through code or how the statistical principles covered in the exercise could apply to other problems you’ve come across in your research or coursework.\nTo help in preparing your post, plan to spend the exercise time in class during the week of your exercise visiting the different groups of students working on the exercise. You can talk to them about how they’re approaching the problem, how they interpret it, etc., to help you develop your own answer.\n Tips  Be sure to refresh yourself on all the Markdown formatting tags you can use to improve the appearance of your post. Be sure to include things like section headings and italics or bold as appropriate. RStudio’s website has some nice cheatsheets on RMarkdown that can help. Make sure you include R code if appropriate. If you put parentheses around an assignment expression in R, it will print out the assigned object and make the assignment in the same call—you might find this useful in writing concise code while still showing what’s in the objects you create. Use the $ and $$ tags in RMarkdown to include mathematical equations in your blog post when appropriate. If you need to read in a dataset for R code in your blog post, save it in the website directory’s “content/post/example_datasets” subdirectory. If your data comes from an online source or from an R library, you won’t need to do this, only if you need a “local” copy of the datafile to run your RMarkdown code. You are welcome to draw from (and cite) other statistics textbooks or dictionaries if you’d like to in explaining the problem and your approach to it. For the code, look at vignettes and helpfiles, especially for packages you are not familiar with. For a lot of Bioconductor packages, object-oriented programming is used pretty heavily. This means that associated data in R packages will often be stored in a format that you haven’t used yet. Look up more information on data classes used in your exercise if you aren’t familiar with them. You can use the class function to determine the class of an object as well as the name of the package that defines that class. The str function is often helpful for exploring a data object class, as well. Many of the Bioconductor object classes will have special accessor methods, which are functions that allow you to extract certain elements from the object—check the helpfile for the object class, as these methods are often listed there with examples. Googling can also be very helpful for learning more about functions, packages, and datasets in R, especially if you don’t yet know what package the item is from. Most Bioconductor packages have very nice vignettes available online and from your R session once you have installed the package. These are a great place to start to find out more about how to use the functions and object classes that come with the package.   ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477683,"objectID":"459571b5f2972f118129cd8d9a5201a1","permalink":"/post/how-to-create-an-exercise-solution-blog-post/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/how-to-create-an-exercise-solution-blog-post/","section":"post","summary":"Each of you will be responsible once or twice over the semester to create a blog post that provides a clean, clearly-presented solution to the in-class exercise for the week. This blog post provides the technical instructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting. Since it will need to be reviewed by the faculty before it can be officially posted, please plan to submit it by the Tuesday after the class for your exercise.","tags":["instructions","blogdown","exercises","github"],"title":"How to create an exercise solution blog post","type":"post"},{"authors":["Sierra Pugh"],"categories":["Chapter 2","vocabulary"],"content":"  These sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n        Markov chain  a sequence where given the current state, the next state is conditionally independent of all previous states    Bayesian paradigm  approaching statistics from the perspective that probability can be viewed as a degree of belief in an event    Beta distribution  a probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics    Exponential distribution  a probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process    Prior  a probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data    Posterior  a probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data    Haplotype  a collection of DNA sequence variants (e.g., alleles) that are spatially close on a chromosome, are usually inherited together, and thus are genetically linked    Marginal distribution  the distribution of a sub-collection of variables after integrating out the remaining variables in the collection.    Monte Carlo integration  a technique for numerical integration where the value of an integral is estimated by simulating data    Quantile-quantile plot (QQ-plot)  a plot comparing the quantiles from one distribution (often a theoretical distribution) to the quantiles of another distribution (often from a sample)    Maximum a posteriori (MAP) estimate  the mode of the posterior distribution associated with the quantity of interest    Escherichia coli  facultative anaerobic, rod-shaped, coliform bacterium commonly found in the lower intestine of warm-blooded organisms    Epigenetics  the study of heritable phenotype changes that do not involve alterations in the DNA sequence    Log-likelihood ratio  the log of the likelihood under one set of assumptions divided by the likelihood under another set of assumptions    Bimodality  when a distribution has two modes    Mixture  in the context of statistics, when the distribution of interest is a combination of two or more different probability distributions    Codon  A three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis)    Codon bias  the differences in how often each spelling of an amino acid occurs in coding DNA    Genetic code  the set of instructions in a gene that tell the cell how to make a specific protein     Sources consulted or cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page NIH Genetics Home Reference. https://ghr.nlm.nih.gov/ NCBI Genetics Review. https://www.ncbi.nlm.nih.gov   Practice   ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581691815,"objectID":"6cf732c0340c1c0a7b23cd9a5e5104b6","permalink":"/post/vocabulary-for-chapter-2-8-2-12/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-2-8-2-12/","section":"post","summary":"These sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n        Markov chain  a sequence where given the current state, the next state is conditionally independent of all previous states    Bayesian paradigm  approaching statistics from the perspective that probability can be viewed as a degree of belief in an event    Beta distribution  a probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics    Exponential distribution  a probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process    Prior  a probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data    Posterior  a probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data    Haplotype  a collection of DNA sequence variants (e.","tags":["Chapter 2","vocabulary"],"title":"Vocabulary for Chapter 2, Part 2","type":"post"},{"authors":["Brooke Anderson"],"categories":["exercises","Chapter 1"],"content":" This exercise asks us to explore the frequency of each of the four nucleotides (A, C, G, and T) in the genome of C. elegans, a type of worm used frequently in scientific research.\nThis solution requires that several R extension packages be loaded in your R session. If you do not have these packages installed to your computer yet, you should follow instructions we’ve posted separately describing the required set-up for this exercise. Once you have installed these packages on your computer, you can load them into your current R session using the library function:\nlibrary(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;) library(\u0026quot;Biostrings\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;knitr\u0026quot;) Part A Part A of the question asks us to explore the nucleotide frequency of the C. elegans genome. This genome is available in the Celegans data that comes with the BSgenome.Clegans.UCSC.ce2 package and is stored within a BSgenome class, which is a special object class provided by the Biostrings package.\nThere is a dedicated function called letterFrequency in the Biostrings package that can be used to count the frequency of letters in a string (like a genome) in an R object like this. In a call to this function, you must also include the possible letters in your “alphabet”—that is, the possible letters that each position in your string could take.\n(nuc_freq \u0026lt;- letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;))) ## A C G T ## 4335 1225 2055 6179 To explore and plot this data, I put this summary data into a tibble, so I could more easily use tidyverse tools with the data.\nnuc_freq_df \u0026lt;- tibble(nucleotide = names(nuc_freq), n = nuc_freq) nuc_freq_df ## # A tibble: 4 x 2 ## nucleotide n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 A 4335 ## 2 C 1225 ## 3 G 2055 ## 4 T 6179 In this format, you can use tidyverse tools to explore the data a bit more. For example, you can determine the total number of nucleotides in the genome and, with that calculate the proportion of each nucleotide across the genome. Along with the kable function from the knitr package, I created a formatted table with this information:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n)) %\u0026gt;% kable(digits = 2, caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;, col.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency\u0026quot;, \u0026quot;Proportion\u0026quot;))  Table 1: Nucleotide frequencies and proportions in C. elegans  Nucleotide Frequency Proportion    A 4335 0.31  C 1225 0.09  G 2055 0.15  T 6179 0.45    For some presentations, it might be clearer to present this information in a slightly different table format, using pivot_longer and then pivot_wider to reformat the table for presentation:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n), n = prettyNum(n, big.mark = \u0026quot;,\u0026quot;), prop = prettyNum(prop, digits = 2)) %\u0026gt;% pivot_longer(cols = c(\u0026quot;n\u0026quot;, \u0026quot;prop\u0026quot;)) %\u0026gt;% pivot_wider(names_from = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate(name = case_when( name == \u0026quot;n\u0026quot; ~ \u0026quot;Frequency of nucleotide\u0026quot;, name == \u0026quot;prop\u0026quot; ~ \u0026quot;Proportion of all nucleotides\u0026quot; )) %\u0026gt;% rename(` ` = name) %\u0026gt;% kable(align = c(\u0026quot;rcccc\u0026quot;), caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;)  Table 2: Nucleotide frequencies and proportions in C. elegans   A C G T    Frequency of nucleotide 4,335 1,225 2,055 6,179  Proportion of all nucleotides 0.31 0.089 0.15 0.45    Here is a plot of the frequency of each of the four nucleotides for the C. elegans nucleotide:\nggplot(nuc_freq_df, aes(x = nucleotide, y = n)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = expression(paste(italic(\u0026quot;C. elegans\u0026quot;), \u0026quot; neucleotide frequency\u0026quot;)), caption = expression(paste(\u0026quot;Based on data from the \u0026quot;, italic(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;), \u0026quot; package.\u0026quot;))) This graph uses a few elements to improve its appearance that you might want to explore if you’re not already familiar with them:\n The labs function is used to add both a title and a caption to the plot. The paste, expression, and italic functions are used together to put “C. elegans” and an R package name in italics in some of the labels on the plot. The scales package is used inside a scale layer for the ggplot2 code to make the y-axis labels a bit nicer. theme calls are used to apply a simpler overall theme than the default and to remove the x- and y-axis titles (with element_blank). The color and fill of the bars are customized in the geom layer (geom_col).  From this plot, it certainly looks like the nucleotides are not uniformly distributed in the C. elegans genome. This question will be investigated more in the next part of the exercise.\n Part B The second part of the exercise asks us to test whether the observed nucleotide data for C. elegans is consistent with the uniform model that all nucleotide frequencies are the same.\nFirst, we can simulate several datasets under this null model and see how a plot of nucleotide frequencies compares to the plot that we obtained with the observed C. elegans data. To make these plots, I first simulated 20 samples under the null model that the distribution is uniform across the four nucleotides, using the rmultinom function with the size argument set to the number of nucleotides in the original C. elegans genome data and the prob argument set to have an equal probability of each nucleotide at each spot on the genome:\n(sim_nuc_freq \u0026lt;- rmultinom(n = 20, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 3397 3447 3540 3504 3396 3506 3369 3451 3463 3475 3451 3484 3467 3419 ## [2,] 3523 3442 3360 3478 3549 3442 3507 3441 3459 3368 3457 3393 3420 3593 ## [3,] 3430 3456 3472 3377 3412 3467 3436 3431 3417 3522 3443 3430 3462 3461 ## [4,] 3444 3449 3422 3435 3437 3379 3482 3471 3455 3429 3443 3487 3445 3321 ## [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 3387 3466 3487 3455 3564 3448 ## [2,] 3406 3439 3496 3489 3323 3434 ## [3,] 3546 3341 3398 3418 3476 3440 ## [4,] 3455 3548 3413 3432 3431 3472 Next, I moved this into a tibble so I could more easily rearrange and plot the data using facetting in ggplot2:\nsim_nuc_freq_df \u0026lt;- as_tibble(sim_nuc_freq) %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide) sim_nuc_freq_df %\u0026gt;% slice(1:10) ## # A tibble: 10 x 3 ## nucleotide sample value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 A 1 3397 ## 2 C 1 3523 ## 3 G 1 3430 ## 4 T 1 3444 ## 5 A 2 3447 ## 6 C 2 3442 ## 7 G 2 3456 ## 8 T 2 3449 ## 9 A 3 3540 ## 10 C 3 3360 ggplot(sim_nuc_freq_df, aes(x = nucleotide, y = value)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = \u0026quot;Simulated neucleotide frequencies under a uniform model\u0026quot;) + facet_wrap(~ sample) + expand_limits(y = max(nuc_freq_df$n)) The y-axis limits were expanded here to cover the same range as that shown for the observed C. elegans nucleotide frequencies, to help make it easier to compare these plots with the plot of our observed data. These plots of data simulated under the null model do show some variation in frequencies among the nucleotides, but it’s certainly much less than in the observed data for C. elegans.\nNext, I repeated this simulation process, but I increased the number of simulations to 1,000:\nsim_nuc_freq_df \u0026lt;- rmultinom(n = 1000, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4)) %\u0026gt;% as_tibble() %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide) sim_nuc_freq_df %\u0026gt;% slice(1:10) ## # A tibble: 10 x 3 ## nucleotide sample value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 A 1 3427 ## 2 C 1 3456 ## 3 G 1 3471 ## 4 T 1 3440 ## 5 A 2 3429 ## 6 C 2 3492 ## 7 G 2 3421 ## 8 T 2 3452 ## 9 A 3 3451 ## 10 C 3 3438 Using this dataframe of simulations, we can measure the mean, minimum, and maximum frequencies of each nucleotide across all 1,000 simulations:\n(sim_summary \u0026lt;- sim_nuc_freq_df %\u0026gt;% group_by(nucleotide) %\u0026gt;% summarize(mean_freq = mean(value), min_freq = min(value), max_freq = max(value))) ## # A tibble: 4 x 4 ## nucleotide mean_freq min_freq max_freq ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 A 3449. 3281 3602 ## 2 C 3446. 3272 3588 ## 3 G 3452. 3301 3652 ## 4 T 3447. 3266 3593 To help compare this with the observed data, we can create a table with information from both the original data and the simulations under the null model:\nnuc_freq_df %\u0026gt;% left_join(sim_summary, by = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate_at(c(\u0026quot;mean_freq\u0026quot;, \u0026quot;min_freq\u0026quot;, \u0026quot;max_freq\u0026quot;, \u0026quot;n\u0026quot;), prettyNum, big.mark = \u0026quot;,\u0026quot;, digits = 0) %\u0026gt;% mutate(simulations = paste0(mean_freq, \u0026quot; (\u0026quot;, min_freq, \u0026quot;, \u0026quot;, max_freq, \u0026quot;)\u0026quot;)) %\u0026gt;% select(nucleotide, n, simulations) %\u0026gt;% kable(col.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency in C. elegans genome\u0026quot;, \u0026quot;Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations\u0026quot;), align = \u0026quot;c\u0026quot;)   Nucleotide Frequency in C. elegans genome Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations    A 4,335 3,449 (3,281, 3,602)  C 1,225 3,446 (3,272, 3,588)  G 2,055 3,452 (3,301, 3,652)  T 6,179 3,447 (3,266, 3,593)    This helps clarify how unusual the observed data would be under the null model—the counts of all four nucleotides in the C. elegans genome are completely outside the range of frequencies in the simulated data.\nAnother way to look at this is with histograms of the distribution of frequencies of each nucleotide under the null model compared to the observed frequencies in the C. elegans nucleotide:\nggplot(sim_nuc_freq_df, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~ nucleotide) + theme_classic() + scale_x_continuous(name = \u0026quot;Frequency of nucleotide in the simulation under the null model\u0026quot;, labels = scales::comma) + scale_y_continuous(name = \u0026quot;# of simulations (out of 1,000)\u0026quot;) + geom_vline(data = nuc_freq_df, aes(xintercept = n), color = \u0026quot;red\u0026quot;) + labs(title = expression(paste(\u0026quot;Nucleotide frequency in \u0026quot;, italic(\u0026quot;C. elegans\u0026quot;), \u0026quot; compared null model simulations\u0026quot;)), caption = \u0026quot;Red line shows the frequency observed for the nucleotide in C. elegans\u0026quot;) Finally, to help in answering this question, it would be interesting to look at a single measure for each simulation (and for the observed data) rather than comparing each nucleotide one at a time. Chapter 1 gives the equation for a statistic to measure variability in multinomial data by calculating the sum of squares for the differences between the observed and expected count of nucleotides for each of the four nucleotides in a sample (p. 12).\nI calculated this statistic for the observed data and then for each of the 1,000 simulations.\n(obs_stat \u0026lt;- nuc_freq_df %\u0026gt;% mutate(expected = mean(n), stat_input = (n - expected) ^ 2 / expected) %\u0026gt;% summarize(variability_stat = sum(stat_input))) ## # A tibble: 1 x 1 ## variability_stat ## \u0026lt;dbl\u0026gt; ## 1 4387. sim_stat \u0026lt;- sim_nuc_freq_df %\u0026gt;% mutate(expected = mean(value), stat_input = (value - expected) ^ 2 / expected) %\u0026gt;% group_by(sample) %\u0026gt;% summarize(variability_stat = sum(stat_input)) sim_stat %\u0026gt;% slice(1:5) ## # A tibble: 5 x 2 ## sample variability_stat ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.318 ## 2 2 0.882 ## 3 3 0.903 ## 4 4 0.476 ## 5 5 1.22 Here is a plot of the distribution of this statistic across the 1,000 simulations:\nggplot(sim_stat, aes(x = variability_stat)) + geom_rect(data = sim_stat, aes(xmin = quantile(variability_stat, prob = 0.025), xmax = quantile(variability_stat, prob = 0.975), ymin = 0, ymax = Inf), fill = \u0026quot;beige\u0026quot;, alpha = 0.5) + geom_histogram(bins = 30, fill = \u0026quot;white\u0026quot;, color = \u0026quot;tan\u0026quot;, alpha = 0.5) + theme_classic() + labs(title = \u0026quot;Variability from expected values\u0026quot;, subtitle = \u0026quot;Values from simulations under the null\u0026quot;, x = \u0026quot;Value of variability statistic\u0026quot;, y = \u0026quot;Number of simulations with given value\u0026quot;, caption = \u0026quot;The shaded yellow area shows the region of the central 95% of\\nstatistic values for the 1,000 simulations under the null model.\u0026quot;) The value of this statistic for the observed nucleotide frequencies for C. elegans is 4387, which is much larger (indicating greater variability from expected values under the null model) than the value observed under most of the simulations. It is, in fact, far outside the central 95% range of values observed in simulations.\n ","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477727,"objectID":"22f4b300c757465ac6d281137e95056c","permalink":"/post/exercise-solution-for-chapter-1/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-1/","section":"post","summary":"This exercise asks us to explore the frequency of each of the four nucleotides (A, C, G, and T) in the genome of C. elegans, a type of worm used frequently in scientific research.\nThis solution requires that several R extension packages be loaded in your R session. If you do not have these packages installed to your computer yet, you should follow instructions we’ve posted separately describing the required set-up for this exercise.","tags":["exercises","Chapter 1"],"title":"Exercise solution for Chapter 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 2","vocabulary"],"content":"  The first portion of Chapter 2 (2.1-2.7) is focused on statistical modeling of data. It introduces a number of distributions commonly used in statistics, as well as model fitting estimation procedures (e.g. maximum likelihood estimation).\nThe vocabulary words for Chapter 2, part 1, are:         statistical inference / up / statistical approach  An upward-reasoning approach that start with data and works towards defining a model that might possibly explain the data.    deduction  Starting from a mathematical/statistical model with known parameters and computing the probability of observing an event.    null model  The model associated with the null hypothesis, which formulates an “uninteresting” baseline.    goodness-of-fit  Evaluation of whether a theorectical distribution/model is appropriate for a data set.    rootogram  Diagram to assess model goodness-of-fit for a data set. Bar chart where the bars “hang” from their theorectical values and will approximately line up with horizontal axis if the model is a good fit to the data.    maximum likelihood estimator (MLE)  A rule, or mathematical formula, that outputs an estimate of a parameter for a model, where that estimate maximizes the probability of the observed data.    conservative (approach)  An analysis approach that errs on the side of caution to avoid concluding an alternative hypothesis (e.g. detecting a signal) when it is not true.    vectorization  In regard to function evaluation, if a vector is supplied to a function that expects a scalar, R will apply the function to each element of the vector.    likelihood function  The probability of the data under a model expressed as a function of the model parameter(s).    estimation  Process of using data to perform inference on population parameters.    statistical testing  Formal decision process to determine if a null model is appropriate for the observed data.    regression  Relating how an outcome measure depends on one or more covariates.    residual  Deviation between the observed data and the expected value of the data point according to a model.    generalized linear model  A class of models for non-continuous or non-negative data that allows regression of an outcome on observed covariates. An extension of linear regression.    chi-squared distribution  A distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).    quantile-quantile (QQ) plot  Used to compare two distributions (or samples). Deviations in the plot from the y=x line suggest differences between the two distributions.    quantile  Value corresponding to a percentile of a distribution.    empirical cumulative distribution function (ECDF)  Function with input value x gives as output the probability that a random variable from the distribution is less than or equal to x. Function is defined using a sample and assigning probability 1/n to each data point.    chi-squared statistic  A summary statistic of a data set that has a theorectical chi-squared distribution.    base pairing  The pattern that adenine (A) and thymine (T) are paired (appear with equal frequency) in the DNA of an organism, and similarly cytosine (C) and guianine (G) are paired.    contingency table  Table of counts summarizing the number of times combinations of factor levels were observed in the data set.    Hardy-Weinberg equilibrium (HWE)  Assuming random mating, this principle characterizes the distribution of genotype frequencies as a function of the relative frequencies of each allele.    position weight matrix (PWM) / position-specific scoring matrix (PSSM)  Table giving the probability of each nucleotide at each position    sequence logo  A graphical summary of the position weight matrix or position-specific scoring matrix.     Practice   ","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581281287,"objectID":"2788b54b4b00409516daea264740162d","permalink":"/post/vocabularly-for-chapter-2-part-1/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/post/vocabularly-for-chapter-2-part-1/","section":"post","summary":"Vocabulary for the first part of Chapter 2","tags":["Chapter 2","vocabulary"],"title":"Vocabularly for Chapter 2, Part 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 1","exercises"],"content":" The code instructions in the exercise statement appear to be outdated. The code below worked on my machine. Note that when asked whether I would like to update packages from the binary version, I said no. (When I said yes, R gave an error.)\nif (!requireNamespace(\u0026quot;BiocManager\u0026quot;, quietly = TRUE)) install.packages(\u0026quot;BiocManager\u0026quot;) BiocManager::install(c(\u0026quot;Biostrings\u0026quot;, \u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;,\u0026quot;BSgenome\u0026quot;)) You can see the various data genome data sets available by loading the BSgenome library and typing available.genomes().\nOnce you have the needed packages installed, you can access the sequence data for this exercise via the following commands.\nsuppressMessages(library(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;)) Celegans ## Worm genome: ## # organism: Caenorhabditis elegans (Worm) ## # provider: UCSC ## # provider version: ce2 ## # release date: Mar. 2004 ## # release name: WormBase v. WS120 ## # 7 sequences: ## # chrI chrII chrIII chrIV chrV chrX chrM ## # (use \u0026#39;seqnames()\u0026#39; to see all the sequence names, use the \u0026#39;$\u0026#39; or \u0026#39;[[\u0026#39; operator ## # to access a given sequence) seqnames(Celegans) ## [1] \u0026quot;chrI\u0026quot; \u0026quot;chrII\u0026quot; \u0026quot;chrIII\u0026quot; \u0026quot;chrIV\u0026quot; \u0026quot;chrV\u0026quot; \u0026quot;chrX\u0026quot; \u0026quot;chrM\u0026quot; Celegans$chrM ## 13794-letter \u0026quot;DNAString\u0026quot; instance ## seq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA class(Celegans$chrM) ## [1] \u0026quot;DNAString\u0026quot; ## attr(,\u0026quot;package\u0026quot;) ## [1] \u0026quot;Biostrings\u0026quot; length(Celegans$chrM) ## [1] 13794 The Biostrings packages provides functions to summarize the sequence. For example:\nlibrary(\u0026quot;Biostrings\u0026quot;) lfM = letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) lfM ## A C G T ## 4335 1225 2055 6179 sum(lfM) ## [1] 13794 ","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581008738,"objectID":"8d16e489e5ef46020884190c7e09cad2","permalink":"/post/chapter-1-exercise-setup/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-exercise-setup/","section":"post","summary":"Instructions on how to get started on Chapter 1, exercise 1.8.","tags":["Chapter 1"],"title":"Chapter 1 exercise setup","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 1"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580933164,"objectID":"3bb9cd5326655191aa8e191c887845d2","permalink":"/post/chapter-1-vocabulary-quiz/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 1"],"title":"Chapter 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","vocabulary","blogdown","github"],"content":" As one of your assignments for this class, you are responsible for creating a blog post with all the vocabulary and definitions for one week of the course. This blog post will explain how you can create and publish that blog post on our course website.\nCreate the blog post Update your fork of the website You should have already forked our website to add your details for the “Person” section. You can use this same fork to add your blog post, but you should make sure you get your fork up-to-date with the current version of the website before you do.\nA fork of a repository does not stay up-to-date with the original repository it copied by itself. Instead, unless you update it, it will continue to be a snapshot of the original repository (plus any changes you’ve made to your copy) as of the time when you forked it. If the original has made a lot of changes since you made your fork, it might be very hard to make a clean pull request as there will be (potentially) lots of conflicts because of changes made to the original. It’s considered polite to make sure that you’re working with an up-to-date fork of a repository if you want to make a pull request back to the original.\nTo update your fork of the original repository, open your “csu_msmb.Rproj” file to open our website’s R Project on your computer. This should open RStudio with the website’s project open (check the top right corner of your RStudio window to confirm—it should say “csu_msmb”).\nThere is a little blue gear symbol in the “Git” pane in RStudio. Click on the down arrow to the right of it and select “Shell…”. This should open a bash shell on your computer. (If your computer uses Windows, there’s a chance that it might open something other than a bash shell. In that case, you can change your preferences in RStudio to reconfigure to always use a bash shell terminal when you ask for a shell from RStudio.)\nIn this shell, you need to run two git commands. First, you’ll add a remote branch to your repository. You already have one remote branch called “origin”—that’s the GitHub repository that you have in your account, which you forked from the original. Now you’ll add the original (the GitHub repository in my account) as another remote branch. Each branch has its own name, and you can use that name to refer to it in later git commands. The convention, if you add an original repository that you forked from as a remote, is to name that remote branch “upstream”. Run the following code in your bash shell to add the original GitHub repository as a remote branch with the name “upstream”:\ngit remote add upstream git@github.com:geanders/csu_msmb.git Now that you have added the original as a remote, you can pull in any commits that were made to it since you originally forked it. There are a few ways you can do that, but one way to do it in one step is with git’s pull command. This fetches the changes and merges them into your local version of the repository, all in one step. Run the following code in your bash shell to do that:\ngit pull upstream master Ideally, all this will have worked seamlessly (if not, check with the faculty and we can help you troubleshoot). Close your bash shell and check your version of the “csu_msmb” project to see if it looks like it’s up-to-date with the original. You can go to the “Commit” button in the “Git” tab, and there is a “History” selection in the window that pops up. Look through that and make sure that you see recent commits to confirm that your version is now up-to-date.\nFinally, this has updated your local version, but not your GitHub remote. Go ahead and use the green up arrow in RStudio’s “Git” pane to push your updated local version up to GitHub. Now both your local (“master”) and remote (“origin”) branches should be up-to-date with our original version, so it will make it much easier to merge in your changes.\nIf you’d like to learn more about this process, there’s a really nice blog post here.\n Making a new blog post In blogdown, each blog post is an RMarkdown document. The stuff at the very top of the file (the YAML with details like the title and author) will look a bit different than plain RMarkdown files, but once you get into the body of the post, you should find that the rules are very similar to RMarkdown.\nYou will be creating a blog post that will include a table with the vocabulary list as well as a few other elements. There are a few ways you can add a new blog post file in blogdown. You’re welcome to use any method you’d like, but if you’re not sure where to start, this is one way.\nMake sure that you have RStudio open to the project for our course’s website. If you do, you should see csu_msmb in the upper right hand corner of your RStudio session. (If not, go to File -\u0026gt; Open Project... and navigate through your file directory to your local version of our project directory and open the csu_msmb.RProj file there.)\nNext, you can use an RStudio “Addin” to make a new blog post using a nice user interface. These Addins are all alternatives to things you could do with a function call in R, but the Addin often provides a more immediately user-friendly interface for you to enter options. For example, the Addin for creating a blog post does all the actions of a blogdown function called new_post, but instead of needing to remember the parameter name to use for the author listing and the title and all that, you can just fill the information into a nice form and go from there.\nTo find the new post Addin, look at the top of your RStudio session window. You should see “Addins” with a down arrow beside it. Click on the down arrow. When you do, you should see a “New Post” option. Select this option. A form should pop up with spaces for you to fill in the title, author, and some other details.\nFill this form out in the following way:\n Title: This should be “Vocabulary for Chapter [x]”, but with “[x]” replaced with your chapter number. Author: Make sure you put your name exactly as listed in the “People” section of the website. This will help the website generator connect this post with your user profile, so when someone reads it they’ll get your picture and a link to find out more about you at the end of the post. Date: This is where you put the publication date of your blog post, and it has a pretty cool feature. Even if you write your blog post earlier, the post will not be published on the blog until the date listed in this section. That means that you can start writing your blog on one day but know that it won’t show up online until later. It also means you can start work on your blog, but a half-finished draft won’t show up online until you get to the publication date. For right now, set the current date in this section, so that your blog post will show up locally as you work on it, but later you’ll actually change this date so that, when you submit your pull request, your post won’t show up until the faculty have had the chance to suggest some changes and for you to make any needed fixes. Categories and tags: For both the “Categories” and the “Tags”, be sure to include vocabulary and Chapter [x] (with [x] replaced with your chapter’s number). These tags will let everyone on our website quickly find all the blog posts on your chapter or all the vocabulary lists. Format: You have several choices for the type of file to use to write your blog post. Since we’re going to be using some R code to make the table look pretty, you’ll need to pick one of the options that allows for R code chunks, so that rules out plain Markdown. I recommend that you use “.Rmd”.  Once you make these entries, click the button labeled “Done”. This creates an RMarkdown file for your blog post and opens it for you. Here’s an example of me doing this process if I were writing the vocabulary list for Chapter 16:\nYou can see, in the RMarkdown file that’s created and opened, that all these details end up getting inserted into the YAML at the top of the RMarkdown file. If you ever need to change anything (like the date or the title), you can change it here in the RMarkdown file. Do so carefully, though—YAML can be pretty picky about things like spacing and special characters (hyphens, for example).\nIf you ever need to find this file later, all of the blog posts are saved in a special place in our project’s directory: in the content subdirectory, there’s a subdirectory called post that contains both the RMarkdown files used to write the posts and the output (an HTML file) that is created by the RMarkdown each time you save the file. You might notice that they all have long file names—the file name for a blog post is a combinataion of its publication date and its “slug”, which is some abbreviation of the original title. If you really want, you can change what the slug will be when you first create the blog post, but I don’t think you really need to.\n Writing in the blog post   via GIPHY Within the body of the blog post RMarkdown file (in other words, below the --- that marks the end of the YAML section), you can write the blog post just as you would any RMarkdown document. This means that you can use things like ** to mark bold text, * for italics, and #, ##, etc., for section headings.\nIt also means that you can insert chunks of R code that will run and add their output within the post. Unlike in regular RMarkdown, you usually won’t have to press the Knit button to knit the document. Instead, the blog post should re-knit every time you save the file. You can check to see by looking at the Viewer pane to look at the current version of the site (if it doesn’t show the site automatically, load the blogdown package and then run serve_site).\nIf you have not worked much with RMarkdown before, you might want to check out some references on how it works. There are several great articles on the RMarkdown website that can help.\nIn your blog post, go ahead and draft a first paragraph that describes the key concepts covered in the chapter. Also, create third-level section headings (i.e., use ### to mark the section heading) for “Sources consulted or cited” and “Practice”. Save your blog post file and check to see if these changes have been made in the version of the website in your Viewer pane!\n  Create the vocabulary list Now, for the content of your post. You’ll be creating a vocabulary list, as well as embedding a Quizlet practice app, so that your classmates can learn the vocabulary for the chapter. This list will be what everyone is responsible for in the weekly vocabulary quiz.\nYou can see an example of a vocabulary blog post for Chapter 1. You can use this as a template for your own post.\nIdentify the vocabulary terms you need to define First, you will need to decide which words from the chapter to define. We expect that you will include all the bolded terms for your chapter. Here are some guidelines for deciding on the vocabulary terms to define for your chapter:\n You should include all words in the chapter that are given in bold. Be sure to look for bolded terms in the sidenotes and end-of-chapter exercises, too! Occassionally, the authors use bold for subheadings (see the “Why R and Bioconductor?” section in the Introduction or the “Summary of this chapter” section of Chapter 1). These subheadings do not need to be included in the vocabulary list for the chapter. If you find one or more common synonyms for a term, you can include that with the term in the list (e.g., “variability / spread / dispersion”). Feel free to change a term from singular to plural or vice versa if it helps you in writing the term’s definition. Similarly, if the bolded term does not include all the words that would be helpful (e.g., the bolded term is “sufficient”, but the term of interest is “sufficient statistic”), you can add a word or two to the bolded term. The bolded terms in the book tend to favor statistical terms over biological ones. If there are some biological terms you needed to look up when you read the chapter, or that you think some people in the class might not know, feel free to add them to your vocabulary list.   Create a .tsv file with terms and definitions While you could directly add the vocabulary into an RMarkdown table, we are asking you to save it into a plain text .tsv file, which will then be read into the RMarkdown document to form a table. We doing this because it creates a few advantages. First, if we have the vocabulary list in a dataframe (which we get when we read it in from a plain text file), we can use some cool R packages to format the table nicely, without having to learn loads of new Markdown or HTML formatting tricks. Second, we want to also use the vocabulary list as input to a Quizlet list, which will let us embed a practice app with flashcards and quizzes. One of the easiest ways to create a Quizlet list is to copy in vocabulary list directly in the tsv format, so this approach makes that secondary use easy.\nIn our website’s repository, there is a special subdirectory for saving vocabulary list .tsv files, with one for each chapter. In the Project directory, go to content -\u0026gt; post -\u0026gt; vocab_list. This is where you want to save the .tsv file for your chapter.\nTo create the file, in RStudio go to the “File” tab in the menu at the top and select “New File” -\u0026gt; “Text File”. This will open a file in RStudio in plain text format. Save the file as “chapter_[x].tsv” (but replace “[x]” with your chapter number). Make sure you save it in the “vocab_list” subdirectory of the project with the rest of the vocabulary list files.\nNow write your vocabulary terms and definitions in this .tsv. This file extension stands for “tab-separated”, so to format the file correctly, you should:\n Put each term / definition pair on its own line. Because some terms will be long, they may visibly “wrap” in the text file you have open, but as long as you don’t press the “Return” key, they should still be on one line of the file. To doublecheck, you may want to make sure that you have line-numbering on in RStudio and make sure that only one line number is listed for each term on the left hand side of the file. Press the “Tab” key to add a tab between the term and definition on each line. This should be the only place you have tabs in the file. R will look for tabs to figure out where to split between vocabulary terms and there definitions (as will Quizlet when you copy the terms into the list there). Sometimes it won’t look like the tab’s added a lot of space, but that’s no problem—the computer can see it even if you can’t! Don’t put any header information at the start of the file. Just start directly with your first vocabulary term.  If you’d like to see an example, check out the “chapter_1.tsv” file in the “vocab_lists” subdirectory. This is the file that serves as input for the Chapter 1 vocabulary list blog post.\nHere are some guidelines for writing your definitions:\n It is fine to use wording from the chapter text or to use wording directly from other websites or sources. However, you must include a list of any of the sources that you used to write your definitions at the bottom of the vocabulary blog post. Further, if you are using sources besides the course textbook, make sure that the definition is appropriate in the context of our course. Often, words will have a number of different definitions across different disciplines. Try to use more formal sources (e.g., textbooks, other published books) rather than less formal websites to find definitions whenever possible. See the Chapter 1 vocabulary list for an example of what we expect for using and listing references. If a vocabulary term was defined in a previous chapter’s vocabulary list, feel free to reuse the definition. Our library has excellent resources that you can use to help write your definitions, including textbooks and dictionaries specific to biology and statistics.   Adding R code to show the list in the post I’ve written up some R code that will read in the vocabulary list and make it into a nicely formatted table in the HTML version of the blog post. You can re-use this R code in your post, you’ll just need to change the name of the input file to the one for your chapter’s file.\nThis R code uses a few R packages beyond the base R code. If you haven’t installed these packages yet, you’ll need to before the code will run. You’ll need to install:\n knitr dplyr readr kableExtra  Once you have these, below your paragraph summarizing the chapter’s theme, write:\n “The vocabulary words for Chapter [x] are:”\n (but with your chapter’s number) and then paste in the following code and change chapter_1.tsv in the code to the correct file name for the .tsv file you created for your chapter’s vocabulary.\n```{r echo = FALSE, message = FALSE, warning = FALSE} # Load packages library(dplyr) library(readr) library(knitr) library(kableExtra) # Read in vocabulary from tsv into a dataframe # This is where you'll need to replace the file name with your own vocab % kable(align = c(\"rl\"), col.names = c(\"\", \"\")) %% kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %% column_spec(1, bold = T, border_right = T) %% column_spec(2, width = \"30em\") ``` This code reads in the data from your .tsv file and then formats it in a nice way. If you’d like to understand it better, try commenting out some lines and see how it changes the output. One of my favorite piece of this code, one that I think might come in useful for you later, is column_spec(2, width = \"30em\"). This sets the width of one of the columns to be 30 ems (the width of the letter “m” in whatever font you’re using). By setting the width, the table won’t automatically expand to fit the text you put in the column onto one row. Instead, it will allow the text to “wrap”, going onto separate lines if the definition entry is long enough.\nIf you want to find out more about creating really fancy tables from RMarkdown, check out the documentation on the kableExtra package. What you can do (and how) is different, depending on whether you’re outputting to a pdf or a HTML file, so there’s separate documentation for each.\nOnce you add this code in, I’ve found that you actually do need to press the Knit button sometimes. If you don’t see your list when you save your file, or if it doesn’t update properly as you make changes to your file, try knitting with the Knit button and that should help.\n Creating and embedding a Quizlet app The last piece of the blog post is the practice section. For this, you’ll create a vocabulary list on Quizlet, which you can then embed in the blog post, so the other students can practice right on our site.\nYou’ll need to sign up for a Quizlet account first. The free account is fine.\nNext, create a new vocabulary list. There’s a “Create” button for making new lists on the main page. While you can add vocabulary by hand, you can also post in a whole list if it’s in a tab-separated or comma-separated format. Copy in the contents of your vocabulary list .tsv file. You can preview the terms lower on the page once you do, to make sure that all the terms and definitions came in correctly. If everything looks good, click on the buttons for “Import” and then “Create”.\nThis will create your list and take you to a page where you can try out your flashcards. On this page, there’s also a button with three dots. If you click on this, there’s a choice of “Embed”. When you embed HTML content, you are inserting an application from one website within another one. Embedding is a really fun trick for enriching blog posts and other RMarkdown documents that are rendered to HTML. For example, you can also embed Shiny apps, YouTube videos, and Google maps in your RMarkdown using the same process we’ll use here.\nWhen you select “Embed”, a pop-up window will open with some HTML code. Copy this and then paste it in the “Practice” section of your vocabulary blog post. Be sure to leave a blank line above and below the text you paste. When you look at your blog post in a web browser now, you should see the practice flashcards embedded in the “Practice” section.\n  Submit the post   via GIPHY So far, you’ve made these changes to your local copy of our website’s repository. To submit the changes to us, you’ll need to push your changes to your remote version of the repository (the one in your GitHub account) and then submit a pull request to us for us to pull those changes into the original website repository (the one in my GitHub account). This process should feel pretty familiar—it’s pretty much what you did to submit your changes to your profile information for the website on the first day of class.\nAs with other steps, there are several ways you can do this, and if you have an idea of how to get it done, any way is fine. If you don’t know where to start, though, you can follow along in this section for one way to do it.\nPushing your changes to your remote repo First, you’ll need to get any changes you’ve made from your local repository up to your remote version on GitHub.\nFirst, commit any changes that you’ve made through the Git window in your RStudio session. This will record the changes you’ve made in the git record for your local repository.\nNext, you’ll need to push these commits to the remote repository, to send these changes to GitHub. In the Git window in RStudio, there’s a green up button. Push that. It should send all your changes up to your GitHub version of the repository. To check, go online to your GitHub account and look through your repositories for your fork of “csu_msmb”. Click on “Commits” to see a history of the commits to the repository—your latest ones should be at the top of the list.\n Submitting a pull request to the original repo At this point, you’ve made changes, checked them, and pushed them to your GitHub version of the repository. Remember, though, that you forked the repository from our original one, and so you’ve been working with a copy of the repository this whole time, rather than changing our original version.\nTo get your changes incorporated into our original version, you’ll need to request that we pull your changes into the original repository. To do this, you can submit a pull request through GitHub. Go to the main page for your fork of the GitHub repository and look for a button that says “New pull request”. When you click this, it will walk you through making a pull request. You’ll have a space to write a message describing the changes you’re recommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation on pull requests.\n  Edit and re-submit the post based on faculty feedback The other faculty members and I will get a notice when you submit your pull request. We’ll take a look and will probably have some suggestions for the wording of some of the vocabulary terms. We’ll give you some feedback through the pull request page, and then we’ll work together to get the list finalized before it’s published for the rest of the class.\n ","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580751472,"objectID":"b6a63bb5b3e96b091185e33290708428","permalink":"/post/creating-a-vocabulary-list-blog-post/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/creating-a-vocabulary-list-blog-post/","section":"post","summary":"As one of your assignments for this class, you are responsible for creating a blog post with all the vocabulary and definitions for one week of the course. This blog post will explain how you can create and publish that blog post on our course website.\nCreate the blog post Update your fork of the website You should have already forked our website to add your details for the “Person” section.","tags":["instructions","vocabulary","blogdown","github"],"title":"How to create a vocabulary list blog post","type":"post"},{"authors":["Brooke Anderson"],"categories":["blogdown","github","instructions"],"content":" One goal of this course is to continue developing your data science programming skills. This will include plenty of work on R programming, but also more to help you learn tools for reproducible research, like RMarkdown and git.\nWe will be using our course website as a collaboration tool during this course. This website was created using blogdown, which allows you to create and update a blogging website with R and RStudio. We are using a GitHub repository to share all the code for this website and serving the site using Netlify.\nDuring this course, you will have two graded products that you will need to submit as blog posts to our site. One will be a glossary of vocabulary terms for one chapter of the book, listing key words and their definitions for the chapter. The second will be the “official” version of one week’s in-course exercise.\nTo help get you up to speed with using blogdown, GitHub, and RMarkdown with our site, we’ll start by having you update your profile details for our website. We’ll also use this to give you all a chance to introduce yourselves to each other and to us. This post covers the details for how to do that.\nRequired set-up This exercise, and this course as a whole, requires a certain set-up on your computer:\nR installed on your laptop RStudio installed on your laptop git installed on your laptop Your own GitHub account  If you already have all this set-up, you can skip to the next section. Otherwise, this section has details on completing this set up.\nInstall R on your laptop You can install R from the Comprehensive R Archive Network (CRAN). Search for the version appropriate for your computer’s operating system.\nIf you already have R installed, check your version number. If it’s older than six months or so, you should probably update your version for the class. You can use the sessionInfo() function to find out details about your current R session, including the version of R you’re currently running:\nsessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 tools_3.6.3 ## [5] htmltools_0.4.0 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 blogdown_0.18 knitr_1.28 stringr_1.4.0 ## [13] digest_0.6.25 xfun_0.13 rlang_0.4.5 evaluate_0.14 Based on the return from this call, for example, I can tell that I have R version 3.6.3 (2020-02-29).\n Install RStudio on your laptop You can download RStudio directly from their website. The free Desktop version will work great for this course. If your version of RStudio is more than a year old, you should probably update it for this course. To check your version of R Studio, open R Studio, go to the “RStudio” tab at the top, and click on “About RStudio”.\n Install git on your laptop The software git is version control software, which will help you record and track changes that you’ve made to code and other plain text documents.\nIt’s free to download. Go to https://git-scm.com/downloads and select the version for your operating system. For this software, you’re probably okay if you downloaded it a little while ago (although if more than two years or so, you might want to update).\n Get a GitHub account You will need a (free) GitHub account for this course. You can sign up for one (if you don’t already have one) at https://github.com/. While there are some fancier paid plans available, the free account will work great for this class.\nWhen you sign up, you’ll get to choose a GitHub handle. You might want to make this something that will be easy for people to remember. For example, if your name is still available, that would be a great option. This handle will form part of the address to all of your GitHub repositories, so it is convenient if it is easy for people you work with to remember (mine, unfortunately, is not!).\n  About blogdown The blogdown package is an R package created by Yihui Xie that allows you to use R and RStudio to create and update your own webpage with a blog. The appeal of being able to do this with R is that you can write blog posts using RMarkdown, so you can include executable R code in each post.\nblogdown creates your site using the Hugo framework. Hugo is software that can build static websites (i.e., ones that can be served to viewers without needing database backends or other fancy things). People have created different templates for Hugo-generated websites, and these templates provide the structure and framework, while you can adapt the content.\nThis means that our website (which is, essentially, a collection of files in a directory written in a form that a web browser can convert to a pretty website) includes a lot of files and code that come straight from a template that someone else wrote, and then places here and there where we can add or change the files to make the website ours.\nOne of the ways that you can change the website is to add posts. You’ll be doing this later in the course by contributing two blog posts of your own, one on the vocabulary for a chapter and one with the “official” version of the exercise for a chapter. We’ll cover more on how to add a blog post later in the course.\nThe other way that you can change the website is to change some of its “front page” data. The website has a section on “People”, with the profiles of everyone in the class. The information shown in this section is all saved in plain text files in our website’s file directory. Today, you’ll change you details in the file dedicated to you and then send those changes back to us so we can update the online version of the site.\nYou will need to install two pieces of software to work on our website. First, you’ll need the R package blogdown. You can install this package in the normal way, using install.packages:\ninstall.packages(\u0026quot;blogdown\u0026quot;) Once you have blogdown, you can install the Hugo software using a function in the blogdown package, install_hugo:\nlibrary(blogdown) install_hugo() For both these installations, your computer will need to be online or you’ll get an error.\n Getting a fork of our repository on your computer All our websites files are posted in a GitHub repository at https://github.com/geanders/csu_msmb. With this (or any) GitHub repository, you can suggest changes by forking the repository, cloning the fork to your computer, making and committing the changes, pushing those commits back up to your fork of the repository on GitHub, and then submitting a pull request.\nForking a GitHub repository   via GIPHY When you fork a GitHub repository, you get a copy of that repository that you can play around with and change yourself, without it affecting the original repository. It’s essentially just copying the whole repository, with all its files, into a repository on your GitHub account.\nThe only thing that makes it different from a plain copy (and what makes it really powerful in some cases) is that, if you decide that your changes might make the original repository better, you can submit a pull request. This requests that the owners of the original repository update it to incorporate the changes you’ve made on your fork of the repository. The original authors can review each of the commits you’ve made, so they can even cherry-pick your changes if they want.\nGitHub also lets the original authors see if there are any merge conflicts created from changes that they’ve made to the original repository since you forked it. This can let the original authors see how hard it will be to incorporate all of your changes in the forked version with their version of the repository.\nTo fork the repository with our course’s website materials, all you’ll need to do is go to our GitHub repository for the course website (while you’re signed in to GitHub) and click on the “Fork” button towards the right of the page. Now go and check in the “Repositories” section of your own GitHub account—you should see that you now have a forked copy of the “csu_msmb” repository.\nIn this exercise, you’ll work with the fork of the repository, and then once you’ve made your changes, you submit a pull request, so that we can get your changes back into the main webpage.\nIf you need more help on how to fork a repository, GitHub has a help page on the topic that might be useful.\n Cloning the fork to your computer Next, you’ll want to get a copy of your forked repository onto your own computer, where you can work with it, make changes, and preview the website with your updates.\nTo do this, you’ll clone your fork of the repository onto your computer. The version of the repository on GitHub is called the remote branch of the repository and the version you get on your computer once you’ve cloned it is the local branch. By cloning (instead of just downloading), you’ll maintain a connection between the remote and local versions through git, which will allow you to push changes that you make and commit on your own computer up to the remote branch on GitHub.\nGo to GitHub, make sure you are logged into your account, and navigate to your forked version of the repository for this class. There should be a button to the right of the page that says “Clone or download” (you may need to scroll down to find it).\nWhen you click on this button, it will give you a choice between “SSH” and “HTTPS” for the protocol to use to connect your local and remote branches of the repository. You’re welcome to try either, but I usually (on a Mac) have better luck with “SSH”. Occasionally, people running Windows in my courses have had better luck with “HTTPS”, although for most folks “SSH” seems to work fine. Once you choose which protocol to use, you can copy the snippet of code that is given in the pop-up.\nNext, you’ll run this code from a bash shell on your own computer to clone the repository. You first will need to open a shell. If you’re on a Mac, you can do that with the “Terminal” application. With Windows, you’ll probably need to use the bash shell that comes with the Windows version of git. Search your programs for “bash” or “git bash” and see if you see something that looks promising.\nOnce you open a shell, you’ll see a command prompt, like this:\nusername$ You can type shell commands here and then press “Return” to run them. You should first move into the directory where you want to clone the repository. Your “Desktop” might be a good place for it for now (unless you have some organization you use for course-related files). The cd shell command lets you “change directory”. If you don’t put anything after cd, it will change to your home directory. Otherwise, it will move to the directory you specify. For example, if the “Desktop” directory is a subdirectory of my home directory, I could move into it by running:\ncd Desktop If you have not use shell commands much before and are having any problems navigating to the directory you’d like, let us know in class, and one of us can help you.\nOnce you are in this directory, you’ll paste git clone followed by the command you copied from the “Clone or download” button on GitHub. It will probably look something like this (but with your GitHub handle in place of “geanders”):\ngit clone git@github.com:geanders/csu_msmb.git When you run this, you may have to put in your GitHub username and password. You may also get some questions about whether you really want to download the repository (you do). If everything’s successful, you should see that there’s a new directory called “csu_msmb” in which directory you decided to put it (“Desktop”, for example).\nThis directory has a special file in it that makes it an R Project—a special version of a file directory with some extra structure and saved preferences. Make sure that you open the project as a whole when you work on it in R Studio, rather than opening just by clicking on one of the files. To do this, you can go in R Studio to \"File\" -\u0026gt; \"Open Project...\" and then navigate through your file directory to the “csu_msmb” directory you just cloned.\nIf you need more help, GitHub has a help page with more on how to clone a repository from GitHub to your own computer.\n Changing and committing in RStudio When you have R Studio open to an R Project that is using git version control, R Studio will include a “Git” pane. You can use this pane to commit changes you make to files in the repository, write messages explaining those commits, and push your changes to your remote branch of the repository on GitHub.\nWhen you commit a change, that change is written into a log of every change made to the files in the repository. You can later look through these commits, so you’ll want the commit messages to make sense when you read them in the future. When you’re collaborating with others, the commit messages will help you see what each other are doing.\nWhen you first commit a change, the commit is only saved in your local branch. To send it up to the remote branch of the repository on GitHub, you’ll need to push those commits. Once you push your local commits, your GitHub repository should exactly mirror your local repository.\nAs soon as you make a change to a file in the repository that’s being track by git, that file will show up in the Git pane, with a little check box beside it. When you’re ready to commit a change, click on the “Commit” button on the top left of the Git pane. This will open a pop-up box.\nIn this box, click the check boxes for all the changed files on the left you’d like to include in the commit. Then write a short commit message, describing the changes you’ve made. You should try to fit it all in the first line of the “commit message” window. If you can’t, write a short description in the first line, skip a line, and then you can write as much as you want.\nOnce you’ve written your commit message, click on the “commit” button. This will record this commit. To check that it has, you can go to the “History” tab and make sure the commit shows up as the last thing in your history.\n  Updating your profile details Rendering blogdown websites in RStudio Once you’ve opened the R Project with our website, you can use the blogdown package to serve the website. This will only update and show the website on your computer (not change our main website online), but it lets you check that everything’s working and preview what the site will look like online.\nRStudio’s “Viewer” pane can work as a web browser. This means that it can show our website. When you have opened the R Project with the cloned repository of our website (“csu_msmb”), try running the following in your R console to render the site:\nlibrary(blogdown) serve_site() If everything worked, you should be able to see a version of the website in your RStudio “Viewer” pane. If you’d like to see it in your usual web browser, click on the “Show in new window” button on the top left of the “Viewer” pane (this looks like a little rectangle with an arrow on it). This will open the website in your default web browser.\nTake a look at the web address when you do—it should start with 127.0.0.1. This is a loopback address—an IP address that refers back to your local computer (localhost), rather than an outside web servers. Anytime you’re building a website and checking it locally, you’ll see this in the web address when you open the site in a web browser. (You can even get T shirts with “There’s no place like 127.0.0.1”).\nAs you work through the next parts of the exercise, the rendered website in the Viewer pane should update every time you save your changes to files in the website. If you have the website open in your default browser, too, you might want to refresh the site with the normal “Refresh” button for your browser. If things ever seem like they’ve gotten out of sink, you can always re-run serve_site().\n Navigating the website’s file directory to find your profile We all have our own author profile in a subdirectory within the website’s files. To find yours, go to the “content” subdirectory of the website files and then the “authors” subdirectory within that. You should see a subdirectory there with your name. Click on that and you’ll see the two files that make up your author profile, \"_index.md\" and “avatar.jpg”.\n Updating your information in \"_index.md\" Your details are all given in the \"_index.md\" file in your author subdirectory. To update your details on the website, you’ll need to change your details in this file.\nThe file is written in a Markup language called YAML. If you’ve used RMarkdown before, you might recognize this syntax from the information that goes at the very top of each RMarkdown file.\nIn your \"_index.md\" file, anywhere there is a placeholder, like “[Year]” or “[Institution]”, replace the placeholder with your own information.\nBe very careful when changing things like spaces and hyphens in the structure, as YAML is based on parsing these elements. As with any Markup language, as you are learning it, it’s best to try to render the final document often as you make changes, so you can make sure the changes make it through like you want and so you can catch any problems quickly.\nMake sure you change the following sections:\n bio: education: email: interests: name: organizations:  Some of the sections in social: are commented out, including the information for buttons for GitHub, GoogleScholar, and Twitter. If you have accounts through any of these services, you can add these buttons with your updated information. Just delete the # at the beginning of all lines in that section and then change the handle or web address information so that it links to your account for that service. In this section, also update your email address, with mailto: at the beginning, for the email icon.\nThe very bottom of the file, under the ---, provides space for you to write a paragraph summarizing who you are and your academic / research interests.\nBe sure to save the file after you’ve made all your changes.\nFor an example of a completed \"_index.md\" file, you can see mine here.\n Updating your avatar picture There’s also a place in your author profile directory to include a photo to represent yourself. To change from the default (the blue guy), replace the “avatar.jpg” file in your author profile directory with the JPG of your choice, and use the same file name (“avatar.jpg”).\nIt would be helpful for you to use a photo of yourself, since that will help us put names with faces, but if you don’t have one or would prefer not to use your own photo, feel free to pick any photo (for which you have appropriate permissions) to use.\nYou might need to crop your photo some to get it to show up in the circle on the website correctly. Try with your uncropped picture once, check the website in the RStudio Viewer pane to see how it looks, and then if it doesn’t work, play around with cropping it until you’re happy.\n  Submitting your updates Pushing the commits back to GitHub When you are ready to push all the changes you’ve committed to your local branch, you can do this from the Git pane in R Studio. In this pane, there are two arrows: a green up arrow and a blue down arrow. Click on the green up arrow to push the commits from your computer (the local branch) to GitHub (the remote branch). Visit your GitHub page for the repository (or refresh it if you already had it open) and check if your changes have successfully been pushed to the remote branch.\nIf you haven’t created an SSH key and shared it with GitHub, you may be asked for your GitHub password every time you try to push. This will get to be a pain, so you’ll probably want to set up an SSH key. For more on how to do this (as well as other help with using RStudio with version control), check out RStudio’s help documentation on the topic.\n Requesting that we pull your changes   via GIPHY At this point, you’ve made changes, checked them, and pushed them to your GitHub version of the repository. Remember, though, that you forked the repository from our original one, and so you’ve been working with a copy of the repository this whole time, rather than changing our original version.\nTo get your changes incorporated into our original version, you’ll need to request that we pull your changes into the original repository. To do this, you can submit a pull request through GitHub. Go to the main page for your fork of the GitHub repository and look for a button that says “New pull request”. When you click this, it will walk you through making a pull request. You’ll have a space to write a message describing the changes you’re recommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation on pull requests.\n  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578952088,"objectID":"aaf734a217c669a7993ece1eb560bb61","permalink":"/post/add-profile-details/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/add-profile-details/","section":"post","summary":"One goal of this course is to continue developing your data science programming skills. This will include plenty of work on R programming, but also more to help you learn tools for reproducible research, like RMarkdown and git.\nWe will be using our course website as a collaboration tool during this course. This website was created using blogdown, which allows you to create and update a blogging website with R and RStudio.","tags":["blogdown","github","instructions"],"title":"How to add your profile details to our course website","type":"post"},{"authors":["Brooke Anderson"],"categories":["vocabulary","Chapter 1"],"content":"  Chapter 1 covers generative modeling for discrete data. It introduces a number of terms covering probablity and statistical modeling, as well as a few biological terms. The vocabulary words for Chapter 1 are:\n        probability model  A mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.    vector  In programming, a one-dimensional array of data, all with the same data type.    discrete event  In statistics, an event that can take a finite or countable number of values (e.g., number of deaths in a community by day).    categorical variable  A variable that can belong to one of a finite set of levels.    levels  In the context of a categorical variable, the set of values to which the variable can be assigned.    factor  In the context of statistical programming, a data type that can take one of a limited number of possible values (e.g., sex, nationality).    exchangeable  A property of a vector of random variables that implies the order in which the variables appear in the vector doesn’t matter.    sufficient statistic  A (summary) statistic that contains all the information about the model parameters that is in the original, uncondensed form of the data.    Bernoulli distribution  A probability distribution describing a random variable that can take on two possible outcomes (e.g., win / loss).    parameter  A numerical value that describes a population.    complementary  A description of two events who are mutually exclusive and whose probabilities sum to one (i.e., either one event or the other is guaranteed to happen, but not both).    binomial random variable  A variable whose values occur according to a binomial probability distribution.    probability mass distribution  A function giving the probability that a discrete random variable is equal to a given value.    Poisson distribution  A probability distribution for count data that has support on the non-negative integers. This distribution is also used to approximate a binomial distribution when the probability of success is small and the number of trials is large.    epitope / antigen determinent  Site on a macromolecular antigen to which an antibody binds. This is the part of an antigen that is recognized by the immune system.    Enzyme-linked immunosorbent assay (ELISA)  An assay that is used to detect specific epitopes at different positions along a protein.    conditional on  Given    cumulative distribution function  A function giving the probability that a random variable is less than any specified value.    extreme value analysis  Analysis focused on the behavior of the very large or the very small outcomes of a random distribution, allowing an exploration of the probability of rare events.    rare event  Something that occurs with a very low probability.    rank statistic  A data vector sorted least to greatest.    Monte Carlo method  A method that uses computer simulation from a generative model to determine probabilities of events.    probability or generative modeling  A method of modeling where all the parameters are known and the mathematical theory allows us to work by deduction.    deduction  A top-down method of reasoning, starting from a theory or principle rather than from data.    statistical modeling  A method of modeling where the distribution of the data is not known.    fit  In the context of statistical modeling, estimating the parameters of a model based on observed data.    multinomial  A generalization of the binomial distribution to cases where there are a finite set of possible outcomes (e.g., a roll of a die).    power / true positive rate  The probability of detecting something if it is there.    null hypothesis  Often, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.    matrix  In programming, a two-dimensional array of data, all with the same data type.    expected value  The average (mean) value of a random variable.    variability / spread / dispersion  In statistics, the amount by which a set of observations deviate from their mean.    statistic  A numerical characteristic of a sample and known constants (i.e. no unknown parameters).    null distribution  The probability distribution under the null hypothesis.    alternative  In the context of a generating process and hypothesis testing, the generating process that is considered in comparison to the generating process under the null hypothesis.    chi-squared distribution  A distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).    p-value  The probability of seeing the observed data or something more extreme under the generative model associated with the null hypothesis.    probability density function  A function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.    default  In the context of arguments to an R function, the value that is used if no custom value is specified.    C. elegans genome nucleotide frequency  How often adenine, cytosine, guanine, and thymine occur in the DNA of a roundwork often used in scientific research.    Bioconductor  Open-source software that provides contributed programs for bioinformatic data analysis.    codon  A three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis).    DNA read  An inferred sequence of base pairs for a single DNA fragment, based on sequencing.    nucleotide  In the context of DNA, one of four compounds (adenine (A); cytosince (C); guanine (G); and tymine (T)) that make up the basic information unit.    genome  An organism’s complete set of DNA, including all of its genes.    replication cycle  In biology, the process that begins with the infection of a host cell by a virus and ends with the release of mature progeny virus particles.    point mutation  A change, addition, or deletion of a single nucleotide in a gene sequence.    genotype  The genetic make-up of an individual’s cells, including how the individual’s genetic make-up differs from others’.    diploid  Having genetic material in two complete sets of chromosomes, from two parents.    protein  A compound made up of amino acids; one of the four types of macromolecules that make up living organisms.    antibody  A type of protein made by certain white blood cells in response to an antigen.    antigen  A foreign substance in the body to which the immune system reacts.     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Bioconductor: Open Source Software for Bioinformatics. https://www.bioconductor.org/ Wikipedia: The Free Encyclopedia. https://en.wikipedia.org/wiki/Main_Page NIH Genetics Home Reference. https://ghr.nlm.nih.gov/ NCI Dictionary of Cancer Terms. https://www.cancer.gov/publications/dictionaries/cancer-terms   Practice   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578957646,"objectID":"689ae0d83c0f37774ff128d4fd9d9d42","permalink":"/post/vocabulary-for-chapter-1/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-1/","section":"post","summary":"Chapter 1 covers generative modeling for discrete data. It introduces a number of terms covering probablity and statistical modeling, as well as a few biological terms. The vocabulary words for Chapter 1 are:\n        probability model  A mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.    vector  In programming, a one-dimensional array of data, all with the same data type.","tags":["vocabulary","Chapter 1"],"title":"Vocabulary for Chapter 1","type":"post"},{"authors":null,"categories":null,"content":" This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe solution to this exercise requires the following R packages to be loaded into your environment.\nRequired Libraries library(\u0026quot;cluster\u0026quot;) library(dplyr) library(ggplot2) library(purrr)  Part A Question 5.1.a asks us to compute the silhouette index for the simdat data that was simulated in Section 5.7. The code is as follows:\nset.seed(1) simdat = lapply(c(0, 8), function(mx) { lapply(c(0,8), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) }) %\u0026gt;% bind_rows }) %\u0026gt;% bind_rows simdatxy = simdat[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] wss = tibble(k = 1:8, value = NA_real_) wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2) for (i in 2:nrow(wss)) { km = kmeans(simdatxy, centers = wss$k[i]) wss$value[i] = sum(km$withinss) } ggplot(wss, aes(x = k, y = value)) + geom_col() The provided code is used to simulate data coming from four separate groups. They use the pipe operator to concatenate four different, randomly generated, data sets. The ggplot2 package is used to take a look at the data as a barchart with the k-means method and k = 4.\npam4 = pam(simdatxy, 4) sil = silhouette(pam4, 4, border = NA) plot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) Next up is the code necessary to plot the silhouette index. The “silhouette” function comes from the “cluster” package, and the resulting graph provides an average silhouette width for k = 4 clusters.\nComputing the Silhouette Index summary(sil)$avg.width ## [1] 0.4985801   Part B Question 5.1.b asks us to change the number of clusters k and assess which k value produces the best silhouette index.\nThe silhouette value is a measure of how similar a cluster is to its own cluster when compared to other clusters. Values can range from -1 to +1. A high value tells us that the object is better matched to its on cluster and more poorly matched to neighboring clusters.\nIn this example, there are a couple of ways to assess which k gives the best silhouette index.One method would be trial and error and determining which k-value produces the highest silhouette index. This method works out for this example, but is impractical for much larger and complex datasets. Included below is the code for testing multiple different k-values and the resulting coefficient values.\npam2 = pam(simdatxy, 2) sil2 = silhouette(pam2, 2) plot(sil2, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) pam3 = pam(simdatxy, 3) sil3 = silhouette(pam3, 3) plot(sil3, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) pam4 = pam(simdatxy, 4) sil = silhouette(pam4, 4) plot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) pam12 = pam(simdatxy, 12) sil12 = silhouette(pam12, 12) plot(sil12, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) pam40 = pam(simdatxy, 40) sil40 = silhouette(pam40, 40) plot(sil40, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) This trial and error method indicates that the highest silhouette index (that was tested) is achieved with k = 4.\nA different (seemingly more appropriate) method is to write a piece of code that will test a range of k-values automatically. This next piece of code is adapted from Amy Fox and the group that she worked with during class. This is a much more practical method that provides a clear answer of which k gives the best silhouette index.\nk \u0026lt;- c(2:10) df_test \u0026lt;- data.frame() for (i in 2:10){ pam_run \u0026lt;- pam(simdatxy, i) sil_run \u0026lt;- silhouette(pam_run, i) row_to_add \u0026lt;- data.frame(i, width = summary(sil_run)$avg.width) df_test \u0026lt;- rbind(df_test, row_to_add) } df_test ## i width ## 1 2 0.4067400 ## 2 3 0.4000560 ## 3 4 0.4985801 ## 4 5 0.4401518 ## 5 6 0.3957347 ## 6 7 0.3717875 ## 7 8 0.3699929 ## 8 9 0.3670770 ## 9 10 0.3516570 ggplot(df_test, aes(i, width)) + geom_point() + geom_line() + xlab(\u0026quot;k\u0026quot;) + ylab(\u0026quot;Silhouette Index\u0026quot;) + ggtitle(\u0026quot;Testing different k values for Silhouette Index\u0026quot;) summary(sil_run) ## Silhouette of 400 units in 10 clusters from pam(x = simdatxy, k = i) : ## Cluster sizes and average silhouette widths: ## 63 38 40 52 33 40 35 33 ## 0.3885059 0.3273800 0.3622990 0.3703291 0.3573781 0.3257945 0.4429236 0.2807700 ## 31 35 ## 0.3944945 0.2335738 ## Individual silhouette widths: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.1778 0.2389 0.3703 0.3517 0.4946 0.6623 The result of summary(sil_run) matches the trial and error method, but in a more efficient manner..\n Part C The last part of this exercise asks us to repeat by calculating the silhouette index on a uniform (unclustered) data distribution over a range of values.\nHere, a new data set is generated without clustering the randomly genereated data.\nset.seed(1) simdat1 = lapply(c(1), function(mx) { lapply(c(1), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) }) %\u0026gt;% bind_rows }) %\u0026gt;% bind_rows simdatxy1 = simdat1[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] ggplot(simdatxy1, aes(x = x, y = y)) + geom_point() pam4.1 = pam(simdatxy1, 4) sil.1 = silhouette(pam4.1, 4) plot(sil.1, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;) The average silhouette width is 0.33, which is much lower than the clustered value of 0.50 that we see with the first simulation.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a1bc886f183f4bb0b2f71b0ef1bd82b3","permalink":"/post/2020-03-12-ex5-1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2020-03-12-ex5-1/","section":"post","summary":"This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe solution to this exercise requires the following R packages to be loaded into your environment.","tags":null,"title":"Exercise Solution for 5.1","type":"post"}]