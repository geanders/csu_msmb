[{"authors":null,"categories":null,"content":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She's currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she's not in the lab, she enjoys traveling and experiencing new food and cultures.\n","date":1589241600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589241600,"objectID":"7208808b98af56edaf6697e234c3877b","permalink":"/authors/amy-fox/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/amy-fox/","section":"authors","summary":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She's currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she's not in the lab, she enjoys traveling and experiencing new food and cultures.","tags":null,"title":"Amy Fox","type":"authors"},{"authors":null,"categories":null,"content":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.\n","date":1588896000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588953161,"objectID":"a99ca1d34add3871e4b8e2225c007a06","permalink":"/authors/bailey-fosdick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bailey-fosdick/","section":"authors","summary":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.","tags":null,"title":"Bailey Fosdick","type":"authors"},{"authors":null,"categories":null,"content":"Sherry WeMott-Colton is a graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.\n","date":1588550400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588643192,"objectID":"93e55912b52bf4b4c2aca0826be7d218","permalink":"/authors/sherry-wemott/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sherry-wemott/","section":"authors","summary":"Sherry WeMott-Colton is a graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.","tags":null,"title":"Sherry WeMott","type":"authors"},{"authors":null,"categories":null,"content":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.\n","date":1588204800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588204800,"objectID":"7979d3030119d29653a50463c423f481","permalink":"/authors/burton-karger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/burton-karger/","section":"authors","summary":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.","tags":null,"title":"Burton Karger","type":"authors"},{"authors":null,"categories":null,"content":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.\n","date":1587686400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589403606,"objectID":"d7248a08ab17e3d5c2bea65700d68a9b","permalink":"/authors/camron-pearce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/camron-pearce/","section":"authors","summary":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.","tags":null,"title":"Camron Pearce","type":"authors"},{"authors":null,"categories":null,"content":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.\n","date":1587513600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586917751,"objectID":"a2f4c4d8a79a270a1ff69d28937404db","permalink":"/authors/sarah-cooper/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sarah-cooper/","section":"authors","summary":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.","tags":null,"title":"Sarah Cooper","type":"authors"},{"authors":null,"categories":null,"content":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.\n","date":1587081600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1587439327,"objectID":"05bf39477bf7e4b59f2e5aed40d43fa8","permalink":"/authors/daniel-dean/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel-dean/","section":"authors","summary":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.","tags":null,"title":"Daniel Dean","type":"authors"},{"authors":null,"categories":null,"content":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She's excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.\n","date":1586476800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586551291,"objectID":"359ce304229cff7b3cd9e06860d9e8f8","permalink":"/authors/sere-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sere-williams/","section":"authors","summary":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She's excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.","tags":null,"title":"Sere Williams","type":"authors"},{"authors":null,"categories":null,"content":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.\n","date":1586390400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586130769,"objectID":"a0b6f7b6e4c0c6781dd1b5bb9f45c4be","permalink":"/authors/sierra-pugh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sierra-pugh/","section":"authors","summary":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.","tags":null,"title":"Sierra Pugh","type":"authors"},{"authors":null,"categories":null,"content":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.\n","date":1586131200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586211407,"objectID":"2c23a0a0af5e97a1815648da4252c925","permalink":"/authors/mikaela-elder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mikaela-elder/","section":"authors","summary":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.","tags":null,"title":"Mikaela Elder","type":"authors"},{"authors":null,"categories":null,"content":"Zach Laubach's research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data. He has carried out his research in birds, hyenas, and humans.\n","date":1585526400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585625857,"objectID":"995dbc8c49ceaac8348f9682a5aeed3c","permalink":"/authors/zach_laubach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zach_laubach/","section":"authors","summary":"Zach Laubach's research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data.","tags":null,"title":"Zach Laubach","type":"authors"},{"authors":null,"categories":null,"content":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.\n","date":1583971200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1584040643,"objectID":"9eeff3b66d9db9aba8ad7e1cd7ebf977","permalink":"/authors/brooke-anderson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brooke-anderson/","section":"authors","summary":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.","tags":null,"title":"Brooke Anderson","type":"authors"},{"authors":null,"categories":null,"content":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"84c043c7c8c0042274f75edaccfe9957","permalink":"/authors/james_dilisio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/james_dilisio/","section":"authors","summary":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.","tags":null,"title":"James DiLisio","type":"authors"},{"authors":null,"categories":null,"content":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f8f117d42fb07fcd0da9af3c5821003","permalink":"/authors/mike-lyons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mike-lyons/","section":"authors","summary":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.","tags":null,"title":"Mike Lyons","type":"authors"},{"authors":["Amy Fox"],"categories":["Exercise solutions","Chapter 12"],"content":" Exercise 12.2 from Modern Statistics for Modern Biologists  Use glmnet for a prediction of a continous variable, i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using Ridge versus Lasso penalty.\n Here are the packages that need to be installed.\nlibrary(dplyr) library(ggplot2) library(glmnet) # perform generalize linear models library(GGally) # used for ggpairs function library(superheat) # used to show correlation between variables  Data for the exercise The ElemStatPackage isn’t on CRAN anymore. But it is possible to download it from Github at: https://github.com/cran/ElemStatLearn/blob/master/data/prostate.RData\nload(\u0026quot;./example_datasets/prostate.RData\u0026quot;) prostate %\u0026gt;% head() ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 ## train ## 1 TRUE ## 2 TRUE ## 3 TRUE ## 4 TRUE ## 5 TRUE ## 6 TRUE This data looks at correlating the level of prostate-specific antigen lpsa and eight other clinical measures in men.\nHere’s what the variables mean:\n lcavol: log cancer volume lweight: log prostate weight age: in years lbph: log of the amount of benign prostatic hyperplasia svi: seminal vesicle invasion lcp: log of capsular penetration gleason: a numeric vector with the Gleason score pgg45: percent of Gleason score 4 or 5 lpsa: response (the thing you are trying to predict), the level of prostate-specific antigen train: a logical vector, of whether the data was to be part of the training dataset (TRUE) or the testing one (FALSE)  So, you’re trying to predict the values of lpsa based on all of the other variables. lpsa is a continuous variable. To get a general view of the lpsa values, we can view a summary of values and create a histogram.\nprostate$lpsa %\u0026gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.4308 1.7317 2.5915 2.4784 3.0564 5.5829 ggplot(prostate, aes(x = lpsa)) + geom_histogram(bins = 30) + ggtitle(\u0026quot;Histogram of lpsa response\u0026quot;) Here we can see the values range from -0.4 to values of almost 6. Based on the histogram, the data seems to also fit a close to normal distribution.\nBefore actually performing any predictions, we may want to see if any of the variables are correlated to each other. This can show if there are potentially two variables measuring similar things. We use the ggpairs function from the GGally package to do this.\nggpairs(prostate) Based on this plot, we can see that some variables, such aspgg45 and gleason, are highly correlated. We can also see the distribution of the data in the testing and training datasets (we will discuss the testing and training column later in this exercise).\nWe can also visually view correlations between the variables using the cor function and a heatmap from thesuperheat package.\nprostate %\u0026gt;% select(-train) %\u0026gt;% cor() %\u0026gt;% superheat() This plot shows the same data as the ggpairs output, but we can visually see the correlation between each of the variables. If the correlation is high and positive, then the rectange is yellow, if the correlation between two variables is low, the rectangle is purple.\nWe will first split the data into testing and training data.\nprostate_train \u0026lt;- prostate %\u0026gt;% filter(train == TRUE) prostate_test \u0026lt;- prostate %\u0026gt;% filter(train == FALSE) There are 67 samples in the training set and 30 samples in the testing set.\nBecause this data was specifically posted to teach machine learning, the authors included a column called train so that users can split the data into testing and training datasets. If this column had not been created, there are many ways to sample the data and split it into two datasets. Here is an example:\ninstall.packages(\"rsample\") library(rsample) prostate_split \u0026lt;- initial_split(prostate, prop = .75) prostate_test \u0026lt;- testing(prostate_split) prostate_train \u0026lt;- training(prostate_split)\n Fit generalized linear model (glmnet) with Lasso and Ridge penalties We will first pull out the predictors and reponse that we want to input into the glmnet function.\n# pull outall of the predictors that we are using predictors \u0026lt;- prostate_train %\u0026gt;% select(lcavol:pgg45) %\u0026gt;% as.matrix() head(predictors) ## lcavol lweight age lbph svi lcp gleason pgg45 ## [1,] -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 ## [2,] -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 ## [3,] -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 ## [4,] -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 ## [5,] 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 ## [6,] -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 # this is the response response \u0026lt;- prostate_train %\u0026gt;% pull(lpsa) When performing statistical analysis on data with many variables, we often have a problem with variance as there are many parameters. We can use penalization to account for this variance-bias trade off. Two examples of penalization are the Lasso and Ridge penalty which we will use in this exercise.\nBased on the glmnet package, when alpha = 1 the Lasso penalty is used, if alpha = 0, then Ridge penalty is used. A great resource for the glmnet package can be found here: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html\nHere, we use a matrix of all of the predictors (x) to try to predict the lpsa column (y). When we plot the penalized regressions, we use the label = TRUE to show which lines correspond to which predictors. In this case, the order of the input vector predictors matches with the variable numbers as follows.\n1: lcavol\n2: lweight\n3: age\n4: lbph\n5: svi\n6: lcp\n7: gleason\n8: pgg45\n# Lasso lasso_glmnet \u0026lt;- glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 1) plot(lasso_glmnet, label = TRUE) title(\u0026quot;Lasso Penalty\u0026quot;, line = -2.5) # Ridge ridge_glmnet \u0026lt;- glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 0) plot(ridge_glmnet, label = TRUE) title(\u0026quot;Ridge Penalty\u0026quot;, line = -1.5) The plots show the estimated coefficients as L1 norm increases. L1 norm is the regularization term. It means that at small L1 norm values, there is a lot of regularization, but as it increases, more variables become part of the model as coefficients take non-zero values. The top axis shows the number of nonzero coefficients correspoding to the lamba at the current L1 norm. From these plots we can see that the variables 1 (log cancer volume), 2 (log prostate weight), and 5 (seminal vesicle invasion) are good predictors for the level of prostate-specific antigen (lpsa response).\n Cross Validation After creating our models, we want to see how good they are. We can do this through cross-validation. In this case, we already split the data into training and testing data, but we could run the full modeling and cross-validation on all of the data. We use the cv.glmnet function to do this. It utilizes k-fold cross validation, meaning that the input data is split multiple times into new trainng and testing sets of sizes n(k-1)/k and n/k, respectively. We use this cross-validation to determine the best \\(\\lambda\\). Again we input a matrix of all of the predictors (x) to look at the lpsa response (y).\nset.seed(2) # Lasso cvglmnet_lasso \u0026lt;- cv.glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 0) cvglmnet_lasso ## ## Call: cv.glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 0) ## ## Measure: Mean-Squared Error ## ## Lambda Measure SE Nonzero ## min 0.0879 0.5939 0.09857 8 ## 1se 0.9873 0.6873 0.08098 8 plot(cvglmnet_lasso) title(\u0026quot;Lasso Cross Validation\u0026quot;, line = -1.5) # Ridge cvglmnet_ridge \u0026lt;- cv.glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 1) cvglmnet_ridge ## ## Call: cv.glmnet(x = predictors, y = response, family = \u0026quot;gaussian\u0026quot;, alpha = 1) ## ## Measure: Mean-Squared Error ## ## Lambda Measure SE Nonzero ## min 0.01336 0.5753 0.07244 7 ## 1se 0.13673 0.6408 0.09222 5 plot(cvglmnet_ridge) title(\u0026quot;Ridge Cross Validation\u0026quot;, line = -1.5) In the data output, 1se is the data point that is within 1 standard error of the minimum \\(\\lambda\\) (min). This is the value that the model suggests that we use in our ultiamate predictive model (indicated by the 2nd dotted line on the plots.) The 1se Measure is similar to the mean squared error. If the measure is small, then the model is better. When comparing the Measure of the 1se between the two penalties, we can see that the Ridge Penalty has a smaller 1se Measure, showing that it performs better.\nThe Nonzero column describes the nonzero coefficients, or the number of predictors that are important in the particular model. There were a total of 8 predictors as the input. The Lasso penalty shows that all 8 predictors are important in building the model, but the Ridge penalty only uses 5 predictors.\nBased on this cross-validation, we can deduce that the Ridge penalty is the best predictive model to use for this data. We have also identified the best \\(\\lambda\\) to use with this model at 0.18074.\n Lasso Prediction Here we pull out the same variables that we used to create the model, but we use the values from the testing data instead of the training data. We will then use the predictors from the testing data to predict the lpsa.\npredictors_test \u0026lt;- prostate_test %\u0026gt;% dplyr::select(lcavol:pgg45) %\u0026gt;% as.matrix() head(predictors_test) ## lcavol lweight age lbph svi lcp gleason pgg45 ## [1,] 0.7371641 3.473518 64 0.6151856 0 -1.3862944 6 0 ## [2,] -0.7765288 3.539509 47 -1.3862944 0 -1.3862944 6 0 ## [3,] 0.2231436 3.244544 63 -1.3862944 0 -1.3862944 6 0 ## [4,] 1.2059708 3.442019 57 -1.3862944 0 -0.4307829 7 5 ## [5,] 2.0592388 3.501043 60 1.4747630 0 1.3480732 7 20 ## [6,] 0.3852624 3.667400 69 1.5993876 0 -1.3862944 6 0 As we used the training data to build the model, we can then test the generalized linear model with the Lasso penalty on the testing data.\nWe start by using the predict function to use the model to predict the lpsa on the testing data. We can then see the correlation between the predicted values and actual values.\ns0 \u0026lt;- cvglmnet_lasso$lambda.1se lasso_predict \u0026lt;- predict(cvglmnet_lasso, newx = predictors_test, s = s0) # create a data frame of the actual lpsa values and the predicted lpsa values actual_lasso_predict_df \u0026lt;- data.frame(prediction = as.vector(lasso_predict), actual = prostate_test$lpsa) We can then see how correlated the prediction and real data are using the cor function.\n# look at the correlation of the prediction and real data cor(actual_lasso_predict_df) ## prediction actual ## prediction 1.0000000 0.7277086 ## actual 0.7277086 1.0000000 The output shows that the actual and predicted values are 72% correlated.\nFinally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values match up exactly, they would sit on the y = x line.\nggplot(actual_lasso_predict_df, aes(x = actual, y = prediction)) + geom_point(color = \u0026quot;#00B0F6\u0026quot;, size = 2) + geom_abline(slope=1, intercept=0)+ ggtitle(\u0026quot;Lasso Prediction\u0026quot;) + theme_light()+ coord_fixed(xlim = c(0.75, 5.6), ylim = c(0.75, 5.6)) We can see in regions of the plot above the y = x line that the model overpredicted the actual values. If a point is below the y = x line, the model underpredicted the values.\n Ridge Prediction We can then perform the same functions using the generalized linear model with the Ridge penalty to test on the testing data.\ns0 \u0026lt;- cvglmnet_ridge$lambda.1se ridge_predict \u0026lt;- predict(cvglmnet_ridge, newx = predictors_test, s = s0) # create a data frame of the predicted values and actual values actual_ridge_predict_df \u0026lt;- data.frame(prediction = as.vector(ridge_predict), actual = prostate_test$lpsa)  We can then see how correlated the prediction and real data are using the cor function again.\ncor(actual_ridge_predict_df) ## prediction actual ## prediction 1.0000000 0.7765403 ## actual 0.7765403 1.0000000 The Ridge prediction and actual values are 77% correlated.\nFinally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values matched up exactly, they would sit on the y = x line.\nggplot(actual_ridge_predict_df, aes(x = actual, y = prediction)) + geom_point(color = \u0026quot;#FF62BC\u0026quot;, size = 2) + geom_abline(slope=1, intercept=0) + ggtitle(\u0026quot;Ridge Prediction\u0026quot;) + theme_light() + coord_fixed(xlim = c(0.75, 5.6), ylim = c(0.75, 5.6)) We can see in regions of the plot above the y = x line that the model overpredicted the actual values. If a point is below the y = x line, the model underpredicted the values.\n Conclusion Comparing the Lasso and Ridge penalty, based on the cross-validation, the Ridge penalty had a smaller 1se Measure, showing that it performs better. When looking at the actual predictions on the testing data, the Ridge penalty had a higher correlation between the predicted and actual values (77%) compared to the Lasso penalty correlation (72%). In conclusion, the Ridge penalty performed better on this particular data set.\nResources - https://stats.stackexchange.com/questions/68431/interpretting-lasso-variable-trace-plots\n ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"9862a0f779546194c4fc7369e02a2492","permalink":"/post/exercise-solution-for-chapter-12/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-12/","section":"post","summary":"Exercise 12.2 from Modern Statistics for Modern Biologists  Use glmnet for a prediction of a continous variable, i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using Ridge versus Lasso penalty.\n Here are the packages that need to be installed.\nlibrary(dplyr) library(ggplot2) library(glmnet) # perform generalize linear models library(GGally) # used for ggpairs function library(superheat) # used to show correlation between variables  Data for the exercise The ElemStatPackage isn’t on CRAN anymore.","tags":["Exercise solutions","Chapter 12"],"title":"Exercise Solution for Chapter 12","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for May 14 We are looking forward to hearing about everyone’s exercise solutions today! Please plan for about 10 minutes to talk through yours. We will let you share the screen when it’s your turn. If you would like to follow along with each post, the “Schedule” section of our website now has links to all the final versions of blog posts, so you can open those for all the currently finalized exercise posts as we get to them.\nLarge group: meeting link\n ","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588953161,"objectID":"8b2cd88dd8bad8c715622698db181895","permalink":"/post/details-for-class-on-may-14/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/post/details-for-class-on-may-14/","section":"post","summary":"Links for May 14 We are looking forward to hearing about everyone’s exercise solutions today! Please plan for about 10 minutes to talk through yours. We will let you share the screen when it’s your turn. If you would like to follow along with each post, the “Schedule” section of our website now has links to all the final versions of blog posts, so you can open those for all the currently finalized exercise posts as we get to them.","tags":["class meeting"],"title":"Details for class on May 14","type":"post"},{"authors":[],"categories":["class meeting"],"content":" Links for May 7 Large group: meeting link\n Group 1 (Sierra, Camron, Sherry, James): meeting link Group 2 (Sarah, Burton, Daniel): meeting link Group 3 (Amy, Sere, Mikaela): meeting link   Vocabulary quiz for May 7 Loading…   ","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588878171,"objectID":"86642d4000906fb89df2cc5b8299cc98","permalink":"/post/details-for-class-on-may-7/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"/post/details-for-class-on-may-7/","section":"post","summary":" Links for May 7 Large group: meeting link\n Group 1 (Sierra, Camron, Sherry, James): meeting link Group 2 (Sarah, Burton, Daniel): meeting link Group 3 (Amy, Sere, Mikaela): meeting link   Vocabulary quiz for May 7 Loading…   ","tags":["class meeting"],"title":"Details for class on May 7","type":"post"},{"authors":["Sherry WeMott"],"categories":["vocabulary","Chapter 12"],"content":"  Chapter 12 covers supervised learning and the statistics of predicting categorical variables. Also discussed are the issues of overfitting and generalizability and how to “train” statistical models.\nThe vocabulary words for Chapter 12 are:\n        predictors  characteristics measured for an observation that may be useful in predicting the target variable    overfitting  the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future obervations reliably    generalization  refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning    statistical learning  framework for machine learning drawing from the fields of statistics and functional analysis. Deals with the problem of finding a predictive function based on data    objective response  in the context of supervised learning, a measurable response    kernel methods  class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). These use kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space    regression  statistical method that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)    classification  the process of grouping observations in a dataset by their similarities in terms of measured characteristics    linear discriminant analysis (LDA)  a common technique used both for supervised learning classification and as a pre-processing dimension reduction step that finds a linear combination of features to help in classification    misclassification rate (MCR)  in regards to statistical learning, the fraction of times the prediction is wrong, specifically when relating to classification of models    leave-one out cross-validation  k-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set    k-fold cross-validation  a technique where observations are repeatedly split into a training set of size around n(k-1)/k and a test set of size of around n/k. Mainly used in prediction when one wants to estimate how accurately a predictive model will perform in practice    curse of dimensionality  refers to the fact that high-dimensional spaces are very hard, if not impossible, to sample thoroughly, because data in any particular region becomes very sparse as dimensions increase    confusion table  in the field of machine learning and specifically the problem of statistical classification, a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. By counting the number of observations truly within each class versus the number predicted by the model to be in each class    sensitvity  true positivity rate or recall, measures the proportion of actual positives that are correctly identified as such    specificity  true negative rate, the probability, measures the proportion of actual negatives that are correctly identified as negative    receiver operating characteristic(ROC)/precision recall curve  a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied    Jaccard index  a statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets    mean-squared error (MSE)  the average squared error    risk function/cost function/objective function  the function that you optimize during the training of a predictive model (e.g., the maximum likelihood function for a classic regression model)    bias  a measure of how different the average of all the different estimates is from the truth    variance  how much an individual estimate might scatter from the average value    penalization  a tool to actively control and exploit the variance-bias tradeoff    regularization  a method used to to ensure stable estimates by helping to prevent overfitting of the model to the training data    logistic regression  a statistical model that in its basic form uses a logistic function to model a binary dependent variable. A binary logistic model has a dependent variable with two possible values (e.g, healthy/sick) which are represented by indicator variables (0,1)    penalty function  a term added to the objective function that consists of a penalty parameter multiplied by a measure of violation of the constraints    ridge regression  a method of regression in which the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. Doing this shrinks coefficients and helps reduce model complexity and mutli-collinearity    lasso  in the context of statistical regression modeling, Least Absolute Shrinkage and Selection Operator that is used in one type of regression modeling to reduce over-fitting and select useful features of hte data for predicting the outcome    elastic net  in the fitting of linear or logistic regression models, a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods    ExperimentalHub  in the context of Bioconductor, provides a central location where curated data from experiments, publications or training courses can be accessed    kingdom  the second highest taxonomic rank, just below domain    phylum  a level of classification of taxonomic rank below kingdom and above class    species  the basic unit of classification and a taxonomic rank of an organism, as well as unit of biodiversity    diagnostic plots  statistical influence plots that help to visualize how well a model fits the data (e.g. Normal Q-Q, Residuals vs Fitted)    tuning parameters  parameters that control the strength of the penalty term in certain types of regression algorithms (e.g., ridge and lasso regression), controlling the amount of shrinkage (where parameter estimates are shrunk towards a central point, like the mean) when fitting the mode    p-value hacking  manipulation of the data until finding a statistic that yields a desired result    workflow  in the context of a computational analysis, the chaining of software tools together in a series of steps that operate on data    scale invariance  a feature of objects or laws that do not change if scales, length, energy, or other variables, are multiplied by a common factor, and thus represent universality     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following source:\nHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom.\nhttps://en.wikipedia.org/\nhttps://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\nhttps://.statisticshowto.com\nhttps://www.cs.cmu.edu/~schneide/tut5/node42.html\nhttps://towardsdatascience.com\nhttps://bioconductor.org\nhttps://pfern.gtihub.io\n Practice   ","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588643192,"objectID":"1286b975b4c8519db83eaeea0acf9f48","permalink":"/post/vocabulary-for-chapter-12/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-12/","section":"post","summary":"Chapter 12 covers supervised learning and the statistics of predicting categorical variables. Also discussed are the issues of overfitting and generalizability and how to “train” statistical models.\nThe vocabulary words for Chapter 12 are:\n        predictors  characteristics measured for an observation that may be useful in predicting the target variable    overfitting  the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future obervations reliably    generalization  refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning    statistical learning  framework for machine learning drawing from the fields of statistics and functional analysis.","tags":["vocabulary","Chapter 12"],"title":"Vocabulary for Chapter 12","type":"post"},{"authors":["Burton Karger"],"categories":["exercises","Chapter 8"],"content":" Exercise 8.1  Do the analyses of Section 8.5 with the edgeR package and compare the results: make a scatterplot of the log 10 p-values, pick some genes where there are large differences, and visualize the raw data to see what is going on. Based on this can you explain the differences?\n Most of the following code is taken straight come the book in section 8.5 for data cleaning/wrangling and DESeq2 analysis. The steps performed in edgeR are quite similar but we do see some differences that we will get to towards the end.\nFirst, we load our libraries.\nlibrary(pasilla) library(edgeR) library(dplyr) library(DESeq2) library(ggplot2) library(pheatmap) library(tidyverse) We will use the same data used in the DESeq2 examples from the section 8.5.\nLoad the example data using the system.file() call for R data stored as part of a R package. We then convert our data to a matrix object called counts.\nfn = system.file(\u0026quot;extdata\u0026quot;, \u0026quot;pasilla_gene_counts.tsv\u0026quot;, package = \u0026quot;pasilla\u0026quot;, mustWork = TRUE) counts = as.matrix(read.csv(fn, sep = \u0026quot;\\t\u0026quot;, row.names = \u0026quot;gene_id\u0026quot;)) head(counts) ## untreated1 untreated2 untreated3 untreated4 treated1 treated2 ## FBgn0000003 0 0 0 0 0 0 ## FBgn0000008 92 161 76 70 140 88 ## FBgn0000014 5 1 0 0 4 0 ## FBgn0000015 0 2 1 2 1 0 ## FBgn0000017 4664 8714 3564 3150 6205 3072 ## FBgn0000018 583 761 245 310 722 299 ## treated3 ## FBgn0000003 1 ## FBgn0000008 70 ## FBgn0000014 0 ## FBgn0000015 0 ## FBgn0000017 3334 ## FBgn0000018 308 The matrix tallies the number of reads seen for each gene in each sample. We call it the count table. It has 14599 rows, corresponding to the genes, and 7 columns, corresponding to the samples.\ndim(counts) ## [1] 14599 7  edgeR Now begins the same analysis with the edgeR package. To do this we follow the vignette for the package that is a downloadable .pdf file that you can get in your Rstudio vignette(\"edgeR\") or online with https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf.\nHere we make the group object which is a vector with values representing the treatment status of each of the 8 samples, where 1 refers to the untreated group, and 2 refers to the treated group. With this group object we can make a DGEList(), from the edgeR package, with our count data and the groups we just made.\ngroup \u0026lt;- factor(c(1,1,1,1,2,2,2)) x \u0026lt;- DGEList(counts=counts, group=group) RNA-seq provides a measure of the relative abundance of each gene in each RNA sample, but does not provide any measure of the total RNA output on a per-cell basis. In other words, RNA-seq measure relative expression rather than absolute expression. This can become an issue in RNA-seq when a small number of highly expressed genes consume a substantial proportion of the total library for a sample causing under sampling of the other expressed genes.\ncalcNormFactors normalizes by finding a set of scaling factors for the library sizes that minimizes the log-fold changes between the samples for most genes. Using a trimmed mean of M-values (TMM) between each pair of samples. If we receive a norm.factors \u0026lt; 1 that means a small number of high count genes are monopolizing the sequencing reducing the counts of other genes. Conversely, a norm.factors \u0026gt; 1 scales up the library size, analogous to downscaling the counts.\nnormalization \u0026lt;- calcNormFactors(x) normalization ## An object of class \u0026quot;DGEList\u0026quot; ## $counts ## untreated1 untreated2 untreated3 untreated4 treated1 treated2 ## FBgn0000003 0 0 0 0 0 0 ## FBgn0000008 92 161 76 70 140 88 ## FBgn0000014 5 1 0 0 4 0 ## FBgn0000015 0 2 1 2 1 0 ## FBgn0000017 4664 8714 3564 3150 6205 3072 ## treated3 ## FBgn0000003 1 ## FBgn0000008 70 ## FBgn0000014 0 ## FBgn0000015 0 ## FBgn0000017 3334 ## 14594 more rows ... ## ## $samples ## group lib.size norm.factors ## untreated1 1 13972512 0.9995731 ## untreated2 1 21911438 1.0081519 ## untreated3 1 8358426 0.9843974 ## untreated4 1 9841335 0.9525077 ## treated1 2 18670279 1.0651817 ## treated2 2 9571826 0.9957012 ## treated3 2 10343856 0.9978557 The model.matrix() function creates a design matrix which is a matrix of values of explanatory variables of a set of objects. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The purpose of this conversion is to prepare the data in a manner that facilitates regression-like modelling (ex. glm).\ndesign \u0026lt;- model.matrix(~group) head(design) ## (Intercept) group2 ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 1 ## 6 1 1 The estimateDisp() function \"Maximizes the negative binomial likelihood to give the estimate of the common, trended and tagwise dispersions across all tags. We have to use this negative binomial (aka gamma-Poisson) model since our experiments vary from replicate to replicate more than the traditional Poisson can account for. This variance can be due to seemingly miniscule experimental conditions such as, temperature of cell culture, pipettor calibration, etc. In the case of the gamma-Poisson we have two inputs for variance and mean instead of just having \\(\\lambda\\) for both variance and mean in the normal Poisson. An important consideration that the edgeR package took into account is the fact that RNA-seq and other Next Generation Sequencing projects are extremely expensive and generally have few samples. Accounting for dispersion with a small number of samples can be challenging and in the edgeR package tackles this conundrum using a qCML method. The qCML method calculates the likelihood by conditioning on the total counts for each tag, and uses pseudo counts after adjusting for library sizes. Given a table of counts or a DGEList object, the qCML common dispersion and tagwise dispersions can be estimated using the estimateDisp() function\ndesign_matrix \u0026lt;- estimateDisp(normalization, design) head(design_matrix) ## An object of class \u0026quot;DGEList\u0026quot; ## $counts ## untreated1 untreated2 untreated3 untreated4 treated1 treated2 ## FBgn0000003 0 0 0 0 0 0 ## FBgn0000008 92 161 76 70 140 88 ## FBgn0000014 5 1 0 0 4 0 ## FBgn0000015 0 2 1 2 1 0 ## FBgn0000017 4664 8714 3564 3150 6205 3072 ## FBgn0000018 583 761 245 310 722 299 ## treated3 ## FBgn0000003 1 ## FBgn0000008 70 ## FBgn0000014 0 ## FBgn0000015 0 ## FBgn0000017 3334 ## FBgn0000018 308 ## ## $samples ## group lib.size norm.factors ## untreated1 1 13972512 0.9995731 ## untreated2 1 21911438 1.0081519 ## untreated3 1 8358426 0.9843974 ## untreated4 1 9841335 0.9525077 ## treated1 2 18670279 1.0651817 ## treated2 2 9571826 0.9957012 ## treated3 2 10343856 0.9978557 ## ## $design ## (Intercept) group2 ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 1 ## 6 1 1 ## 7 1 1 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 ## attr(,\u0026quot;contrasts\u0026quot;) ## attr(,\u0026quot;contrasts\u0026quot;)$group ## [1] \u0026quot;contr.treatment\u0026quot; ## ## ## $common.dispersion ## [1] 0.0228685 ## ## $trended.dispersion ## [1] 0.12060195 0.04196786 0.11986264 0.12040360 0.01632837 0.02002502 ## ## $tagwise.dispersion ## [1] 0.12060195 0.02742256 0.70902130 0.09430132 0.01321566 0.01722448 ## ## $AveLogCPM ## [1] -2.636763 2.953356 -1.966526 -2.223010 8.454625 5.088412 ## ## $trend.method ## [1] \u0026quot;locfit\u0026quot; ## ## $prior.df ## [1] 5.886884 ## ## $prior.n ## [1] 1.177377 ## ## $span ## [1] 0.3013916 Now that we have our design_matrix we can begin fitting modified linear models to it. Here we use a quasi-likelihood negative binomial generalized log-linear model, which is a mouth full. “Quasi-likelihood estimation is one way of allowing for overdispersion, that is, greater variability in the data than would be expected from the statistical model used.” Since we have already stated that we will have variation in our experiments, possibly due to the most minute factors, this issue of overdispersion is apparent. Instead of using a traditional probability functions, a variance function is used (variance as a function of the mean) and allows for an overdispersion parameter input which is used to “fix” the variance function to resemble that of an existing probability function (ex. Poisson).\nThe goal of this fit is to identify genes where the intensity level (gene expression level) is notably different between our treated and untreated groups. Running the glmQLF...() functions gives the null model against which the full model is compared. Tags can then be ranked in order of evidence for differential expression, based on the p-value computed for each tag.\nfit \u0026lt;- glmQLFit(design_matrix, design) qlf \u0026lt;- glmQLFTest(fit, coef=2) edgeRoutput \u0026lt;- topTags(qlf) edgeRoutput ## Coefficient: group2 ## logFC logCPM F PValue FDR ## FBgn0039155 -4.601317 5.882317 953.1722 1.646470e-14 2.403682e-10 ## FBgn0025111 2.905756 6.923428 714.2877 1.257310e-13 9.177735e-10 ## FBgn0035085 -2.548257 5.684922 452.3866 3.068717e-12 1.311741e-08 ## FBgn0003360 -3.173036 8.452776 442.2207 3.594058e-12 1.311741e-08 ## FBgn0029167 -2.188103 8.221274 412.0926 5.866109e-12 1.404698e-08 ## FBgn0039827 -4.142255 4.397963 408.8548 6.195926e-12 1.404698e-08 ## FBgn0034736 -3.492036 4.186934 403.9614 6.735313e-12 1.404698e-08 ## FBgn0029896 -2.434452 5.305827 336.3777 2.386477e-11 4.355023e-08 ## FBgn0000071 2.685868 4.795202 288.1793 6.900283e-11 1.119303e-07 ## FBgn0034434 -3.624878 3.214994 282.6144 7.884818e-11 1.151105e-07 head(as.data.frame(qlf)) ## logFC logCPM F PValue ## FBgn0000003 1.90105753 -2.636763 1.40991069 0.25938432 ## FBgn0000008 0.01020453 2.953356 0.00282599 0.95833875 ## FBgn0000014 -0.21077864 -1.966526 0.03020978 0.86444776 ## FBgn0000015 -1.61118380 -2.223010 1.65428857 0.21877991 ## FBgn0000017 -0.23044399 8.454625 3.99686462 0.06492692 ## FBgn0000018 -0.09673451 5.088412 0.53070377 0.47805393 Our outputs can be summarized by looking at our logFC column (log fold change) where the higher the number the higher expression was read during sequencing. Secondly our PValue allows the rejection of our null hypothesis, which is, there is equal expression regardless of what gene you look at.\nNow we want to visualize the data points after their regression fits. We must tidy up the data sets a bit to apply some tidyverse magic. First the data is converted to a data frame, as.data.frame(), then we use the rownames_to_column() function which sounds like its name, and pulls the row names gene_id into a new column of the dataframe. Lastly we subset the data using the select() function for only the columns we want, and order the data using the arrange() function to start with the largest values with respect to the padj via the desc() argument inside select().\ntidy_pasilla \u0026lt;- pasilla %\u0026gt;% results() %\u0026gt;% as.data.frame() %\u0026gt;% rownames_to_column(var = \u0026quot;gene_id\u0026quot;) %\u0026gt;% select(gene_id, pvalue, padj) %\u0026gt;% arrange(desc(padj)) tidy_edgeR \u0026lt;- qlf %\u0026gt;% as.data.frame() %\u0026gt;% rownames_to_column(var = \u0026quot;gene_id\u0026quot;) %\u0026gt;% select(gene_id, PValue) Using full_join() from dplyr package we are able to subset these two data frames based on the gene_id column and keeping all other matching columns.\nfull_join(tidy_pasilla, tidy_edgeR, \u0026quot;gene_id\u0026quot;) %\u0026gt;% ggplot(aes(x = pvalue, y = PValue)) + labs(x = \u0026quot;DESeq2 pvalue\u0026quot;, y = \u0026quot;edgeR PValue\u0026quot;) + scale_x_log10() + scale_y_log10() + geom_point(alpha = 0.5) + geom_abline(slope = 1, intercept = 0, color = \u0026quot;blue\u0026quot;) + geom_vline(xintercept = 1e-25, color = \u0026quot;red\u0026quot;) + ggtitle(\u0026quot;DESeq2 vs EdgeR\u0026quot;, subtitle = \u0026quot;Each point represents a single gene, p-value is for whether the gene has differential expression between groups\u0026quot;) + theme_bw() The reference line drawn here using geom_abline(slope = 1, intercept = 0) is to show what it the data would look like if the two methods were identical. Looking at the graph above, we see that most of the data are falling on relatively close to one another when our pvalue \u0026gt; 1e-25. We know this because the alpha of the ggplot object is set to 0.5, so if we see a black dot, it means there are two points, one on top of each other. This is what we would expect considering these packages DESeq2 and edgeR have the same purpose in mind and is why we are comparing their outputs in this exercise!\nBelow we subset the data again, this time selecting those points with pvalues \u0026lt;= 1e-25 (log10 transform). When plotting these we don’t see much overlapping, supporting the variation between the edgeR and DESeq2 modes of analysis.\nfull_join(tidy_pasilla, tidy_edgeR,\u0026quot;gene_id\u0026quot;) %\u0026gt;% filter(pvalue \u0026lt;= 1e-25) %\u0026gt;% ggplot(aes(x = pvalue, y = PValue)) + labs(x = \u0026quot;DESeq2 pvalue\u0026quot;, y = \u0026quot;edgeR PValue\u0026quot;) + geom_point(alpha = 0.5) + scale_x_log10() + scale_y_log10() + ggtitle(\u0026quot;Genes with DESeq pvalue \u0026lt;= -25\u0026quot;, subtitle = \u0026quot;Each point represents a single gene, p-value is for whether the gene has differential expression between groups\u0026quot;) + theme_bw()  References  https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf https://en.wikipedia.org/wiki/Quasi-likelihood http://web.stanford.edu/class/bios221/book/Chap-CountData.html https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf https://geanders.github.io/RProgrammingForResearch/   ","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588204800,"objectID":"98b25b94552c09e6749dcbe11b0ebec3","permalink":"/post/ex-8-1/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/post/ex-8-1/","section":"post","summary":"Exercise 8.1  Do the analyses of Section 8.5 with the edgeR package and compare the results: make a scatterplot of the log 10 p-values, pick some genes where there are large differences, and visualize the raw data to see what is going on. Based on this can you explain the differences?\n Most of the following code is taken straight come the book in section 8.5 for data cleaning/wrangling and DESeq2 analysis.","tags":["exercises","Chapter 8"],"title":"Exercise solution for Chapter 8, Part 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 30 Large group: meeting link\n Group 1 (Sierra, Camron, Sere): meeting link Group 2 (Burton, Mikaela, Sherry, Amy): meeting link Group 3 (Daniel, James, Sarah): meeting link   Additional links More on spatial point processes:\n https://crd150.github.io/lab6.html http://spatstat.org/ https://training.fws.gov/courses/references/tutorials/geospatial/CSP7304/documents/PointPatterTutorial.pdf  More on EBImages:\n https://www.bioconductor.org/packages/release/bioc/vignettes/EBImage/inst/doc/EBImage-introduction.html   Vocabulary quiz for April 30 Loading…   ","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588107139,"objectID":"0904a5238979fa30d069384bec6a2b15","permalink":"/post/details-for-class-on-april-30/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/post/details-for-class-on-april-30/","section":"post","summary":" Links for April 30 Large group: meeting link\n Group 1 (Sierra, Camron, Sere): meeting link Group 2 (Burton, Mikaela, Sherry, Amy): meeting link Group 3 (Daniel, James, Sarah): meeting link   Additional links More on spatial point processes:\n https://crd150.github.io/lab6.html http://spatstat.org/ https://training.fws.gov/courses/references/tutorials/geospatial/CSP7304/documents/PointPatterTutorial.pdf  More on EBImages:\n https://www.bioconductor.org/packages/release/bioc/vignettes/EBImage/inst/doc/EBImage-introduction.html   Vocabulary quiz for April 30 Loading…   ","tags":["class meeting"],"title":"Details for class on April 30","type":"post"},{"authors":["Camron Pearce"],"categories":["vocabulary","Chapter 11"],"content":"  Chapter 11 is focused on learning how to read, write, and manipulate images in R. The first sections are helping the reader understand when to apply different filters and transformations to an image and why it is necessary. It then touches on segmentation and feature extraction, two components that are utilized to simplify an image for machine learning and recognition. Finally, statistal methods are introduced to analyze spacial distributions and spatial point process is introduced on a basic level.\nThe vocabulary words for Chapter 11 are:\n        segmentation  partitioning an image to assign a label to every pixel or group of pixels with similar characteristics    slot  a part, element, or “property” of an object in the context of object-oriented programming in R    classification  the process of grouping observations in a dataset by their similarities in terms of measured characteristics    feature extraction  the process of building derived values to describe observations or features from the initial set of measured data, with the aim of creating a new set of characteristics that is informative.    spatial point process  mechanism that generates a random collection of coordinates or points randomly located along an underlying mathematical space. There is at most one point observed at any location.    Poisson process  mechanism that generates instantaneous events (in time and/or space) based on the Poisson distribution    Ripley’s K function  a descriptive statistic for detecting the deviations from spatial homogeneity that can help determine if points are random, dispersed, or clustered    pair correlation function  a description of how the point density varies as a function of distance from the point of reference    spatial transformation  changes to a coordinate system that provides a new approach to defining variation of material parameters    linear filter  a tool for refining an image such that the output pixel is a combination of the time-varying input pixels subject to the constraint of linearity    dynamic range  the ratio or logarithmic value of the difference between the largest and smallest values    noise reduction  the process of removing the undesirable variation in image pixelation    adaptive thresholding  segmenting an image using smaller regions that are defined by the range of local intensity values    binary image  an image consisting of pixels that can only have one of exactly two values, usually black and white    morphological operation  image processing in which each individual pixel in the image is adjusted based on the other pixels in the neighborhood    Voronoi tessellation  partitioning an image plane into regions closest to each set of points. Line segments are formed equidistant to the two nearest points    convex hull  the smallest polygon that encloses all of the points of interest in a set    spatial dependence  the propensity for nearby points to influence each other and possess similar attributes    virion  infectious nucleic acid surrounded by a protective protein capsid    actin  globular proteins that form the microfilaments essential for cell mobility and division    macrophages  immune cells responsible for engulfing potential pathogens and other lymphatic particles    dendritic cells  immune cells responsible for processing foreign material and presenting it to other cells in the immune system    light sheet microscopy  a method that illuminates a specimen in a single plane and detected from the perpendicular direction     Sources Consulted https://www.cso.ie/en/methods/classifications/classificationsexplained/ http://www.stat.umn.edu/geyer/8501/points.pdf\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4528837/\nhttps://www.e-education.psu.edu/natureofgeoinfo/c1_p18.html\nhttps://www.leica-microsystems.com/science-lab/topics/light-sheet-microscopy/\nhttps://www.mathworks.com/help/images/morphological-filtering.html\n Practice   ","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587767890,"objectID":"72bafb87518d67654ebda7aee740e4ff","permalink":"/post/vocabulary-for-chapter-11/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-11/","section":"post","summary":"Chapter 11 is focused on learning how to read, write, and manipulate images in R. The first sections are helping the reader understand when to apply different filters and transformations to an image and why it is necessary. It then touches on segmentation and feature extraction, two components that are utilized to simplify an image for machine learning and recognition. Finally, statistal methods are introduced to analyze spacial distributions and spatial point process is introduced on a basic level.","tags":["vocabulary"],"title":"Vocabulary for Chapter 11","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 23 Large group: meeting link\n Group 1 (Sierra, Sarah, Daniel, Sere): meeting link Group 2 (Mikaela, Sherry, James): meeting link Group 3 (Burton, Camron, Amy): meeting link   Additional links  Link for HIV amino acid sequences: https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html Online book on ggtree: https://yulab-smu.github.io/treedata-book/ Introductory vignette for phangorn: https://cran.r-project.org/web/packages/phangorn/vignettes/Trees.pdf Vignette for ape: https://cran.r-project.org/web/packages/ape/vignettes/MoranI.pdf Online book with (much!) more on studying networks: http://networksciencebook.com/ Relevant COVID-19 article: https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30273-5/fulltext#.Xp_6SyUko0Y.twitter   Slido comments  https://admin.sli.do/event/lkcufqsv/questions   Vocabulary quiz for April 23 Loading…   ","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587606297,"objectID":"9dd7feb31b35870ea662ccd1a51a32b8","permalink":"/post/details-for-class-on-april-23/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/details-for-class-on-april-23/","section":"post","summary":"Links for April 23 Large group: meeting link\n Group 1 (Sierra, Sarah, Daniel, Sere): meeting link Group 2 (Mikaela, Sherry, James): meeting link Group 3 (Burton, Camron, Amy): meeting link   Additional links  Link for HIV amino acid sequences: https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html Online book on ggtree: https://yulab-smu.github.io/treedata-book/ Introductory vignette for phangorn: https://cran.r-project.org/web/packages/phangorn/vignettes/Trees.pdf Vignette for ape: https://cran.r-project.org/web/packages/ape/vignettes/MoranI.pdf Online book with (much!) more on studying networks: http://networksciencebook.com/ Relevant COVID-19 article: https://www.","tags":["class meeting"],"title":"Details for class on April 23","type":"post"},{"authors":["Sarah Cooper"],"categories":["vocabulary","Chapter 10"],"content":"  Chapter 10 discusses the use of networks and trees to visualize biological data. It covers the main components of each and how different data sets can be appropriately transformed into specific networks and trees based on what you are trying to present. The vocabulary words for Chapter 10 are:\n        Graph  A structure formed by a set of nodes or vertices and a set of edges between these vertices    Adjacency matrix  The matrix representation of edges of a graph with as many rows as nodes in the graph    Network  A weighted, directed graph    Sparse  In the context of graphs, a term to describe a graph when the number of edges is similar to the number of nodes    Dense  In the context of graphs, a term to describe a graph when the number of edges is (approximately) a quadratic function of the nodes    Arrows/directed edges  Graph edges that directionally connect nodes    Annotation variables  Graph visualization characteristics that help to demonstrate strength of a link in a graph by changing the width of the edge or covariates associated to the size or color of the node    Graph layouts  Different ways to plot a graph, either for aesthetic or practical reasons    Binary data  Data in which each observation can take only one of two values (e.g., 0 or 1)    Differentially expressed genes  A term to describe changes in gene expression levels between different experimental groups    Bipartite graph  A graph where each edge connects a node    Overrepresented or enriched  In the context of gene expresion, a term to describe increased expression of a gene / set of genes of interest    Gene Ontology (GO)  A resource aimed to unify the vocabulary to describe genes and gene product attributes across all species    Fisher’s exact test / Hypergeometric testing  Two-way table testing used to account for the fact that some categories are extremely numerous and others are rarer    Known skeleton graph  A graph that projects significance scores such as p-values    Perturbation  In the context of a network, an alteration of the function of a biological system, induced by external or internal mechanisms    Hotspots  In the context of a graph, areas with high event density    Rooted binary tree  A data tree in which each node has at most two children    Cycles  In the context of graphs, another word for loops: either self-loops or ones that go through several vertices    Ancestral taxa  Correspond to inner nodes and are inferred from the contemporaneous data on the tips    Contemporaneous  In the context of data for phylogenetic tress, organisms at terminal nodes that exist at the same time and are related to each other, thus revealing information about their common ancestors    OTUs (Operational Taxonomic Units)  A method of clustering organisms based on DNA similarity of a certain taxonomic marker gene (Tips of the tree)    Parameter  In the context of statistics, numerical value that describes a population    Gene trees  Structures produced when different genes show differences in their evolutionary histories    Markov chain  A sequence where given the current state, the next state is conditionally independent of all previous states    Molecular clock hypothesis  A term that describes a technique that uses the mutation rate to infer what happened to a species historically    Non-identifiability  The inability to distinguish a parameterization of a model based on observed data    Time homogeneity  In the context of Markov chains, the state of the mutation rate being constant across history    Generator  In the context of Markov chains, the instantaneous change probability matrix describing transitions between steps of the chain    Transition matrix  A matrix that contains all probabilities of any state changes for a Markov chain    Parsimony tree  A structure created using nonparametric method that minimizes the number of changes necessary to explain the data    Maximum likelihood tree  A structure created using a parametric method that uses efficient optimization algorithms to maximize the likelihood of a tree under the model assumptions.    Bayesian posterior distributions for trees  A method that uses MCMC to find posterior distributions of the phylogenies    Distance-based methods  Semi-parametric methods similar to the hierarchical clustering algorithms but use the parametric evolutionary models    Aligning  Arranging different sequences of DNA, RNA, or protein together to identify similarities or differences between them    indel (inserted-deleted) event  Insertion or deletion of bases in the genome of an organism    Filtering operations  In the context of low-quality rRNA reads, the removal of low-quality reads and trimming of remaining reads to a consistent length    Interactive  In the context of a plot, enabling direct actions on a graphical plot to change elements and link multiple plots    Spanning tree  A tree that goes through all points at least once    Minimum spanning tree (MST)  Given distances between vertices, a tree that spans all the points and has the minimum total length    Jitter  To slightly move coordinates on a graph to avoid too much overlapping    Undirected network  A term describing a graph without arrows between nodes    Associated  A term indicating that two variables are related    Friedman-Rafsky tests  Tests for two/multiple sample segregation on a minimum spanning tree    Pure edges  In the context of graphs, edges whose two nodes have the same level of the factor variable    Microbiome  The aggregate of all microbiota that reside on or within an organism tissues and biofluids along with the corresponding anatomical sites in which they reside    Exponential random graph models (ERGMs)  Models that can be used to predict vertex covariates    Protein interaction networks  In the context of graphing, a way to visualize observed protein-protein interactions    Phylogenetic tree  A tree used to visualize evolutionary relationships among species    Strain  In the context of a virus, a genetic variant or subtype    Taxa  A group of one or more populations of an organism making up a single unit, typically disected to the level of genus and species    Protein  A compound made up of amino acids; one of the four types of macromolecules that make up living organisms.     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page   Practice   ","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586917751,"objectID":"3f55cfe386c5e28463c916e16a0504a2","permalink":"/post/vocabulary-for-chapter-10/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-10/","section":"post","summary":"Chapter 10 discusses the use of networks and trees to visualize biological data. It covers the main components of each and how different data sets can be appropriately transformed into specific networks and trees based on what you are trying to present. The vocabulary words for Chapter 10 are:\n        Graph  A structure formed by a set of nodes or vertices and a set of edges between these vertices    Adjacency matrix  The matrix representation of edges of a graph with as many rows as nodes in the graph    Network  A weighted, directed graph    Sparse  In the context of graphs, a term to describe a graph when the number of edges is similar to the number of nodes    Dense  In the context of graphs, a term to describe a graph when the number of edges is (approximately) a quadratic function of the nodes    Arrows/directed edges  Graph edges that directionally connect nodes    Annotation variables  Graph visualization characteristics that help to demonstrate strength of a link in a graph by changing the width of the edge or covariates associated to the size or color of the node    Graph layouts  Different ways to plot a graph, either for aesthetic or practical reasons    Binary data  Data in which each observation can take only one of two values (e.","tags":["vocabulary","Chapter 10"],"title":"Vocabulary for Chapter 10","type":"post"},{"authors":["Daniel Dean"],"categories":["exercises","Chapter 9"],"content":" Exercise 9.2  “Correspondence Analysis on color association tables: Here is an example of data collected by looking at the number of Google hits resulting from queries of pairs of words. The numbers in Table 9.4 [not reproduced] are to be multiplied by 1000. For instance, the combination of the words “quiet” and “blue” returned 2,150,000 hits. Perform a correspondence analysis of these data. What do you notice when you look at the two-dimensional biplot?\"\n In this exercise, we will essentially be repeating the correspondence analysis process seen in section 9.4.2, this time using associations between color and sentiment terms in search engine queries, rather than hair and eye color. Rather than testing whether certain hair/eye combinations are more likely to co-occur, we will be exploring whether a given color is more or less likely to be associated with certain sentiments (e.g. would we expect “orange” and “happy” to co-occur more frequently than would be expected by chance?). The same general steps can be repeated without many changes.\n Step 1: Loading libraries library(ade4) library(sva) library(tidyverse) library(factoextra) library(janitor) #optional; function `clean_names()` makes column names easier to work with library(ggplot2) library(ggrepel)  Step 2: Two ways to load data: At least working from the online version of the text, there are two ways to obtain (roughly) equal data for this exercise. One option is to copy table 9.4 directly from the book into Excel, shift the header one cell to the right to align columns with their proper names, export a .csv (ex_9.2_color_table.csv in this example), and load it into R using a command like:\ncolor_matrix \u0026lt;- read_csv(\u0026quot;example_datasets/ex_9_2_color_table.csv\u0026quot;, col_names = TRUE) %\u0026gt;% column_to_rownames(var = \u0026#39;X1\u0026#39;) %\u0026gt;% as.matrix Something to note is that this data is rounded to units of thousands (e.g. “19” is ~19,000), while the course data in the downloadable data file gives more-precise values. I don’t know that this would disrupt any major correlations, but it could cause some minor discrepancies on comparison.\nAlternatively, the file is included in the course data as colorsentiment.csv, although in a different, three-column format:\nhead(read_csv(\u0026quot;example_datasets/colorsentiment.csv\u0026quot;)) ## Parsed with column specification: ## cols( ## `\u0026lt;X\u0026gt;` = col_character(), ## `\u0026lt;Y\u0026gt;` = col_character(), ## Results = col_double() ## ) ## # A tibble: 6 x 3 ## `\u0026lt;X\u0026gt;` `\u0026lt;Y\u0026gt;` Results ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 blue happy 8310000 ## 2 blue depressed 957000 ## 3 blue lively 1250000 ## 4 blue clever 1270000 ## 5 blue perplexed 71300 ## 6 blue virtuous 80200 This can be converted to match our correspondence table format using the pivot_wider function and other tidyverse formatting tools:\ncolor_matrix \u0026lt;- read_csv(\u0026quot;example_datasets/colorsentiment.csv\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% #standardizes column name format arrange(desc(results)) %\u0026gt;% # Rearranging to match row/col order in table 9.4 pivot_wider(names_from = x, values_from = results) %\u0026gt;% # converts colors from column entries (`x`) to column names column_to_rownames(var = \u0026#39;y\u0026#39;) color_matrix ## black white green blue orange purple grey ## happy 19300000 9150000 8730000 8310000 4220000 2610000 1920000 ## angry 2970000 1730000 1740000 1530000 1040000 710000 752000 ## quiet 2770000 2510000 2140000 2150000 1220000 821000 875000 ## lively 1840000 1480000 1350000 1250000 621000 488000 659000 ## clever 1650000 1420000 1320000 1270000 693000 416000 495000 ## depressed 1480000 1270000 983000 957000 330000 102000 147000 ## virtuous 179000 165000 102000 80200 24700 17300 20000 ## perplexed 110000 109000 80100 71300 23300 15200 18900  Correspondence Analysis (following section 9.4.2 as a model) Setting up the correspondence analysis object using the correspondence analysis dudi function from the ade4 package:\ncolor_matrix_ca \u0026lt;- dudi.coa(color_matrix, n = 2, scannf = FALSE) # scannf = FALSE stops automatic printing of eigenvalues This creates a special “Duality Diagram” list object (used by the ade4 package for correspondence analysis, but also principal component analysis and other methods) containing a variety of data generated by the analysis; the call ?dudi() will give a list of what each component contains (axis weights, point coordinates, etc.).Stored features include as the base data table (tab), a vector of eigenvalues (eig), and a variety of information on row and column data (e.g. weights, coordinates, and principal components).\nQuestion 9.2 specifies that we use two dimensions, but visualizing the eigenvalues with the factoextra package confirms that this is a good representation of the system, with almost all variance being explained by the first two dimensions:\nfviz_eig(color_matrix_ca, geom = \u0026#39;bar\u0026#39;) # visualizing eigenvalues  Following the book’s example in 9.4.2, we can explicitly calculate a residual matrix, which shows the difference between expected (assuming random distribution) and observed values for given row/column intercepts, in the following steps. This doesn’t feed into visualizations, but it may be helpful to have a quantitative reference for residuals:\nrowsums_colors \u0026lt;- as.matrix(apply(color_matrix, 1, sum)) colsums_colors \u0026lt;- as.matrix(apply(color_matrix, 2, sum)) expected_colors \u0026lt;- rowsums_colors %*% t(colsums_colors) / sum(colsums_colors) # using matrix multiplication to see what # \u0026quot;average\u0026quot; values should look like if row and column sums are distributed evenly across the dataset #sum((color_matrix - expected_colors)^2 / expected_colors) # subtracting the \u0026quot;expected\u0026quot; matrix from the observed values to see discrepancies (residual_table_colors \u0026lt;- color_matrix - expected_colors %\u0026gt;% t() %\u0026gt;% round()) ## black white green blue orange purple grey ## happy 2604538 7252731 6644019 7090159 3616948 2332753 1890798 ## angry -6856953 -19511 -241131 891748 657779 448415 620320 ## quiet -6291637 848427 1103422 1745469 859372 639948 797493 ## lively -6766161 610622 693006 868322 -1000836 381433 587529 ## clever -2852964 868979 700121 -965911 -261613 317732 427122 ## depressed -1374026 750108 -1383422 -359058 -550269 8671 111484 ## virtuous -2513797 -3678280 -1290876 -1133364 -811323 -31532 -2510 ## perplexed -3113357 -2153156 -1204300 -1081265 -414128 -15750 -2339 Here, we can see that, for instance, the combination of “happy” and “black” in searches occurs significantly more often (~26,000,000 additional instances) than we’d expect given no correlations between colors and sentiments, while e.g. “happy” and “grey” is much rarer.\nTo take these patterns more intuitively, we can use a mosaic plot to visualize the proportional distribution of color/sentiment terms in searches (e.g. “back” and “happy” are both popular terms, so could be expected to occur more frequently than e.g. “perplexed” and “purple” in any case), as well as color coding for residuals:\nmosaicplot(t(color_matrix / 2000), las = 2, shade = TRUE, type = \u0026#39;pearson\u0026#39;) # dividing values by 2,000 because the mosaic plot function doesn\u0026#39;t seem to auto-scale colors, meaning that the unaltered matrix is all \u0026quot;flattened\u0026quot; to the \u0026lt;4 or \u0026gt;4 category.  #transposing is just aesthetic; seems easier to follow with sentiments on the y axis as far as labels and visual row/column continuity.  From this pattern, it’s easier to see broad patterns and associations among colors and sentiments. For instance, if we focus on colors, we can see black is quite distinct from most colors, with more than expected with happy and lower for other sentiments. All other sentiments behave more like each other than black, but do show a smaller division between white, blue, and green; this is distinguished from orange, purple, and grey by being rarer than expected (under random distribution) with angry and more common with depressed, virtuous, and perplexed. Based on this, in a 2D projection we might expect to see the largest separation between black and all other colors, with a smaller but obvious distinction between the two other color clusters. Because quiet, lively, and clever are more common than expected for everything but black, the will likely be about equidistant between these clusters.\nTo check how close we got with these eyeballed estimates, we can use the factoextra biplot visualization function fviz_ca_biplot for correspondence analysis to see our biplot for sentiment and color searches. I thought using the option to represent one as vector arrow, rather than points, also improves legibility (e.g. the relationship between angry and orange/purple become more obvious):\nfviz_ca_biplot(color_matrix_ca, arrows = c(FALSE, TRUE), repel = TRUE) As we can see, sentiment and color groups show similar relationships from what we might expect comparing patterns of positive/negative residuals in the mosaic plot. However, this makes it easier to see some patterns, such as the strong opposition between black and grey on dimension 1, or the fact that most of dimension 2 is due to the differences of green-white-blue and perplexed-depressed-virtuous from the rest of the data. We can also make out some smaller trends that weren’t obvious (at least to me) from the mosaic visualization, like grey being more distinct from orange and purple than we could make out with the mosaic plot’s effective “resolution”. This separation appears to be driven by higher co-occurrence with lively, quiet, and clever. Looking back to our mosaic plot, the latter three have lighter shades of blue in orange and purple, with no obvious difference with angry.\n Plotting directly with ggplot: While factoextra uses custom functions to streamline the process, it’s possible to approximate the same visualization using ggplot and components of the dudi object. In the color_matrix_ca object, the ‘row’ and ‘column’ factor coordinates (emotion and color, respectively) are stored at .$li and .$co, This allows direct plotting; you could also look at e.g. normalized scores in .$l1 and .$c1 as well. I was able to get a general sense of how the factoextra authors approached this using the call View(fviz_ca_biplot) to pull up the function’s R code; this ultimately pointed to the more fundamental fviz function.\nUsing this information, we can render single plots for color:\n#Single plots: (roughly equivalent to default output for `fviz_ca_col` and `fviz_ca_row`) color_nmds \u0026lt;- color_matrix_ca$co %\u0026gt;% ggplot() + aes(x = Comp1, y = Comp2) + geom_point(color = \u0026#39;blue\u0026#39;) + geom_text(label = rownames(color_matrix_ca$co), nudge_y = 0.01, color = \u0026#39;blue\u0026#39;) + coord_fixed() color_nmds and sentiment (below). We can also use the geom_segment function in ggplot to replicate the arrows seen in the factoExtra biplot:\nemotion_nmds \u0026lt;- color_matrix_ca$li %\u0026gt;% ggplot() + aes(x = Axis1, y = Axis2) + geom_point(color = \u0026#39;red\u0026#39;) + ggrepel::geom_text_repel(label = rownames(color_matrix_ca$li), nudge_y = 0.01, color = \u0026#39;red\u0026#39;) + geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2), arrow = arrow(length = unit(0.3,\u0026quot;cm\u0026quot;)), color = \u0026quot;red\u0026quot;) + coord_fixed() emotion_nmds We can combine these elements into a biplot by combining the above dataframes, with one column for each value, and plotting the result:\n# Biplot (there are probably more efficient/correct approaches) #manually joining the two datasets using a common index (generates a partial row of NAs, with 8 sentiments and 7 colors) color_component \u0026lt;- color_matrix_ca$co %\u0026gt;% rownames_to_column(var = \u0026quot;color\u0026quot;) %\u0026gt;% rownames_to_column(var = \u0026quot;index\u0026quot;) emotion_component \u0026lt;- color_matrix_ca$li %\u0026gt;% rownames_to_column(var = \u0026quot;emotion\u0026quot;) %\u0026gt;% rownames_to_column(var = \u0026quot;index\u0026quot;) biplot_composite \u0026lt;- color_component %\u0026gt;% full_join(emotion_component, by = \u0026quot;index\u0026quot;) (biplot_composite) ## index color Comp1 Comp2 emotion Axis1 Axis2 ## 1 1 black 0.17794592 0.04039331 happy 0.11532145 0.01602901 ## 2 2 white -0.05317953 -0.10399481 angry -0.10756934 0.09837664 ## 3 3 green -0.03347037 -0.03237667 quiet -0.20048082 -0.01564279 ## 4 4 blue -0.03552655 -0.04588027 lively -0.20105512 0.01174266 ## 5 5 orange -0.09284092 0.07047109 clever -0.17886743 -0.03423825 ## 6 6 purple -0.15049601 0.15422498 depressed 0.03935532 -0.24782949 ## 7 7 grey -0.36826914 0.10335528 virtuous 0.05572888 -0.24684976 ## 8 8 \u0026lt;NA\u0026gt; NA NA perplexed -0.04792928 -0.22173448 biplot_composite_plot \u0026lt;- biplot_composite %\u0026gt;% ggplot() + aes(x = Comp1, y = Comp2) + geom_point(color = \u0026#39;red\u0026#39;) + geom_text(label = biplot_composite$color, nudge_y = 0.01, color = \u0026#39;red\u0026#39;) + geom_segment(aes(x = 0, y = 0, xend = Comp1, yend = Comp2), arrow = arrow(length = unit(0.3,\u0026quot;cm\u0026quot;)), color = \u0026quot;red\u0026quot;) + geom_point(aes(x = Axis1, y = Axis2), color = \u0026#39;blue\u0026#39;) + geom_text_repel(aes(x = Axis1, y = Axis2), label = biplot_composite$emotion, nudge_y = 0.01, color = \u0026#39;blue\u0026#39;) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty = 2) + labs(x = paste0(\u0026quot;Dim1(\u0026quot;,round(color_matrix_ca$eig[1]/sum(color_matrix_ca$eig)*100, 1),\u0026quot;%)\u0026quot;), y = paste0(\u0026quot;Dim2(\u0026quot;,round(color_matrix_ca$eig[2]/sum(color_matrix_ca$eig)*100, 1),\u0026quot;%)\u0026quot;) + coord_fixed() ) biplot_composite_plot ## Warning: Removed 1 rows containing missing values (geom_point). ## Warning: Removed 1 rows containing missing values (geom_text). ## Warning: Removed 1 rows containing missing values (geom_segment).  Second Approach Dr. Anderson also worked out a more-efficient approach making use of purrr package function, unclass, map_at and other tools to unite the two coa / dudi objects with fewer intermediate steps:\ncolor_matrix_ca %\u0026gt;% # `unclass` to work with this as a regular list unclass() %\u0026gt;% # `keep` lets you keep just some elements of a list. We\u0026#39;ll keep \u0026quot;co\u0026quot; and \u0026quot;li\u0026quot; elements keep(names(.) %in% c(\u0026quot;co\u0026quot;, \u0026quot;li\u0026quot;)) %\u0026gt;% # both of these have important info in their rownames, so move those into a column. # `map` allows you to do the same thing to every element of the list (now just \u0026quot;co\u0026quot; and \u0026quot;li\u0026quot; map(rownames_to_column) %\u0026gt;% # `map_at` lets you do something to *just* some elements of a list. So here, to be able # to bind the two dataframe elements in the list into one dataframe, you need to # make sure they have the same column names. Currently, they don\u0026#39;t, so we need to # change the column names for one of them. map_at(\u0026quot;co\u0026quot;, ~ rename(.x, Axis1 = Comp1, Axis2 = Comp2)) %\u0026gt;% # Now that we have a list where each element is a dataframe with the same number # of columns, and where those columns have the same names and data types, you # can use `bind_rows` to stick them together into one dataframe (it turns out that # this is a *super* helpful function). After this step, you have a tidy dataframe. The # `.id` parameter is adding a column with the original list element name, so you can # tell which rows originally came from \u0026quot;co\u0026quot; and which from \u0026quot;li\u0026quot; bind_rows(.id = \u0026quot;id\u0026quot;) %\u0026gt;% ggplot(aes(x = Axis1, y = Axis2, color = id)) + geom_point() + # You can add arrows to your segments with the `arrow` function (from the # `grid` package, which is very old school graphics and what ggplot is built on) # To have everything come from the center, you set the starting point to 0 for # both x- and y-axes geom_segment(aes(x = 0, y = 0, xend = Axis1, yend = Axis2), arrow = arrow(length = unit(0.1, \u0026quot;inches\u0026quot;))) + geom_text_repel(aes(label = rowname)) + # `coord_fixed` ensures that the x- and y-axes are scaled so they have the # same unit size coord_fixed() + # `str_c` lets you stick character strings together (`paste0` would also work here) labs(x = str_c(\u0026quot;Dim 1 (\u0026quot;, round(100 * color_matrix_ca$eig[1] / sum(color_matrix_ca$eig), 1), \u0026quot;%)\u0026quot;), y = str_c(\u0026quot;Dim 2 (\u0026quot;, round(100 * color_matrix_ca$eig[2] / sum(color_matrix_ca$eig), 1), \u0026quot;%)\u0026quot;)) + scale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;)) + # Get rid of the color legend theme(legend.position = \u0026quot;none\u0026quot;) + # Add some reference lines for 0 on the x- and y-axes geom_hline(aes(yintercept = 0), linetype = 3) + geom_vline(aes(xintercept = 0), linetype = 3)  ","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587439327,"objectID":"917ac810562487ec558ba1b0a5192055","permalink":"/post/exercise-solution-for-chapter-9/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-9/","section":"post","summary":"Exercise 9.2  “Correspondence Analysis on color association tables: Here is an example of data collected by looking at the number of Google hits resulting from queries of pairs of words. The numbers in Table 9.4 [not reproduced] are to be multiplied by 1000. For instance, the combination of the words “quiet” and “blue” returned 2,150,000 hits. Perform a correspondence analysis of these data. What do you notice when you look at the two-dimensional biplot?","tags":["exercises","Chapter 9"],"title":"Exercise solution for Chapter 9","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 16 Large group: meeting link\n Group 1 (Sarah, James, Burton, Mikeala): meeting link Group 2 (Daniel, Amy, Sierra): meeting link Group 3 (Sere, Camron, Sherry): meeting link   Space for comments, questions  For Chapter 9: https://www.sli.do/ Event code: #MSMB_CH9 For Chapter 10 (will be live starting this weekend): https://www.sli.do/ Event code: #MSMB_CH10   Vocabulary quiz for April 16 Loading…   ","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587065300,"objectID":"f84c67b395e7a50d1ff7e669661c0f1a","permalink":"/post/details-for-class-april-16/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/details-for-class-april-16/","section":"post","summary":" Links for April 16 Large group: meeting link\n Group 1 (Sarah, James, Burton, Mikeala): meeting link Group 2 (Daniel, Amy, Sierra): meeting link Group 3 (Sere, Camron, Sherry): meeting link   Space for comments, questions  For Chapter 9: https://www.sli.do/ Event code: #MSMB_CH9 For Chapter 10 (will be live starting this weekend): https://www.sli.do/ Event code: #MSMB_CH10   Vocabulary quiz for April 16 Loading…   ","tags":["class meeting"],"title":"Details for class April 16","type":"post"},{"authors":["Sere Williams"],"categories":["vocabulary","Chapter 9"],"content":"  Chapter 9 covers multivariate methods for heterogenous data. It builds on methods covered in Chapter 7, like dimension reduction, by extending these ideas to more complex, heterogenous data.\nThe vocabulary words for Chapter 9 are:\n        multidimensional scaling (MDS)  a linear dimension reduction method applied in cases where distances between observations are available    clusters  in the context of data analysis, data points that group together    robust  in the context of a statistical method, a ‘sturdy’ estimator that is not heavily influenced by outliers    outlier  a single data point with large distances to other data points, thus potentially dominating and skewing the analysis    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    non-metric multidimensional scaling (NMDS)  a robust ordination method which attempts to embed data points in a new space while maintaining their respective order to one another    metadata  information, data, or descriptions that characterize other data    batch effects  hidden factors that affect the data but are not documented; e.g. running samples at the same time have a degree of similarity from being run in the same batch    confounded effects  a term describing when there is uncertainty in the source of variation impacting data    supplementary  in the context of variables for a statistical model, categorical variables added to continuous variables in heterogenous data    supplementary points  points created using the group-means of points in each of the groups    interactive  in the context of plots, data visualizations that can be manipulated in real time by the observer    contingency table  the result of counting the co-occurrence of any pair of categorical variables measured in a set of observations; for example, two phenotypes    chi-square distance  weighted Eucledian distance using relative counts and standardized by the mean, not the variance    biplots  a type of exploratory graph that displays information on both the observations and the variables of a data matrix    co-occurence matrix  a matrix that captures the extent to which variables are jointly observed in observations    correspondence analysis (CA) / dual scaling  a method for computing low dimensional projections that explain dependencies in categorical data    ordination method  a method which enables one to detect and interpret a hidden ordering, gradient or latent variable in the data    clustering  in the context of statistical methods, a way to detect and interpret a hidden factor/categorical variable    kernel  a linear algorithm designed to determine a non-linear decision boundary; used in pattern analysis to better understand general types of relations like clusters, rankings, principal components, or correlations    local linear embedding (LLE)  a nonlinear method for estimating nonlinear trajectories by points in the relevant state spaces    isomap  a nonlinear method for estimating nonlinear trajectories by points in the relevant state spaces    inertia  in the context of counts in a contingency table, the weighted sum of the squares of distances between observed and expected frequencies    covariance  measure of the joint variability of two random variables    matrix association  correlation of vectors derived from matrices based on dissimilarity    RV coefficient  the global measure of similarity of two data tables as opposed to two vectors; correlation coefficient for tables    penalty  a method to constrain the typical optimization algorithm, added to interpret correlation when there are too many degrees of freedom    sparsity penalty  an approach to maintain the number of non-zero coefficients to a minimum    heterogenous data  a mixture of many continuous and a few categorical variables    canonical correlation  a method for finding a few linear combinations of variables from each table that are as correlated as possible    nonlinear  a regression equation where the equation is not ‘linear in the parameters,’ meaning the relationship between parameters cannot be calulated by multiplying, exponentiating, or transforming independent variables    species tree  a simplified term for a diagram showing the relatedness of organisms based on biological, often genetic sequence, information    assay  an investigative (analytic) procedure in laboratory medicine, pharmacology, environmental biology and molecular biology for qualitatively assessing or quantitatively measuring the presence, amount, or functional activity of a target entity (the analyte)    protocol  a predefined written procedural method of conducting experiments    microarray  a ‘lab-on-a-chip’ method to assess many samples at once, often used in gene expression studies    taxon  a group of one or more populations of an organism making up a single unit, typically disected to the level of genus and species    mutation  an alteration in the nucleotide sequence of the genome of an organism or virus    phenotype  a visually observed genetic trait or characteristic    cell development  the process of a cell transitioning from one state to another, such as in the case of a cell transitioning from growth to division in mitosis    metabolite  an intermediate or end product of metabolism; typically a small, organic molecule     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. http://www.econ.upf.edu/~michael/stanford/maeb4.pdf https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/ https://www.wikipedia.org   Practice   ","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586551291,"objectID":"b8ab79ced61153083dce2875f84e1d4b","permalink":"/post/vocabulary-for-chapter-9/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-9/","section":"post","summary":"Chapter 9 covers multivariate methods for heterogenous data. It builds on methods covered in Chapter 7, like dimension reduction, by extending these ideas to more complex, heterogenous data.\nThe vocabulary words for Chapter 9 are:\n        multidimensional scaling (MDS)  a linear dimension reduction method applied in cases where distances between observations are available    clusters  in the context of data analysis, data points that group together    robust  in the context of a statistical method, a ‘sturdy’ estimator that is not heavily influenced by outliers    outlier  a single data point with large distances to other data points, thus potentially dominating and skewing the analysis    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    non-metric multidimensional scaling (NMDS)  a robust ordination method which attempts to embed data points in a new space while maintaining their respective order to one another    metadata  information, data, or descriptions that characterize other data    batch effects  hidden factors that affect the data but are not documented; e.","tags":["vocabulary","Chapter 9"],"title":"Vocabulary for Chapter 9","type":"post"},{"authors":["Sierra Pugh"],"categories":["exercises","Chapter 7"],"content":" Exercise 7.4 from Modern Statistics for Modern Biology  Let’s revisit the Hiiragi data and compare the weighted and unweighted approaches. 7.4a Make a correlation circle for the unweighted Hiiragi data xwt. Which genes have the best projections on the first principal plane (best approximation)? 7.4b Make a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.\n Read in and clean data We start by loading the libraries:\nlibrary(\u0026quot;Hiiragi2013\u0026quot;) library(\u0026quot;ade4\u0026quot;) library(\u0026quot;factoextra\u0026quot;) library(\u0026quot;pander\u0026quot;) library(\u0026quot;knitr\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;ggrepel\u0026quot;) You can install the libraries using install.packages for the ade4 and factoextra packages and such, and the following line for Hiiragi2013:\nBiocManager::install(\u0026quot;Hiiragi2013\u0026quot;) We will analyze data from the Hiiragi2013 package containing a gene expression microarray dataset describing the transcriptomes of about 100 cells from mouse embryos at different times during early development.\nI copied the code from the textbook to clean the data. A tidyverse version of this code is available on Brooke Andersons’s github. In either case, we select the wildtype (WT) samples. Then we select the 100 features with the largest variance.\ndata(\u0026quot;x\u0026quot;, package = \u0026quot;Hiiragi2013\u0026quot;) xwt \u0026lt;- x[, x$genotype == \u0026quot;WT\u0026quot;] sel \u0026lt;- order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100] xwt \u0026lt;- xwt[sel, ] The resulting data is of the ExpressionSet class, a class designed to combine many types of data such as microarray data, metadata, and protocol information. The data is shown below. I transposed the data so the rows correspond to samples and the first 101 columns correspond to different genes. I only showed the first the columns and then skipped to the end for brevity. The last columns give additional data about each sample, such as the total number of cells and the scan date.\nkable(head(as.data.frame(xwt)[,c(1:3,102:108)]))    X1434584_a_at X1437325_x_at X1420085_at Embryonic.day Total.number.of.cells lineage genotype ScanDate sampleGroup sampleColour    1 E3.25 13.24888 2.332223 3.027715 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6  2 E3.25 11.98757 2.475742 9.293017 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6  3 E3.25 12.72695 1.955642 2.940142 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6  4 E3.25 12.51926 2.061255 9.715243 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6  5 E3.25 12.01299 2.308667 8.924228 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6  6 E3.25 10.50750 2.202948 11.325952 E3.25 32  WT 2011-03-16 E3.25 #CAB2D6     a) Make a correlation circle Next we plot a correlation circle. This allows us to plot the original genes projected onto the first two principal axes. We can interpret the angles between the vectors as a measure of the correlation between the two corresponding genes. The length of the arrows indicates the correlation of a gene with the first principal axes. This allows us to visualize the correlation between genes, as well as see which genes are best described by our first two principal components. The first principal component is the linear combination of the genes with maximum variance. The second principal component is the linear combination of the genes with maximum variance while being orthogonal to the first principal component.\nFirst I used dudi.pca to perform principal component analysis and get a pca and dudi class object. In the text book, they performed a weighted PCA because the groups had very different sample sizes ranging from 4 to 36. A weighted analysis would give the groups equal weight, while an unweighted would give each observation equal weight (and thus give less weight to groups with less members). We are interested in the difference in the genes at the various developmental phases, so in general it would make sense to do a weighted analysis in order to let each developmental phase group have an equal say. To compare the weighted with an unweighted PCA I didn’t use the row.w option as the textbook did in order to fit an unweighted PCA.\nIn the following code t takes the transpose so the rows correspond to samples. The exprs function is used to access the expression and error measurements of the data. I chose to center and scale the data, meaning make each column have a mean of 0 and a variance of 1. The scannf option prevents the function from plotting a scree plot and nf tells it to keep two axes (i.e. 2 principal components).\npcaMouse \u0026lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))), center = TRUE, scale = TRUE, nf = 2, scannf = FALSE) Then I used the fviz_pca_var function to plot the correlation circle. I used geom= \"arrow\" to remove the labels of each arrow as they were all overlapping. The default for this argument is to print both arrows and text labels for each line.\nfviz_pca_var(pcaMouse, col.circle = \u0026quot;black\u0026quot;,geom= \u0026quot;arrow\u0026quot;) + ggtitle(\u0026quot;\u0026quot;)  We can compare that plot to the one below showing the correlation circle from a weighted analysis. We can see in the weighted analysis the first principal plane does a better job of explaining the variation as indicated by the percent of variation explained by each dimension as well as the length of the arrows.\ntab = table(xwt$sampleGroup) xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup]) pcaMouseWeighted \u0026lt;- dudi.pca(as.data.frame(t(Biobase::exprs(xwt))), row.w = xwt$weight, center = TRUE, scale = TRUE, nf = 2, scannf = FALSE) fviz_pca_var(pcaMouseWeighted, col.circle = \u0026quot;black\u0026quot;,geom= \u0026quot;arrow\u0026quot;) + ggtitle(\u0026quot;\u0026quot;)  The genes that have the best projections on the first principal plane (meaning those with the strongest correlation with the principal axes) are those with the longest arrows on the plots above. I extracted the genes with arrow lengths greater than 0.8 from the unweighted PCA plot with the following code:\ncorrCircle \u0026lt;- fviz_pca_var(pcaMouse, col.circle = \u0026quot;black\u0026quot;)$data arrowLengths \u0026lt;- sqrt(corrCircle$x^2+corrCircle$y^2) cutoff \u0026lt;- 0.8 kpInd \u0026lt;- order(arrowLengths, decreasing=TRUE)[1:sum(arrowLengths\u0026gt;cutoff)] genes \u0026lt;- corrCircle[kpInd,\u0026quot;name\u0026quot;] genes 1456270_s_at, 1450624_at, 1449134_s_at, 1418153_at, 1420085_at, 1420086_x_at, 1434584_a_at, 1450843_a_at, 1429483_at, 1437308_s_at, 1456598_at, 1460605_at, 1429388_at, 1426990_at, 1436392_s_at and 1452270_s_at\nTo summarize, the code above saves output from the fviz_pca_var function to an object called corrCircle. I used the output (which included x and y coordinates) to calculate the arrow length for each gene. I then created a vector, kpInd, with the indice number for genes that had a length greater then the cutoff of 0.8. Finally, I output the names of those genes with length great than 0.8. Below is a plot showing just these genes with the ``best projection,\" meaning they are most correlated with the plane of maximum variation.\ncorrCircle %\u0026gt;% mutate(length = sqrt(x^2 + y^2)) %\u0026gt;% filter(length \u0026gt;= 0.8) %\u0026gt;% ggplot(aes(x = 0, xend = x, y = 0, yend = y)) + geom_segment(arrow = arrow(length = unit(0.1, \u0026quot;inches\u0026quot;))) + geom_label_repel(aes(x = x, y = y, label = name), size = 2, alpha = 0.7) + coord_fixed()  b) Make a biplot Next we make a biplot to visualize samples and the genes in one plot. I used the fviz_pca_biplot function. The col.var and col.ind options allow you to color the different genes and sample points, respectively, by particular groups. I used label=\"\" to remove the labels on the vectors and points. The above8 variable is simply a vector of either “Less than 0.8” or “Greater than 0.8” to indicate if each gene’s vector length was less than or greater than our cutoff value on the correlation circle.\nabove8 \u0026lt;- rep(\u0026quot;Less than 0.8\u0026quot;, dim(xwt)[1]) above8[1:100 %in% kpInd] \u0026lt;- \u0026quot;Greater than 0.8\u0026quot; fviz_pca_biplot(pcaMouse, col.var=above8, col.ind=xwt$sampleGroup, label=\u0026quot;\u0026quot;) + ggtitle(\u0026quot;\u0026quot;)  The colors/shapes of the point indicate the sample group of each sample point. The colors of the arrows indicate whether or not the corresponding gene had length greater than or less than 0.8 on the correlation circle.\nWe can see the EPI and PE groups (the groups with the fewest samples) appear more extreme in the first principle plane when using the unweighted PCA. This was not the case in the weighted PCA (shown below) because then groups were weighted equally. Here, the PCA mostly depends on the larger groups (to best understand the most observations), so the smaller groups appear more extreme on the principal plane.\nabove8 \u0026lt;- rep(\u0026quot;Less than 0.8\u0026quot;, dim(xwt)[1]) above8[1:100 %in% kpInd] \u0026lt;- \u0026quot;Greater than 0.8\u0026quot; fviz_pca_biplot(pcaMouseWeighted, col.var=above8, col.ind=xwt$sampleGroup, label=\u0026quot;\u0026quot;) + ggtitle(\u0026quot;\u0026quot;)    ","date":1586390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586130769,"objectID":"4fcf087ef48a38e4311bcc8bb2eaa569","permalink":"/post/exercise-solution-for-chapter-7/","publishdate":"2020-04-09T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-7/","section":"post","summary":"Exercise 7.4 from Modern Statistics for Modern Biology  Let’s revisit the Hiiragi data and compare the weighted and unweighted approaches. 7.4a Make a correlation circle for the unweighted Hiiragi data xwt. Which genes have the best projections on the first principal plane (best approximation)? 7.4b Make a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.","tags":["Chapter 7","exercises"],"title":"Exercise solution for Chapter 7","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" Links for April 9 Large group: meeting link\n Group 1 (Sierra, Daniel, Burton): meeting link Group 2 (Camron, Sarah, Mikaela): meeting link Group 3 (Sere, James, Sherry, Amy): meeting link   Additional links  DESeq2 vignette edgeR vignette   Vocabulary quiz for April 9 Loading…   ","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586364432,"objectID":"3aee6c58c71f4e9cbdf1b625d5ec25ad","permalink":"/post/details-for-class-on-april-9/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/details-for-class-on-april-9/","section":"post","summary":" Links for April 9 Large group: meeting link\n Group 1 (Sierra, Daniel, Burton): meeting link Group 2 (Camron, Sarah, Mikaela): meeting link Group 3 (Sere, James, Sherry, Amy): meeting link   Additional links  DESeq2 vignette edgeR vignette   Vocabulary quiz for April 9 Loading…   ","tags":["class meeting"],"title":"Details for class on April 9","type":"post"},{"authors":["Mikaela Elder"],"categories":["vocabulary","chapter 8"],"content":"  Chapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n        RNA-Seq  sequencing of RNA molecules found in a population of cells or in a tissue    ChIP-Seq  sequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)    RIP-Seq  sequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein    DNA-Seq  sequencing of genomic DNA    HiC  high-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA    cDNA  complementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq    genetic screens  a technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification    read  the sequence obtained from a fragment    sequencing library  the collection of DNA molecules used as input for the sequencing machine    fragments  molecules being sequenced during a sequencing analysis    count table  a matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample    dynamic range  a ratio between the maximum and minimum values    heteroskedasticity  a phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different    normalization  a technique that adjusts for the nature and magnitude of systematic sampling biases    rare events  occurrences in the tail(s) of a distribution; observations that are extraordinarily high or low    dispersion  a measure of the spread of the data; a common measure is the standard deviation or variance    gamma-Poisson  negative binomial distribution with 2 parameters; 𝛼 and 𝛽    systematic biases  systematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment    metadata  a set of data that describes or gives information about other data    multifactorial design  an experimental design with more than one independent variable    balanced  in the context of study design, these are where there is an equal number of observations of all combinations of factors being tested    differential expression analysis  a type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups    intercept  a coefficient representing the base level of the measurement in the negative control    design factors  binary indicator variables    interaction effect  a parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation    design matrix  a matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions    residuals  a term in a model that reflects the experimental fluctuations (i.e. random noise)    least sum-of-squares fitting  a type of model fitting that minimizes the sum of the squared residuals    linear model  a model that is a linear function of parameters, i.e. takes the form: y_j = sum_k (x_jk * beta_k + e_j)    analysis of variance (ANOVA)  an analysis that decomposes patterns in the data into systematic variability and noise    noise  variability unaccounted for by model parameters    systematic variability  variability accounted for by model parameters    breakdown point  a measure of the robustness of an estimator; larger values indicate more robust estimators    robust  a “sturdy” estimator that is not heavily influenced by outliers    least absolute deviations  minimization of the sum of the absolute values of the residuals    least quantile of squares  a type of regression where the difference between the model quantile and empirical quantile is minimized    least trimmed sum of squares  a type of regression that minimized the sum of squared residuals, where the sum is over only a fraction of the smallest residuals    logistic regression  a type of generalized linear regression for binary data where the outcome is transformed by the logistic function and bounded between 0 and 1    maximum likelihood  a method for parameter estimation that finds the parameter value that maximizes the probablity of the observed data under the model    likelihood  a function of a model parameter which is equal to the probability of the observed data under the model    maximum-likelihood estimates  model parameters that are estimated by maximizing the probability of the observed data under the model    nuisance factor / blocking factor  a factor that has some effect on the response but is of no interest to the experiment    batch effects  hidden factors that affect the data but are not documented; e.g. running samples at the same time have a degree of similarity from being run in the same batch    pseudocounts  transformations that take the form y = log2(n + n_0) where n is the count and n_0 is a chosen positive constant    variance stabilizing transformation  a transformation that has finite values and finite slope, even for counts close to zero    regularized logarithm (rlog) transformation  a technique that transforms the original count data to a log2-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data    Cook’s distance  a measure of how much a single sample is influencing the coefficients in a model; large values indicate an outlier count    sampling without replacement  a random sample in which no observation occurs more than one time in the sample    null hypothesis  often, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.    variability  in statistics, the amount by which a set of observations deviate from their mean    outlier  a data point that does not follow the pattern of the rest of the data; often this data point will have a large residual    M-estimation  a type of regression analysis that is more robust than OLS to outliers or data that does not follow a normal distribution; it minimizes the sum of the penalization function applied to the residuals    conservative  an approach that prioritizes reducing false positives    splicing  a process in eukaryotic organisms where mRNA is cut down from the full-length gene to just the exons before being translated    exons  segments of a gene that actually get used during translation or encode for a protein    isoforms  different forms of the same gene that result from splicing events that combine different exons in an mRNA script    upregulated  a term used to describe the increased expression of a gene    gene knockdown  a way of inactivating a gene by targeting its mRNA transcript for inactivation or degradation    gene knockout  deletion of a gene from the genome    transcriptome  the total of all of the mRNA expressed from genes in an organism    polymorphism  genetic variation within a population     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Lexico: https://www.lexico.com Statistics How To: https://www.statisticshowto.com Lavrakas, 2008. Sampling without replacement. Encyclopedia of Survey Research Methods. https://dx.doi.org/10.4135/9781412963947.n516   Practice   ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586211407,"objectID":"faf087f699e0eef13cfb4f9ef67d569b","permalink":"/post/chapter-8-vocabulary-list/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/chapter-8-vocabulary-list/","section":"post","summary":"Chapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n        RNA-Seq  sequencing of RNA molecules found in a population of cells or in a tissue    ChIP-Seq  sequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)    RIP-Seq  sequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein    DNA-Seq  sequencing of genomic DNA    HiC  high-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA    cDNA  complementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq    genetic screens  a technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification    read  the sequence obtained from a fragment    sequencing library  the collection of DNA molecules used as input for the sequencing machine    fragments  molecules being sequenced during a sequencing analysis    count table  a matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample    dynamic range  a ratio between the maximum and minimum values    heteroskedasticity  a phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different    normalization  a technique that adjusts for the nature and magnitude of systematic sampling biases    rare events  occurrences in the tail(s) of a distribution; observations that are extraordinarily high or low    dispersion  a measure of the spread of the data; a common measure is the standard deviation or variance    gamma-Poisson  negative binomial distribution with 2 parameters; 𝛼 and 𝛽    systematic biases  systematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment    metadata  a set of data that describes or gives information about other data    multifactorial design  an experimental design with more than one independent variable    balanced  in the context of study design, these are where there is an equal number of observations of all combinations of factors being tested    differential expression analysis  a type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups    intercept  a coefficient representing the base level of the measurement in the negative control    design factors  binary indicator variables    interaction effect  a parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation    design matrix  a matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions    residuals  a term in a model that reflects the experimental fluctuations (i.","tags":["vocabulary","chapter 8"],"title":"Chapter 8 Vocabulary List","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":" General course information We are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post. 3:10-4:00 pm: Large group meeting for group discussion. 4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n Links for April 2 Large group: meeting link\n Group 1 (Sierra, Camron, James): meeting link Group 2 (Sherry, Amy, Mikaela): meeting link Group 3 (Daniel, Burton, Sarah, Sere): meeting link   Vocabulary quiz for April 2 Loading…   ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585665132,"objectID":"a3086d5472c57c6fd375723b87696627","permalink":"/post/details-for-class-on-april-2/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/details-for-class-on-april-2/","section":"post","summary":"General course information We are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post. 3:10-4:00 pm: Large group meeting for group discussion. 4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n Links for April 2 Large group: meeting link","tags":["class meeting"],"title":"Details for class on April 2","type":"post"},{"authors":["Zach Laubach"],"categories":["Chapter 7","vocabulary"],"content":"  Chapter 7 covers multivariate analysis, with a focus on principal component analysis and dimension reduction in general.\n        principal component analysis (PCA)  an unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data    matrix  a two dimensional arrangement of rows and columns used to store data    mass spectroscopy  a measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance    correlation coefficient  a measure of how two variables co-vary, reported as a single summary value    centering  subtracting the mean of the data so the new mean is 0    scaling / standardizing  dividing data values by the data’s standard deviation so the new standard deviation is 1    data simplification  a broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data    dimension reduction  summarizing data to reduce the number of variables for downstream analyses    principal scores  a normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component    unsupervised learning  a machine learning method used to find patterns in the data without a priori variable ranking or labeling    status  in the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.g., to consider one variable as the outcome or goal and the rest as potential predictive variables)    projection  a representation of data from a higher dimensional space to a lower dimensional space    linear  in the context of a statistical technique, a description that describes the search for relationships between variables that can be expressed as a linear combination of predictors    regression line  a linear function of the form y = mx + b which is used to project two-dimensional data onto a 1 dimensional line    linear regression  a supervised method that models the relationship between explanatory and response variables by minimizing the residual sum of squares with respect to the response variable    supervised learning  in the context of a statistical learning technique, a machine learning method that uses specified, user defined inputs to map patterns (input/output associations) in data    predictor  an independent, explanatory, or ‘x’ variable in a model    response  an outcome or ‘y’ variable in a model that is thought to be affected by a predictor    principal components  uncorrelated latent variables created by the PCA procedure, of which there are as many as there are original variables entered into the procedure    inertia  in the context of variability of points, the total variance of a point cloud based on the sum of squares of the projection of points    linear combination  mathematical expression in which terms are scaled by constants and then added together    loadings  in the context of principal components, these values quantify the weight of each original variable in a principal component    singular value decomposition (SVD)  a way to decompose a rectangular matrix by factoring it into three different matrices in a way that has some useful mathematical applications    rank  in the context of a matrix, the maximum number of linearly independent column or row vectors    norm  in the context of a vector, a positive scalar quantity reflecting its size/magnitude    singular value  a non-negative, normalizing value from a singular value decomposition quantifying the relative importance of the corresponding singular vectors    orthonormal  the characteristic of a set of vectors that are both orthogonal (uncorrelated) and normalized    principal plane  a 2-dimensional space across which the data are most spread out or variable    trace  in the context of matrices, the sum of the diagonal elements of a square matrix    supplementary information  extra information or instruction to help clarify research question, procedure or results    metadata  information, data, or descriptions that characterize other data    biplot  a type of exploratory graph that displays information on both the observations and the variables of a data matrix    biometric characteristics  physical, physiological, demographic, or behavioral features of an organism that can be measured and quantified    proliferation rate  speed at which the number of cells increase through the process of cellular division    gene expression profile  a snapshot measure of the level of activity/expression (transcription) of a collection (thousands) of genes, representing a global measure of gene function    T-cell populations  groups of differentiated white blood cells that function in immune response    operational taxonomic units (OTUs)  clusters of closely related species of bacteria based on sequence similarity    transcriptome data  the complete set of all RNA molecules measured from a biological sample generated from genome-wide sequencing methods, like RNA-seq    sequence read  an inferred sequence of base pairs, or fragments of the genome, generated from one of many genomics methods    proteomic profile  a snapshot measure of the levels of all proteins measured in a biological sample    molecule  two or more chemically bond atoms that lack a charge    m/z ratio  mass to charge ratio used in mass spectrometry to differentiation molecules    wild-type  a normal allele or phenotype that occurs under natural conditions     Source Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following source:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page   Practice   ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585625857,"objectID":"599af682b514a006f13ca6642d76980a","permalink":"/post/vocabulary-for-chapter-7/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-7/","section":"post","summary":"Chapter 7 covers multivariate analysis, with a focus on principal component analysis and dimension reduction in general.\n        principal component analysis (PCA)  an unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data    matrix  a two dimensional arrangement of rows and columns used to store data    mass spectroscopy  a measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance    correlation coefficient  a measure of how two variables co-vary, reported as a single summary value    centering  subtracting the mean of the data so the new mean is 0    scaling / standardizing  dividing data values by the data’s standard deviation so the new standard deviation is 1    data simplification  a broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data    dimension reduction  summarizing data to reduce the number of variables for downstream analyses    principal scores  a normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component    unsupervised learning  a machine learning method used to find patterns in the data without a priori variable ranking or labeling    status  in the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.","tags":["Chapter 7","vocabulary"],"title":"Vocabulary for Chapter 7","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 6","quiz"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584040643,"objectID":"4bf4bf373f3cd7c717411b606027c93c","permalink":"/post/chapter-6-vocabular-zuiz/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/chapter-6-vocabular-zuiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["Chapter 6","quiz"],"title":"Chapter 6 vocabulary quiz","type":"post"},{"authors":["Camron Pearce"],"categories":["exercises","Chapter 5"],"content":" This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe book explains the equation by first defining that the average dissimilarity of a point \\(x_i\\) to a cluster \\(C_k\\) is the average of the distances from \\(x_i\\) to all of the points in \\(C_k\\). From this, let \\(A(i)\\) be the average dissimlarity of all points in the cluster that \\(x_i\\) belongs to, and \\(B(i)\\) is the lowest average of dissimlarity of \\(x_i\\) to any other cluster of which \\(x_i\\) is NOT a member.Basically, we are subtracting the mean distance to other instances in the same cluster from the mean distance to the instances of the next closest cluster, and dividing it by which of the two values is larger. The output is a coefficient that will vary between -1 and 1, where a value closer to 1 implies that the instance is closest to the correct cluster.\nThe solution to this exercise requires the following R packages to be loaded into your environment.\nRequired Libraries library(cluster) library(dplyr) library(ggplot2) library(purrr)  Part A Question 5.1.a asks us to compute the silhouette index for the simdat data that was simulated in Section 5.7.\nThe provided code is used to simulate data coming from four separate groups. They use the pipe operator to concatenate four different, randomly generated, data sets. The ggplot2 package is used to take a look at the data as a barchart of the within-groups sum of squared distances (WSS) obtained from the k means method.\nFirst off, we need to set the seed to ensure reproducible results with a randomly generated data set.\nset.seed(1) The following chunk of code utilizes the lapply function two times to generate a datset with four distinct clusters. The lapply function comes from base R, and is most often used to apply a function over an entire list or vector.\nset.seed(1) simdat = lapply(c(0, 8), function(mx) { lapply(c(0,8), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) }) %\u0026gt;% bind_rows }) %\u0026gt;% bind_rows simdatxy = simdat[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] # data without class label The technique the authors used to generate a clustered dataset is tricky. The lapply within an lapply, paired with two bind_rows functions can be confusing. The next sections are included to demonstrate what the data looks like through various steps in this process, and help bring understanding to the reader how the code is working.\nThe inner lapply function The first lapply is generating a vector of n = 100 normally distributed random numbers, and creating two separate dataframes packed into a list that consist of the mean (my) and standard deviation, respectively. Each individual value is specifically assigned a 0 or an 8.\nsimdatmy = lapply(c(0,8), function(my) { tibble(y = rnorm(100, mean = my, sd = 2), class = paste(my, sep = \u0026quot;:\u0026quot;)) }) summary(simdatmy) ## Length Class Mode ## [1,] 2 tbl_df list ## [2,] 2 tbl_df list  The outer lapply function The second (outer) lapply uses the same idea to apply the same, random, 0 or 8 assignment to values in the mx function. The ouput is now four separate dataframes within a list that contain all of the mx data and all of the my data. Within the tibble function, they include the code class = to ensure that each row in each of the 4 the lists is assigned one of the four possible two-way combinations of 0 and 8. This is important to simulate a clustered dataset.\nsimdatmx = lapply(c(0, 8), function(mx) { lapply(c(0,8), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) })}) summary(simdatmx) ## Length Class Mode ## [1,] 2 -none- list ## [2,] 2 -none- list  Putting it together The last step is to bind the list of dataframes into one single dataframe. The final dataframe includes all of the x and y data, each with assigned classes, defined by a combination of 0 and 8.\nset.seed(1) simdat = lapply(c(0, 8), function(mx) { lapply(c(0,8), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) }) %\u0026gt;% bind_rows }) %\u0026gt;% bind_rows head(simdat) ## # A tibble: 6 x 3 ## x y class ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 -1.25 -1.24 0:0 ## 2 0.367 0.0842 0:0 ## 3 -1.67 -1.82 0:0 ## 4 3.19 0.316 0:0 ## 5 0.659 -1.31 0:0 ## 6 -1.64 3.53 0:0 unique(simdat$class) ## [1] \u0026quot;0:0\u0026quot; \u0026quot;0:8\u0026quot; \u0026quot;8:0\u0026quot; \u0026quot;8:8\u0026quot; The final simdat dataframe includes 400 random points witih an assigned class to simulate clustering. We can look at the data using a simple ggplot scatterplot, color coded by the class of each point.\nggplot(simdat, aes(x = x, y = y, color = class)) + geom_point() The next part of exploring the data is to compute the within-groups sum of squares (WSS) for the clusters that we just generated. The goal of this section is to observe how the WSS changes as the number of clusters is increased from 1 to 8 when using the k-means. Chapter 5 provides us with the following code and graph:\n#1 wss = tibble(k = 1:8, value = NA_real_) #2 wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2) #3 for (i in 2:nrow(wss)) { km = kmeans(simdatxy, centers = wss$k[i]) wss$value[i] = sum(km$withinss) } ggplot(wss, aes(x = k, y = value)) + geom_col() What is really going on here?\nThis first chunk is setting up a one-column dataframe with blank NA values. The NAs will be filled in with values as the rest of the code processes.\n#1 wss = tibble(k = 1:8, value = NA_real_) wss ## # A tibble: 8 x 2 ## k value ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA ## 2 2 NA ## 3 3 NA ## 4 4 NA ## 5 5 NA ## 6 6 NA ## 7 7 NA ## 8 8 NA The second chunk of code is calculating the value for k = 1 individually. They use the scale function to scale down the value for k = 1 because it is so much larger than the rest of the k-values. Without scaling down the k = 1 value, it would be difficult to observe any sharp decreases that might indicate a “potential sweet spot” for the number of clusters.\n#2 wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2) wss ## # A tibble: 8 x 2 ## k value ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 15781. ## 2 2 NA ## 3 3 NA ## 4 4 NA ## 5 5 NA ## 6 6 NA ## 7 7 NA ## 8 8 NA The last part of this chunk is running a k-means clustering on the remaining k 2 through 8 and then pulling out the withinss value for all of the observations, summing it, and assigning that value to each individual k-value.\n#3 for (i in 2:nrow(wss)) { km = kmeans(simdatxy, centers = wss$k[i]) wss$value[i] = sum(km$withinss) } km$withinss ## [1] 275.2533 165.2368 199.8292 257.9090 285.8292 131.9047 239.8457 339.1308 wss ## # A tibble: 8 x 2 ## k value ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 15781. ## 2 2 9055. ## 3 3 5683. ## 4 4 3088. ## 5 5 2755. ## 6 6 2441. ## 7 7 2152. ## 8 8 1895. These corresponding values are then neatly displayed in a barchart of the WSS stastistic as a function of k. The sharp decrease between k = 3 and k = 4 (at the elbow) is indicative of the number of clusters present in the dataset.\nggplot(wss, aes(x = k, y = value)) + geom_col()  Computing the silhouette index for simdat Next up is the code necessary to plot the silhouette index. The silhouette function comes from the cluster package, and the resulting graph provides an average silhouette width for k = 4 clusters.\npam4 = pam(simdatxy, 4) sil = silhouette(pam4, 4, border = NA) The pam (partitioning around medoids) function is doing the same thing as the kmeans call from the earlier chunk of code, but using the cluster package’s algorithm to calculate the k-means clustering. We use the pam function here because the we need the “pam” and “partition” output class to run the silhouette function. With this information, we can then compute the silhouette index and view the output summary and plot.\nclass(pam4) ## [1] \u0026quot;pam\u0026quot; \u0026quot;partition\u0026quot; sil = silhouette(pam4, 4, border = NA) summary(sil) ## Silhouette of 400 units in 4 clusters from pam(x = simdatxy, k = 4) : ## Cluster sizes and average silhouette widths: ## 103 100 99 98 ## 0.5279715 0.5047895 0.4815427 0.4785642 ## Individual silhouette widths: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.04754 0.41232 0.54916 0.49858 0.63554 0.71440 Before we explore the final output plot, it might be interesting to look at plots of the simulated values with their respective cluster assignments based on pam k-means clustering and the silhouette index. With some (a lot of) help from Brooke, we have the following code to view this.\nFor the most part, all of the points were assigned to the same cluster as the original, with the occational border point mis-assigned to the neighboring cluster. Interestingly, the silhouette index approaches zero when you near the border of of the cluster and is much higher near the center of the cluster. Although we would expect this, it can be helpful to view this graphically.\nsil %\u0026gt;% unclass() %\u0026gt;% as.data.frame() %\u0026gt;% tibble::rownames_to_column(var = \u0026quot;orig_order\u0026quot;) %\u0026gt;% arrange(as.numeric(orig_order)) %\u0026gt;% bind_cols(simdat) %\u0026gt;% ggplot(aes(x = x, y = y, shape = as.factor(cluster), color = sil_width)) + geom_point() + facet_wrap(~ class) And finally, a silhouette plot with the n = 400 data points. The average silhouette width is a metric that we can use to summarize everything at a level of the full clustering process. Essentially, the closer that this average is to 0.5, then the more accurate our number of clusters k is. This concept is further explored in the Part B.\nplot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;)   Part B Question 5.1.b asks us to change the number of clusters k and assess which k value produces the best silhouette index.\nIn this example, there are a couple of ways to assess which k gives the best silhouette index.One method would be trial and error and determining which k-value produces the highest silhouette index. This method works out for this example, but is impractical for much larger and complex datasets. Included below is the code for testing multiple different k-values and the resulting coefficient values.\npam2 = pam(simdatxy, 2) sil2 = silhouette(pam2, 2) plot(sil2, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) pam3 = pam(simdatxy, 3) sil3 = silhouette(pam3, 3) plot(sil3, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) pam4 = pam(simdatxy, 4) sil = silhouette(pam4, 4) plot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) pam12 = pam(simdatxy, 12) sil12 = silhouette(pam12, 12) plot(sil12, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) pam40 = pam(simdatxy, 40) sil40 = silhouette(pam40, 40) plot(sil40, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) This trial and error method indicates that the highest silhouette index (that was tested) is achieved with k = 4.\nA different (seemingly more appropriate) method is to write a piece of code that will test a range of k-values automatically. This next piece of code is adapted from Amy Fox and the group that she worked with during class. This is a much more practical method that provides a clear answer of which k gives the best silhouette index.\nk \u0026lt;- c(2:10) df_test \u0026lt;- data.frame() for (i in 2:10){ pam_run \u0026lt;- pam(simdatxy, i) sil_run \u0026lt;- silhouette(pam_run, i) row_to_add \u0026lt;- data.frame(i, width = summary(sil_run)$avg.width) df_test \u0026lt;- rbind(df_test, row_to_add) } df_test ## i width ## 1 2 0.4067400 ## 2 3 0.4000560 ## 3 4 0.4985801 ## 4 5 0.4401518 ## 5 6 0.3957347 ## 6 7 0.3717875 ## 7 8 0.3699929 ## 8 9 0.3670770 ## 9 10 0.3516570 ggplot(df_test, aes(i, width)) + geom_point() + geom_line() + xlab(\u0026quot;k\u0026quot;) + ylab(\u0026quot;Silhouette Index\u0026quot;) + ggtitle(\u0026quot;Testing different k values for Silhouette Index\u0026quot;) summary(sil_run) ## Silhouette of 400 units in 10 clusters from pam(x = simdatxy, k = i) : ## Cluster sizes and average silhouette widths: ## 63 38 40 52 33 40 35 33 ## 0.3885059 0.3273800 0.3622990 0.3703291 0.3573781 0.3257945 0.4429236 0.2807700 ## 31 35 ## 0.3944945 0.2335738 ## Individual silhouette widths: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.1778 0.2389 0.3703 0.3517 0.4946 0.6623 The result of summary(sil_run) matches the trial and error method, but in a more efficient manner.\nIn summary, k = 4 provides us with the best silhouette index value. This is because there truly are four groups in the dataset based on how we created it.\n Part C The last part of this exercise asks us to repeat by calculating the silhouette index on a uniform (unclustered) data distribution over a range of values.\nHere, a new data set is generated without clustering the randomly generated data. The 0 and 8 assignment values have been removed and replaced with a singular 1. This assigns all of the values to have the same class.\nset.seed(1) simdat1 = lapply(c(1), function(mx) { lapply(c(1), function(my) { tibble(x = rnorm(100, mean = mx, sd = 2), y = rnorm(100, mean = my, sd = 2), class = paste(mx, my, sep = \u0026quot;:\u0026quot;)) }) %\u0026gt;% bind_rows }) %\u0026gt;% bind_rows simdatxy1 = simdat1[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)] ggplot(simdatxy1, aes(x = x, y = y)) + geom_point() pam4.1 = pam(simdatxy1, 4) sil.1 = silhouette(pam4.1, 4) plot(sil.1, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;, border = \u0026quot;NA\u0026quot;) The average silhouette width is 0.33, which is much lower than the clustered value of 0.50 that we see with the first simulation. It should be pointed out that several of the points end up with negative silhouette widths. These observations were assigned to the wrong group entirely.\nResources\nModern Statistics for Modern Biology - Chapter 5\nSilhouette Clustering - Wikipedia\nBlog on Selecting Optimal Number of Clusters\n ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589403606,"objectID":"b0ad4793bf1a9ab49d5bec457a48ff6b","permalink":"/post/exercise-solution-for-5-1/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/exercise-solution-for-5-1/","section":"post","summary":"This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe book explains the equation by first defining that the average dissimilarity of a point \\(x_i\\) to a cluster \\(C_k\\) is the average of the distances from \\(x_i\\) to all of the points in \\(C_k\\).","tags":["exercises","Chapter 5"],"title":"Exercise Solution for 5.1","type":"post"},{"authors":["Daniel Dean"],"categories":["Chapter 6","vocabulary"],"content":"  Chapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n        Occam’s razor  Heuristic stating that the simplest explanation for a phenomenon is often the best    rejection region  Subset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.g. outcomes with a null-distribution probability \u0026lt; 0.05; if an outcome falls within this region (e.g., p \u0026lt; 0.05), it suggests that the null hypothesis is not true.    test statistic  Metric for measuring how well a null hypothesis fits the data    null hypothesis  Hypothesis describing some ‘uninteresting’ outcome (e.g., that no difference exists between certain groups of events/outcomes)    null distribution  Distribution of possible outcomes, given that the null hypothesis is true    alternative hypothesis  A hypothesis providing a different probability distribution than the null hypothesis; conceptually, holds that some difference from the null hypothesis exists (e.g. different means, frequencies, trends)    significance level/false positive rate/Type I error  Probability of incorrectly rejecting the null hypothesis due to outcomes falling within the rejection region by chance; in terms of the null distribution, total probability that the outcome could fall within the rejection region given that the null hypothesis is true.    power  True positive rate of a test (i.e., probability that an outcome falls in the rejection region of the null distribution, given that the alternative hypothesis is true)    false negative rate/Type II error  Probability of incorrectly failing to reject the null hypothesis when an outcome from the alternative hypothesis distribution fails to fall within the rejection region of the null hypothesis.    specificity  Complement of false positive rate (Type I error); probability of a test failing to reject the null hypothesis when it is true.    power/sensitivity/true negative rate  Complement of false negative rate (Type II error); probability of correctly rejecting null hypothesis if the alternative hypothesis is true.    assumption of independence  Treating every observation in a dataset as if it has no influence on the outcomes of other observations (or at least none unaccounted-for in the model).    p-value hacking  Fallaciously ‘fishing’ for significant results by running tests until a small p-value is obtained by chance; this can be deliberate or inadvertently caused by a scattershot approach to testing.    hypothesis switching  Fallacy of generating and/or changing hypotheses for a set of known results until a significant result is obtained by chance.    family-wide error rate (FWER)  Probability of at least one false positive occurring in repeated tests. Assuming independent tests, this is the complement of the probability of only true positives occurring, and approaches 1.0 as the number of tests approaches infinity.    p-value histogram  Visualization to get a quick sense of p-value distribution of possible test outcomes for a null hypothesis. The distribution is a mixture of cases where the null hypothesis is rejected (small p-values) or retained (larger p-values).    false discovery rate (FDR)  The proportion of false positives among all cases where the null hypothesis is rejected across an entire distribution.    local false discovery rate (fdr)  The probability of Type I Error at a given p-value when the distribution of the p-values is treated as a mixture model of the null distribution and alternative hypothesis distribution. This varies based on the p-value, rather than being a property of the entire distribution.    tail-area false disovery rate (Fdr)  Integration-based extension of the local false discovery rate to obtain a false discovery rate for the entire distribution.    independent filtering  Method to increase test power by filtering variables with criteria that are independent under the null hypothesis, but correlated under the alternative    independent hypothesis weighting  A method of improving power of multiple testing by weighting hypotheses based on their power    Bonferroni adjustment  Method used to compensate for inflated Type I (false positive) error in multiple testing by dividing the test significance level/hypothesis threshold (e.g., alpha = 0.05) by the number of tests performed    whole genome sequencing  Method used to determine and record the DNA base values and order across all of an organism’s genes    marker gene  A gene used to determine membership in a group of interest (e.g., a taxon, genotype within a population, or possessing a certain metabolic trait)    expression level  The realtive abundance of transcriptions of a gene of interest present in, e.g., a cell or environment    reagent  a compound used in creating a chemical reaction like an assay    hypothesis testing  Evaluating whether outcomes are sufficently unlikely under the null hypothesis (holding that outcomes are determined fully by chance) that it can be rejected in favor of an alternative hypothesis    workflow  A sequence of steps used in carrying out a larger operation or process    two-sided test  A statistical test which rejects the null hypothesis if an observed test statistic is either too large or too small compared to that expected under the null hypothesis    one-sided test  A statistical test which rejects the null hypothesis if an observed test statistic departs from the expected range in a single, predetermined direction (i.e. larger or smaller)    two-sample  In the context of statistical testing, a situation whether the data belong to two known groups.    unpaired  In the context of statistical tests, these are used when comparing groups with independent measurements (e.g. the observations for one group have no association with observations from the other group)    equal variances  When groups being compared have (substantially) equivalent levels of variability.    dependence  When the outcomes of two variables are associated with one another.    expected value  For a random quantity, this is the value of the mean, i.e. “average value”.     Sources Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page Bourgon, R., Gentleman, R. \u0026amp; Huber, W. Independent filtering increases detection power for high-throughput experiments. Proceedings of the National Academy of Sciences 107, 9546–9551 (2010). Ignatiadis, N., Klaus, B., Zaugg, J. et al. Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nat Methods 13, 577–580 (2016). https://doi.org/10.1038/nmeth.3885 https://www.statisticssolutions.com/bonferroni-correction/ https://bioconductor.org/packages/release/bioc/vignettes/IHW/inst/doc/introduction_to_ihw.html https://www.statisticshowto.datasciencecentral.com/familywise-error-rate/ https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/ https://www.biostars.org/p/273537/\nPractice    ","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"daf9e88176b809bdbed22af1b1b580a4","permalink":"/post/vocabulary-for-chapter-6/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-6/","section":"post","summary":"Chapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n        Occam’s razor  Heuristic stating that the simplest explanation for a phenomenon is often the best    rejection region  Subset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.","tags":["Chapter 6","vocabulary"],"title":"Vocabulary for Chapter 6","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 5","quiz"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583445540,"objectID":"4ca9d7604bf3c82206fd30c69052f4e2","permalink":"/post/chapter-5-vocabulary-quiz/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/chapter-5-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["Chapter 5","quiz"],"title":"Chapter 5 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 4"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582825339,"objectID":"47162ba198ed97a383b0f7af77226d51","permalink":"/post/chapter-4-vocabulary-quiz/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/chapter-4-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 4"],"title":"Chapter 4 vocabulary quiz","type":"post"},{"authors":["Sarah Cooper"],"categories":["exercises","Chapter 2"],"content":" Exercise 2.6 The first part of the exercise asks you to:\n Choose your own prior for the parameters of the beta distribution. You can do this by sketching it here: https://jhubiostatistics.shinyapps.io/drawyourprior.\n After sketching a plot, I chose the parameters to set up a prior: \\(\\alpha\\) = 2.47 and \\(\\beta\\) = 8.5.\n Using this prior Next, the exercise asks you:\n Once you have set up a prior, re-analyse the data from Section 2.9.2, where we saw Y = 40 successes out of n = 300 trials.\n To be able to use the loglikelihood function from the text, I first needed to redefine it here:\nloglikelihood = function(theta, n = 300, k = 40) { ## Function definition from the textbook log(choose(n, k)) + k * log(theta) + (n - k) * log(1 - theta) } Then, I created a vector of \\(\\theta\\) values between 0 and 1, spaced 0.001 units wide. The plot below shows different possible values of \\(\\theta\\) and the log likelihood for each of these values:\nthetas = seq(0, 1, by = 0.001) plot(thetas, loglikelihood(thetas), xlab = expression(theta), ylab = expression(paste(\u0026quot;log f(\u0026quot;, theta, \u0026quot; | y)\u0026quot;)),type = \u0026quot;l\u0026quot;) Next, I used rbeta to draw 1,000,000 random samples from a beta distribution with my new picks for the parameters for \\(\\alpha\\) and \\(\\beta\\):\nrtheta = rbeta(1000000, shape1 = 2.47, shape2 = 8.5) After running the above, for each of these \\(\\theta\\) values, we then generate a random sample of \\(Y\\) as observed in the histogram (with orange bars):\ny = vapply(rtheta, function(th) { rbinom(1, prob = th, size = 300) }, numeric(1)) hist(y, breaks = 50, col = \u0026quot;orange\u0026quot;, main = \u0026quot;\u0026quot;, xlab = \u0026quot;\u0026quot;) Our next step is to use this information to generate a posterior distribution of \\(\\theta\\) at a fixed \\(Y\\) value. In this example they used \\(Y=40\\).\nAfter running the above, for each of these thetas, we generated simulated values for the posterior distribution of \\(\\theta\\) at \\(Y=40\\) as observed in this histogram (with green bars).\nthetaPostEmp = rtheta[ y == 40 ] hist(thetaPostEmp, breaks = 40, col = \u0026quot;chartreuse4\u0026quot;, main = \u0026quot;\u0026quot;, probability = TRUE, xlab = expression(\u0026quot;posterior\u0026quot;~theta), ylim=c(0,40)) densPostTheory = dbeta(thetas, 42.47, 268.5) You can check how this compares to the theoretical posterior distribution for \\(\\theta\\) at \\(Y = 40\\):\nhist(thetaPostEmp, breaks = 40, col = \u0026quot;chartreuse4\u0026quot;, main = \u0026quot;\u0026quot;, probability = TRUE, xlab = expression(\u0026quot;posterior\u0026quot;~theta)) lines(thetas, densPostTheory, type=\u0026quot;l\u0026quot;, lwd = 3) We can also check the means of both distributions computed above.\nmean(thetaPostEmp) # Empirical ## [1] 0.1366924 dtheta = thetas[2]-thetas[1] sum(thetas * densPostTheory * dtheta) # Theoretical ## [1] 0.1365727 Monte Carlo integration We can use Monte Carlo integration instead and then check the agreement between our Monte Carlo sample thetaPostMC and our sample thetaPostEmp with a QQ plot:\nthetaPostMC = rbeta(n = 1e6, 42.47, 268.5) mean(thetaPostMC) ## [1] 0.1365865 qqplot(thetaPostMC, thetaPostEmp, type = \u0026quot;l\u0026quot;, asp = 1) abline(a = 0, b = 1, col = \u0026quot;blue\u0026quot;) densPost2 = dbeta(thetas, 42.47, 268.5) mcPost2 = rbeta(1e6, 42.47, 268.5) sum(thetas * densPost2 * dtheta) # mean, by numeric integration ## [1] 0.1365727 mean(mcPost2) # mean, by MC ## [1] 0.1365756 thetas[which.max(densPost2)] # MAP estimate ## [1] 0.134 quantile(mcPost2, c(0.025, 0.975)) ## 2.5% 97.5% ## 0.1008104 0.1767992   ","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"68c73d5252d4251fde0a1767c7a11c10","permalink":"/post/ex-2-6/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/ex-2-6/","section":"post","summary":"Exercise 2.6 The first part of the exercise asks you to:\n Choose your own prior for the parameters of the beta distribution. You can do this by sketching it here: https://jhubiostatistics.shinyapps.io/drawyourprior.\n After sketching a plot, I chose the parameters to set up a prior: \\(\\alpha\\) = 2.47 and \\(\\beta\\) = 8.5.\n Using this prior Next, the exercise asks you:\n Once you have set up a prior, re-analyse the data from Section 2.","tags":["exercises","Chapter 2"],"title":"Exercise solution for Chapter 2, Part 2","type":"post"},{"authors":["Burton Karger"],"categories":["Chapter 5","vocabulary"],"content":"  Chapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n        unsupervised method  A learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.    status  A variable’s classification as an outcome/predictor (e.g. independent/dependent) in an analysis.    distance  A measure of the difference between two random variables.    The Euclidean distance  A distance metric equal to the “ordinary” straight-line distance between two points.    Manhattan distance  A distance metric equal to the sum of the absolute differences between the coordinate values for two points.    Maximum distance  A distance metric equal to the largest absolute difference between the coordinate values for two points.    Weighted Euclidean distance  A distance metric, which is a generalization of the ordinary Euclidean distance, that differentially weights the differences between the coordinate values for two points.    Minkowski distance  A distance metric equal to the mth root of the sum of the absolute differences between the coordinate values each raised to the mth power.    Edit or Hamming distance  A distance metric for comparing character sequences that counts the number of differences between two character strings.    Binary distance  A distance metric for binary strings based on the proportion of features having only one bit on amongst those features that have at least one bit on.    Jaccard distance  A distance metric that quantifies how dissimilar two sets are.    co-occurrence  The fact of two or more things occurring together or simultaneously.    Jaccard index  A statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets.    Jaccard dissimilarity  1 - the Jaccard index.    Correlation-based distance  A distance metric that measures two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.    Clusters of Differentiation (CDs)  At different stages of their development, immune cells express unique combinations of proteins on their surfaces.    Rectangular gating  A method of identifying groups of cells from a flow cytometry experiment using either a line (one-dimensional) or the quadrants created by two perpendicular lines (two-dimensional)    Hyperbolic Arcsine (asinh)  A transform function often preferred over the log tranform for flow cytometry data because it can be applied to negative values.    density-based clustering (dbscan)  The dbscan method clusters points in dense regions according to the density-connectedness criterion. It looks at small neighborhood spheres to see if points are connected.    curse of dimensionality  When the dimensionality increases, the volume of the space increases so fast that the available data become sparse    density-reachability  A fundamental criterion in dbscan that quantifies whether two points are close enough together and surrounded by sufficiently many other points.    recursive partitioning methods  A class of methods for dividing heterogeneous populations into more homogeneous subgroups, often used to make decision trees, that starts by separating the whole population into a few groups and iteratively continues separating each into subgroups.    minimal jump method/single linkage method/nearest neighbor method  A clustering method that computes the distance between clusters as the smallest distance between any two points in the two clusters.    maximum jump method/complete linkage method  A clustering method that defines the distance between clusters as the largest distance between any two objects in the two clusters.    average linkage method  A clustering method that defines the distance between clusters as the average distance between a point in one cluster and another point in the other cluster.    Ward’s method  A clustering method that takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to break the clusters up into ones of smaller sizes.    Within-groups sum of squares (WSS)  A measure of the variability among data points within an identified cluster.    Calinski-Harabasz index  Quantifies the relative variability between groups (between group sum of squares) and within groups (within-groups sum of sqaures), similar to the F statistic used in analysis of variance.    Between-groups sum of squares (BSS)  A measure of the variability between clusters.    gap statistic  A metric used to perform model selection which quantifies the amount of model fit improvement when using a more complex model. These can be used to select the number of clusters for a data set.    technical / batch effects  Depedence in data observations that results from technical differences between samples, such as the type of sequencing machine or the technician that ran the sample, rather than from scientifically interesting causes.    computational complexity  A measure of the computational resources needed to run an algorithm.    noise  Unexplained variability within a data sample.    operational taxonomic unit (OTU)  A method of clustering organisms based on DNA sequence similarity of a certain taxonomic marker gene.    bias  The tendency of a statistic to overestimate or underestimate a parameter.    representativeness heuristic  A method of learning or discovery that assesses similarity of objects and organizes them based around a category prototype (e.g., like goes with like, and causes and effects should resemble each other).    rare variants  An alternative form of a gene that occur just once or twice in an individual sample but more often across all samples.    insertion-deletion (indel)  insertion or deletion of bases in the genome of an organism.    neighboring cluster  The cluster with the lowest average dissimilarity to a given cluster.    silhouette index  A metric quantifying the degree to which a given data point belongs to its designated cluster.    Microbiome  The aggregate of all microbiota that reside on or within an organim’s tissues and biofluids along with the corresponding anatomical sites in which they reside.    filtering  in the context of low-quality rRNA reads removal of low-quality reads and trimming them to a consistent length    Histopathology  The microscopic examination of tissue in order to study the manifestations of disease.    Molecular signature  Sets of genes, proteins, genetic variants or other variables that can be used as markers for a particular phenotype    Gene expression data  Gene expression measurements : from gene¬scale to genome¬scale    Single-cell RNA-Seq experiment  a measurement of the gene expression profiles of individual cells.    gene transcript  An RNA molecule of defined size over the length of a gene.    cell lineage dynamics  Visualized with tools such as scRNA-seq to track individual cells through their natural progression.    Flow cytometry  A technique for identifying and sorting cells and their components (such as DNA) by staining with a fluorescent dye and detecting the fluorescence usually by laser beam illumination    Mass cytometry  A variation of flow cytometry in which antibodies are labeled with heavy metal ion tags rather than fluorochromes. Readout is by time-of-flight mass spectrometry.    Immune cells  cells that are part of the immune system and help the body fight infections and other diseases    CD marker / antigen marker  are specific types of molecules found on the surface of cells that help differentiate one cell type from another.    CD4  A glycoprotein found on the surface of immune cells such as T helper cells, monocytes, macrophages, and dendritic cells.    helper T cells  A type of T cell that provides help to other cells in the immune response by recognizing foreign antigens and secreting substances called cytokines that activate T and B cells    Isotope  Two or more forms of the same element that contain equal numbers of protons but different numbers of neutrons in their nuclei, and hence differ in relative atomic mass but not in chemical properties;    Inner cell mass (ICM)  Pluripotent cell lineage in the blastocyst. forms within the blastocyst, prior to its implantation within the uterus.    Blastocyst  A thin-walled hollow structure in early embryonic development that contains a cluster of cells called the inner cell mass from which the embryo arises.    Pluripotent epiblast (EPI)  The functional progenitors of soma and germ cells which later differentiate into three layers: definitive endoderm, mesoderm and ectoderm    primitive endoderm (PE)  The second extraembryonic tissue to form during embryogenesis in mammals. The PE develops from pluripotent cells of the blastocyst inner cell mass    variable regions  in the context of taxon identification of bacteria bacterial 16S ribosomal RNA (rRNA) genes contain nine “hypervariable regions” (V1 – V9) that demonstrate considerable sequence diversity among different bacteria.    Chimera  An organism or tissue that contains at least two different sets of DNA, most often originating from the fusion of as many different zygotes (fertilized eggs).     Sources Consulted or Cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page https://academic.oup.com/biolreprod/article/85/5/946/2530522 https://discovery.lifemapsc.com/in-vivo-development/inner-cell-mass/inner-cell-mass https://study.com/academy/lesson/inner-cell-mass-icm-definition-function-quiz.html https://www.sciencedirect.com/ https://www.medicinenet.com/ https://www.niaid.nih.gov/ https://iti.stanford.edu/ https://sysbiowiki.soe.ucsc.edu/node/323 https://www.statisticshowto.datasciencecentral.com/between-group-variation/ https://vsoch.github.io/2013/the-gap-statistic/   Practice   ","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582321282,"objectID":"e98258856605bd61f466b24e8c688fd1","permalink":"/post/vocabulary-for-chapter-5/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-5/","section":"post","summary":"Chapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n        unsupervised method  A learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.    status  A variable’s classification as an outcome/predictor (e.","tags":["Chapter 5","vocabulary"],"title":"Vocabulary for Chapter 5","type":"post"},{"authors":["Amy Fox"],"categories":["vocabulary","Chapter 4"],"content":"  Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n        finite mixture  in the context of statistic, when the distribution of interest is a combination of a few different probability distributions    infinite mixture  in the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)    mixture model  a model for a combination of two or more different probability distributions    probability density function  a function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.    bimodal distribution  a distribution comprised of two modes    expectation-maximization (EM) algorithm  an algorithm that allows for parameter estimation in probabilistic models with incomplete data    data augmentation  adding variables that are not measured (latent variables) to the data    latent variables  variables not measured in the data    bivariate distribution  a combined distribution made of two random variables    mixture fraction  a fraction used to describe the inhomogeneity in the mixture composition    identifiability  an issue where there can be several explanations for the same observed values; occurs when there are too many degrees of freedom in parameters    marginal likelihood  the sum of the marginal distributions    expectation function  a function that calculates the average of all possible values of the group that an observation belongs to    maximization step  a step to optimize the parameters of a model    soft averaging  the process in which observations are not assigned to groups, rather they are added to multiple groups by using probabilities of memberships as weights    model averaging  the process of using several models and combining them together into a weighted model    zero-inflated data  data that contains a large number of zero counts    ChIP-Seq data  sequencing data that identifies DNA binding sites for proteins    chromosome  a DNA molecule that contains the genetic material of an organism    binding site  in the context of molecular biology, a specific region to which a macromolecule binds    deoxyribonucleotide monophosphate  a single phosphate group in a unit of DNA    gene expression measurement  the measurement of a functional gene product (i.e., protein or RNA)    microarray  a laboratory tool used to detect gene expression    promoter  in the context of genetics, a region of DNA that initiates transcription of a gene    point mass  a finite probabiliity concentrated at a point in the proability mass distribution at which there is a discontinuous segment in probability density function    sampling distribution  the probability distribution calculated from a random sample    empirical cumulative distribution function (ECDF)  a step distribution function based on empirical data measurements    density  in the context of probability distributions, the derivitive of the distribution function    bootstrap  an approximation of the true sampling distribution; created by drawing new samples from the empirical distribution of the original sample    non-parametric method  a statistical method that does not make assumptions about population distribution or sample size    nonparametric bootstrap  an approximation of the true sampling distribution not based off of a specific assumption or a particular model    Laplace distribution  a distribution that shows differences between two independent variates with identical exponential distributions    gamma distribution  a distribution that is positively valued and continuous with two parameters: shape and scale    negative binomial distribution/ gamma-Poisson distubtion  the probability distribution of the number of failures before the kth success in a sequence of Bernoulli trials    dispersion  the amount by which a set of observations deviate from their mean    variance-stabilizing transformations  transformations designed to give approximate independence between mean and variance    heteroscedasticity  the variance of the data is different in different regions of the data    delta method  a calculus procedure that uses random variables to approximate the expected value and variance of a function     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Zero-Inflated Poisson Regression. Institute for Digital Research and Education Statistical Consulting. https://stats.idre.ucla.edu/r/dae/zip/. Berrar, 2019. Introduction to Non-parametric Bootstrap. Research Gate. https://www.researchgate.net/ Do and Batzoglou, 2008. What is the expectaion maximization algorithm?. Nature Biotechnology. Wikipedia: The Free Encylcopedia. https://en.wikipedia.org/wiki/Main_Page Google Oxford American Dictionary. https://www.google.com d’Auzay, et al., 2019. Statistics of progress variable and mixture fraction gradients in an open turbulent jet spray flame. Fuel. Brownlee, 2019. A Gentle Introduction to Expectation-Maximization (EM Algorithm). Machine Learning Mastery. https://www.machinelearningmastery.com Non-parametric Methods. R tutorial. https://www.r-tutor.com Precise analysis of DNA–protein binding sequences. Illumina. https://www.illumina.com Microarray. Nature. https://www.nature.com   Practice   ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"379ab172062419319ec4efa5c87e334a","permalink":"/post/vocabulary-for-chapter-4/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-4/","section":"post","summary":"Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n        finite mixture  in the context of statistic, when the distribution of interest is a combination of a few different probability distributions    infinite mixture  in the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)    mixture model  a model for a combination of two or more different probability distributions    probability density function  a function giving the relative likelihood that a continuous random variable is equal to a given value.","tags":["vocabulary","Chapter 4"],"title":"Vocabulary for Chapter 4","type":"post"},{"authors":[],"categories":["quiz","Chapter 2"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582161527,"objectID":"ef44a84c1820dfdedb0a09c0a4ae4fb1","permalink":"/post/chapter-2-part-2-vocabulary-quiz/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/chapter-2-part-2-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 2"],"title":"Chapter 2, part 2, vocabulary quiz","type":"post"},{"authors":["Sere Williams"],"categories":["exercises","Chapter 2"],"content":" As always, load libraries first.\nlibrary(ggplot2) library(tidyverse) library(dplyr) Exercise 2.3 from Modern Statistics for Modern Biologists A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.) The code is redundant. But is the redundancy even among codons that code for the same amino acid? In other words, if alanine is coded by 4 different codons, do these codons code for alanine equally (each 25%), or do some codons appear more often than others? Here we use the tuberculosis genome to explore codon bias.\n a) Explore the data, mtb Use table to tabulate the AmAcid and Codon variables.\nEach amino acid is encoded by 1–6 tri-nucleotide combinations.\nmtb = read.table(\u0026quot;example_datasets/M_tuberculosis.txt\u0026quot;, header = TRUE) codon_no \u0026lt;- rowSums(table(mtb)) codon_no ## Ala Arg Asn Asp Cys End Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Thr Trp Tyr ## 4 6 2 2 2 3 2 2 4 2 3 6 2 1 2 4 6 4 1 2 ## Val ## 4 The PerThousands of each codon can be visualized, where each plot represents an amino acid and each bar represents a different codon that codes for that amino acid. But what does the PerThousands variable mean?\nggplot(mtb, aes(x=Codon, y=PerThous)) + geom_col()+ facet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1))  b) The PerThous variable How was the PerThous variable created?\nThe sum of all of the numbers of codons gives you the total number of codons in the M. tuberculosis genome: all_codons. Remember that this is not the size of the M. tuberculosis genome, but the number of codons in all M. tuberculosis genes. To get the size of the genome, multiply each codon by 3 (for each nucleotide) and add all non-coding nucleotides (which we do not know from this data set).\nall_codons = sum(mtb$Number) all_codons ## [1] 1344223 The PerThousands variable is derived by dividing the number of occurrences of the codon of interest by the total number of codons. Because this number is small and hard to interpret, multiplying it by 1000 gives a value that is easy to make sense of. Here is an example for proline. The four values returned align to the four codons that each code for proline.\npro = mtb[mtb$AmAcid == \u0026quot;Pro\u0026quot;, \u0026quot;Number\u0026quot;] pro / all_codons * 1000 ## [1] 31.560240 6.121752 3.405685 17.032144  c) Codon bias Write an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.\nFirst, let’s look at the expected frequencies of each codon.\ncodon_expected \u0026lt;- data.frame(codon_no) %\u0026gt;% rownames_to_column(var = \u0026quot;AmAcid\u0026quot;) %\u0026gt;% mutate(prob_codon = 1/codon_no) codon_expected ## AmAcid codon_no prob_codon ## 1 Ala 4 0.2500000 ## 2 Arg 6 0.1666667 ## 3 Asn 2 0.5000000 ## 4 Asp 2 0.5000000 ## 5 Cys 2 0.5000000 ## 6 End 3 0.3333333 ## 7 Gln 2 0.5000000 ## 8 Glu 2 0.5000000 ## 9 Gly 4 0.2500000 ## 10 His 2 0.5000000 ## 11 Ile 3 0.3333333 ## 12 Leu 6 0.1666667 ## 13 Lys 2 0.5000000 ## 14 Met 1 1.0000000 ## 15 Phe 2 0.5000000 ## 16 Pro 4 0.2500000 ## 17 Ser 6 0.1666667 ## 18 Thr 4 0.2500000 ## 19 Trp 1 1.0000000 ## 20 Tyr 2 0.5000000 ## 21 Val 4 0.2500000 Next, calculate the observed frequencies for each codon seen in the data set and use the chi-squared test statistic to determine if the difference between expected and observed codon frequencies is even or if some codon sequences are used more than others.\nTo start, you can group the data by amino acid and then determine a few things about the amino acid or the possible codons for it, including the total observations across all codons for the amino acid (total), the number of codons for that amino acid (n_codons), and the expected count for each codon for that amino acid (the total number of observations for that amino acid divided by the number of codons, giving an expected number that’s the same for all codons of an amino acid; expected).\ncodon_compared \u0026lt;- mtb %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(total = sum(Number), n_codons = n(), expected = total / n_codons) codon_compared ## # A tibble: 64 x 7 ## # Groups: AmAcid [21] ## AmAcid Codon Number PerThous total n_codons expected ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Gly GGG 25874 19.2 132810 4 33202. ## 2 Gly GGA 13306 9.9 132810 4 33202. ## 3 Gly GGT 25320 18.8 132810 4 33202. ## 4 Gly GGC 68310 50.8 132810 4 33202. ## 5 Glu GAG 41103 30.6 62870 2 31435 ## 6 Glu GAA 21767 16.2 62870 2 31435 ## 7 Asp GAT 21165 15.8 77852 2 38926 ## 8 Asp GAC 56687 42.2 77852 2 38926 ## 9 Val GTG 53942 40.1 114991 4 28748. ## 10 Val GTA 6372 4.74 114991 4 28748. ## # … with 54 more rows The mutate function is used after group_by to do all this within each amino acid group of codons, but without collapsing to one row per amino acid, as a summarize call would.\nTo convince yourself that this has worked out correctly, you can repeat the plot we made before and see that the bars for the expected values are always equal across all codons for an amino acid:\nggplot(codon_compared, aes(x=Codon, y=expected)) + geom_col()+ facet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Finally, we can calculate the chi-squared (\\(\\chi^2\\)) statistic and compare it to the chi-squared distribution to get the p-value when testing against the null hypothesis that the amino acid observations are uniformly distributed across codons. The \\(\\chi^2\\) is calculated as:\n\\[ \\chi^2 = \\sum_i{\\frac{(O_i-E_i)^2}{E_i}} \\]\nwhere:\n \\(O_i\\) is the observed value of data point \\(i\\) (Number in our data); and \\(E_i\\) is the expected value of data point \\(i\\) (expected in our data)  In our data, we can calculate the contribution to the total \\(\\chi^2\\) statistic from each data point (in this case, each codon within an amino acid) using mutate, and then add these values up using group_by to group by amino acid followed by summarize to sum up across all the data points for an amino acid. The other information we need to get is the number of codons for the amino acid, because we’ll need this to determine the degrees of freedom for the chi-squared distribution. Next, we used mutate with pchisq to determine the p-values within each amino acid group for the test against the null that the codons are uniformly distributed for that amino acid (i.e., that there isn’t codon bias). These p-values turn out to be super small, so we’re using a technique to get the log-transform versions of them instead, which we explain a bit more later. Finally, we used arrange to list the amino acids by evidence against uniform distribution of the codons, from most evidence against (smallest p-value so most negative log(p-value)) to least evidence against (although still plenty of evidence against) and added an index with the ranking for each codon by adding a column with the sequence of numbers from 1 to the number of rows in the data (n()).\ncodon_compared %\u0026gt;% filter(n_codons \u0026gt; 1) %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(chi_squared = ((Number - expected)^2/expected)) %\u0026gt;% summarise(chi_squared = sum(chi_squared), n = n()) %\u0026gt;% mutate(p_value = pchisq(chi_squared, df = n-1, log = TRUE, lower.tail = FALSE)) %\u0026gt;% arrange(p_value) %\u0026gt;% mutate(rank = 1:n()) ## # A tibble: 19 x 5 ## AmAcid chi_squared n p_value rank ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Leu 135432. 6 -67700. 1 ## 2 Ala 75620. 4 -37805. 2 ## 3 Arg 72183. 6 -36076. 3 ## 4 Thr 58767. 4 -29378. 4 ## 5 Val 58737. 4 -29363. 5 ## 6 Ile 56070. 3 -28035. 6 ## 7 Gly 52534. 4 -26262. 7 ## 8 Pro 45400. 4 -22695. 8 ## 9 Ser 36742. 6 -18357. 9 ## 10 Asp 16208. 2 -8109. 10 ## 11 Phe 13444. 2 -6727. 11 ## 12 Asn 11404. 2 -5707. 12 ## 13 Gln 9376. 2 -4693. 13 ## 14 Lys 6382. 2 -3195. 14 ## 15 Glu 5947. 2 -2978. 15 ## 16 His 5346. 2 -2678. 16 ## 17 Tyr 4738. 2 -2373. 17 ## 18 Cys 2958. 2 -1483. 18 ## 19 End 928. 3 -464. 19 As you may notice, these log transforms of the p-values (which we got rather than untransformed p-values in the pchisq call because we used the option log = TRUE) are large in magnitude and negative (so very tiny once you take the exponent if you re-transformed them to p-values) values. If you tried to calculate the untransformed p-values (and we did!), this number is so small (0.00000000e+00) that it is too small for R—it shows up as exactly zero in R, even though it actually is a very tiny, but still non-zero, number. To get around this issue, we told pchisq to work on these p-values as log transforms, and then we left the p-value as that log-transformed value. A group of numbers that are log transformed will be in the same order as their untransformed versions, so we don’t need to convert back to figure out which amino acid had that smallest p-value. We can just sort the amino acids from most negative to less negative using these log-transformed versions of the p-values. We now have the amino acids ranked from most biased codons (1) to least (19).\n ","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582151148,"objectID":"40f5d68248e48973d3f72a984ce57de5","permalink":"/post/exercise-solution-for-chapter-2/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-2/","section":"post","summary":"As always, load libraries first.\nlibrary(ggplot2) library(tidyverse) library(dplyr) Exercise 2.3 from Modern Statistics for Modern Biologists A sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.","tags":["exercises","Chapter 2"],"title":"Exercise solution for Chapter 2, Part 1","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 2"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581612248,"objectID":"7c8103a2670fda3b5cb4a7557b69a29c","permalink":"/post/chapter-2-vocabulary-quiz/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/chapter-2-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 2"],"title":"Chapter 2 Part 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","blogdown","exercises","github"],"content":" Each of you will be responsible once or twice over the semester to create a blog post that provides a clean, clearly-presented solution to the in-class exercise for the week. This blog post provides the technical instructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting. Since it will need to be reviewed by the faculty before it can be officially posted, please plan to submit it by the Tuesday after the class for your exercise. Student assignments for the exercises are given in the Schedule section of our course website.\nOverview of creating a post You will be submitting your exercise solution as a blog post. Creating one for our website will follow all the same steps as creating a blog post for a vocabulary list, just with different content. Please read the post on creating a vocabulary list and follow the steps there to:\n Update your fork of the website Make a new blog post Use RMarkdown syntax to write the blog post Submit the blog post   Content for the blog post The blog post should provide a walk-through of the solution to that week’s in-course exercise. We have posted an example for the exercise for Chapter 1 to give you an idea of what you should aim to write.\nGenerally, this exercise will be a resource for everyone in the class, to make sure they’ve understood the exercise, as well as to see how someone else tackled the problem. Your solution should cover all parts of the exercise (for example, if there’s a part A and B, you should cover both). You can start by writing it as you would if you were assigned the exercise as a homework problem, but then you should do a second step of revision to provide some context and dig a bit deeper into how you tackled the question. Since we are only requiring you to write up exercise answers once or twice over the semester (rather than submitting homework for exercises every week), we expect this product to be more in-depth and polished than a typical homework solution.\nFirst, make sure that you have provided text explaining what the exercise asks for, in case the reader hasn’t recently read the exercise prompt. Second, please add a few details either about how you tackled the problem through code or how the statistical principles covered in the exercise could apply to other problems you’ve come across in your research or coursework.\nTo help in preparing your post, plan to spend the exercise time in class during the week of your exercise visiting the different groups of students working on the exercise. You can talk to them about how they’re approaching the problem, how they interpret it, etc., to help you develop your own answer.\n Tips  Be sure to refresh yourself on all the Markdown formatting tags you can use to improve the appearance of your post. Be sure to include things like section headings and italics or bold as appropriate. RStudio’s website has some nice cheatsheets on RMarkdown that can help. Make sure you include R code if appropriate. If you put parentheses around an assignment expression in R, it will print out the assigned object and make the assignment in the same call—you might find this useful in writing concise code while still showing what’s in the objects you create. Use the $ and $$ tags in RMarkdown to include mathematical equations in your blog post when appropriate. If you need to read in a dataset for R code in your blog post, save it in the website directory’s “content/post/example_datasets” subdirectory. If your data comes from an online source or from an R library, you won’t need to do this, only if you need a “local” copy of the datafile to run your RMarkdown code. You are welcome to draw from (and cite) other statistics textbooks or dictionaries if you’d like to in explaining the problem and your approach to it. For the code, look at vignettes and helpfiles, especially for packages you are not familiar with. For a lot of Bioconductor packages, object-oriented programming is used pretty heavily. This means that associated data in R packages will often be stored in a format that you haven’t used yet. Look up more information on data classes used in your exercise if you aren’t familiar with them. You can use the class function to determine the class of an object as well as the name of the package that defines that class. The str function is often helpful for exploring a data object class, as well. Many of the Bioconductor object classes will have special accessor methods, which are functions that allow you to extract certain elements from the object—check the helpfile for the object class, as these methods are often listed there with examples. Googling can also be very helpful for learning more about functions, packages, and datasets in R, especially if you don’t yet know what package the item is from. Most Bioconductor packages have very nice vignettes available online and from your R session once you have installed the package. These are a great place to start to find out more about how to use the functions and object classes that come with the package.   ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477683,"objectID":"459571b5f2972f118129cd8d9a5201a1","permalink":"/post/how-to-create-an-exercise-solution-blog-post/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/how-to-create-an-exercise-solution-blog-post/","section":"post","summary":"Each of you will be responsible once or twice over the semester to create a blog post that provides a clean, clearly-presented solution to the in-class exercise for the week. This blog post provides the technical instructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting. Since it will need to be reviewed by the faculty before it can be officially posted, please plan to submit it by the Tuesday after the class for your exercise.","tags":["instructions","blogdown","exercises","github"],"title":"How to create an exercise solution blog post","type":"post"},{"authors":["Sierra Pugh"],"categories":["Chapter 2","vocabulary"],"content":"  These sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n        Markov chain  a sequence where given the current state, the next state is conditionally independent of all previous states    Bayesian paradigm  approaching statistics from the perspective that probability can be viewed as a degree of belief in an event    Beta distribution  a probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics    Exponential distribution  a probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process    Prior  a probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data    Posterior  a probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data    Haplotype  a collection of DNA sequence variants (e.g., alleles) that are spatially close on a chromosome, are usually inherited together, and thus are genetically linked    Marginal distribution  the distribution of a sub-collection of variables after integrating out the remaining variables in the collection.    Monte Carlo integration  a technique for numerical integration where the value of an integral is estimated by simulating data    Quantile-quantile plot (QQ-plot)  a plot comparing the quantiles from one distribution (often a theoretical distribution) to the quantiles of another distribution (often from a sample)    Maximum a posteriori (MAP) estimate  the mode of the posterior distribution associated with the quantity of interest    Escherichia coli  facultative anaerobic, rod-shaped, coliform bacterium commonly found in the lower intestine of warm-blooded organisms    Epigenetics  the study of heritable phenotype changes that do not involve alterations in the DNA sequence    Log-likelihood ratio  the log of the likelihood under one set of assumptions divided by the likelihood under another set of assumptions    Bimodality  when a distribution has two modes    Mixture  in the context of statistics, when the distribution of interest is a combination of two or more different probability distributions    Codon  A three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis)    Codon bias  the differences in how often each spelling of an amino acid occurs in coding DNA    Genetic code  the set of instructions in a gene that tell the cell how to make a specific protein     Sources consulted or cited Some of the definitons above are based in part or whole on listed definitions in the following sources:\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Wikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page NIH Genetics Home Reference. https://ghr.nlm.nih.gov/ NCBI Genetics Review. https://www.ncbi.nlm.nih.gov   Practice   ","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581691815,"objectID":"6cf732c0340c1c0a7b23cd9a5e5104b6","permalink":"/post/vocabulary-for-chapter-2-8-2-12/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-2-8-2-12/","section":"post","summary":"These sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n        Markov chain  a sequence where given the current state, the next state is conditionally independent of all previous states    Bayesian paradigm  approaching statistics from the perspective that probability can be viewed as a degree of belief in an event    Beta distribution  a probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics    Exponential distribution  a probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process    Prior  a probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data    Posterior  a probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data    Haplotype  a collection of DNA sequence variants (e.","tags":["Chapter 2","vocabulary"],"title":"Vocabulary for Chapter 2, Part 2","type":"post"},{"authors":["Brooke Anderson"],"categories":["exercises","Chapter 1"],"content":" This exercise asks us to explore the frequency of each of the four nucleotides (A, C, G, and T) in the genome of C. elegans, a type of worm used frequently in scientific research.\nThis solution requires that several R extension packages be loaded in your R session. If you do not have these packages installed to your computer yet, you should follow instructions we’ve posted separately describing the required set-up for this exercise. Once you have installed these packages on your computer, you can load them into your current R session using the library function:\nlibrary(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;) library(\u0026quot;Biostrings\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;knitr\u0026quot;) Part A Part A of the question asks us to explore the nucleotide frequency of the C. elegans genome. This genome is available in the Celegans data that comes with the BSgenome.Clegans.UCSC.ce2 package and is stored within a BSgenome class, which is a special object class provided by the Biostrings package.\nThere is a dedicated function called letterFrequency in the Biostrings package that can be used to count the frequency of letters in a string (like a genome) in an R object like this. In a call to this function, you must also include the possible letters in your “alphabet”—that is, the possible letters that each position in your string could take.\n(nuc_freq \u0026lt;- letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;))) ## A C G T ## 4335 1225 2055 6179 To explore and plot this data, I put this summary data into a tibble, so I could more easily use tidyverse tools with the data.\nnuc_freq_df \u0026lt;- tibble(nucleotide = names(nuc_freq), n = nuc_freq) nuc_freq_df ## # A tibble: 4 x 2 ## nucleotide n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 A 4335 ## 2 C 1225 ## 3 G 2055 ## 4 T 6179 In this format, you can use tidyverse tools to explore the data a bit more. For example, you can determine the total number of nucleotides in the genome and, with that calculate the proportion of each nucleotide across the genome. Along with the kable function from the knitr package, I created a formatted table with this information:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n)) %\u0026gt;% kable(digits = 2, caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;, col.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency\u0026quot;, \u0026quot;Proportion\u0026quot;))  Table 1: Nucleotide frequencies and proportions in C. elegans  Nucleotide Frequency Proportion    A 4335 0.31  C 1225 0.09  G 2055 0.15  T 6179 0.45    For some presentations, it might be clearer to present this information in a slightly different table format, using pivot_longer and then pivot_wider to reformat the table for presentation:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n), n = prettyNum(n, big.mark = \u0026quot;,\u0026quot;), prop = prettyNum(prop, digits = 2)) %\u0026gt;% pivot_longer(cols = c(\u0026quot;n\u0026quot;, \u0026quot;prop\u0026quot;)) %\u0026gt;% pivot_wider(names_from = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate(name = case_when( name == \u0026quot;n\u0026quot; ~ \u0026quot;Frequency of nucleotide\u0026quot;, name == \u0026quot;prop\u0026quot; ~ \u0026quot;Proportion of all nucleotides\u0026quot; )) %\u0026gt;% rename(` ` = name) %\u0026gt;% kable(align = c(\u0026quot;rcccc\u0026quot;), caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;)  Table 2: Nucleotide frequencies and proportions in C. elegans   A C G T    Frequency of nucleotide 4,335 1,225 2,055 6,179  Proportion of all nucleotides 0.31 0.089 0.15 0.45    Here is a plot of the frequency of each of the four nucleotides for the C. elegans nucleotide:\nggplot(nuc_freq_df, aes(x = nucleotide, y = n)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = expression(paste(italic(\u0026quot;C. elegans\u0026quot;), \u0026quot; neucleotide frequency\u0026quot;)), caption = expression(paste(\u0026quot;Based on data from the \u0026quot;, italic(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;), \u0026quot; package.\u0026quot;))) This graph uses a few elements to improve its appearance that you might want to explore if you’re not already familiar with them:\n The labs function is used to add both a title and a caption to the plot. The paste, expression, and italic functions are used together to put “C. elegans” and an R package name in italics in some of the labels on the plot. The scales package is used inside a scale layer for the ggplot2 code to make the y-axis labels a bit nicer. theme calls are used to apply a simpler overall theme than the default and to remove the x- and y-axis titles (with element_blank). The color and fill of the bars are customized in the geom layer (geom_col).  From this plot, it certainly looks like the nucleotides are not uniformly distributed in the C. elegans genome. This question will be investigated more in the next part of the exercise.\n Part B The second part of the exercise asks us to test whether the observed nucleotide data for C. elegans is consistent with the uniform model that all nucleotide frequencies are the same.\nFirst, we can simulate several datasets under this null model and see how a plot of nucleotide frequencies compares to the plot that we obtained with the observed C. elegans data. To make these plots, I first simulated 20 samples under the null model that the distribution is uniform across the four nucleotides, using the rmultinom function with the size argument set to the number of nucleotides in the original C. elegans genome data and the prob argument set to have an equal probability of each nucleotide at each spot on the genome:\n(sim_nuc_freq \u0026lt;- rmultinom(n = 20, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 3540 3449 3544 3437 3447 3402 3518 3461 3412 3477 3351 3354 3524 3534 ## [2,] 3435 3447 3388 3549 3466 3443 3480 3473 3478 3432 3480 3518 3478 3490 ## [3,] 3328 3496 3447 3378 3488 3435 3459 3418 3461 3452 3523 3452 3340 3449 ## [4,] 3491 3402 3415 3430 3393 3514 3337 3442 3443 3433 3440 3470 3452 3321 ## [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 3438 3530 3460 3523 3323 3540 ## [2,] 3378 3438 3487 3339 3510 3340 ## [3,] 3463 3395 3374 3505 3451 3500 ## [4,] 3515 3431 3473 3427 3510 3414 Next, I moved this into a tibble so I could more easily rearrange and plot the data using facetting in ggplot2:\nsim_nuc_freq_df \u0026lt;- as_tibble(sim_nuc_freq) %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide) sim_nuc_freq_df %\u0026gt;% slice(1:10) ## # A tibble: 10 x 3 ## nucleotide sample value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 A 1 3540 ## 2 C 1 3435 ## 3 G 1 3328 ## 4 T 1 3491 ## 5 A 2 3449 ## 6 C 2 3447 ## 7 G 2 3496 ## 8 T 2 3402 ## 9 A 3 3544 ## 10 C 3 3388 ggplot(sim_nuc_freq_df, aes(x = nucleotide, y = value)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = \u0026quot;Simulated neucleotide frequencies under a uniform model\u0026quot;) + facet_wrap(~ sample) + expand_limits(y = max(nuc_freq_df$n)) The y-axis limits were expanded here to cover the same range as that shown for the observed C. elegans nucleotide frequencies, to help make it easier to compare these plots with the plot of our observed data. These plots of data simulated under the null model do show some variation in frequencies among the nucleotides, but it’s certainly much less than in the observed data for C. elegans.\nNext, I repeated this simulation process, but I increased the number of simulations to 1,000:\nsim_nuc_freq_df \u0026lt;- rmultinom(n = 1000, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4)) %\u0026gt;% as_tibble() %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide) sim_nuc_freq_df %\u0026gt;% slice(1:10) ## # A tibble: 10 x 3 ## nucleotide sample value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 A 1 3444 ## 2 C 1 3468 ## 3 G 1 3370 ## 4 T 1 3512 ## 5 A 2 3492 ## 6 C 2 3380 ## 7 G 2 3460 ## 8 T 2 3462 ## 9 A 3 3507 ## 10 C 3 3443 Using this dataframe of simulations, we can measure the mean, minimum, and maximum frequencies of each nucleotide across all 1,000 simulations:\n(sim_summary \u0026lt;- sim_nuc_freq_df %\u0026gt;% group_by(nucleotide) %\u0026gt;% summarize(mean_freq = mean(value), min_freq = min(value), max_freq = max(value))) ## # A tibble: 4 x 4 ## nucleotide mean_freq min_freq max_freq ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 A 3449. 3281 3637 ## 2 C 3450. 3307 3589 ## 3 G 3447. 3313 3638 ## 4 T 3449. 3289 3612 To help compare this with the observed data, we can create a table with information from both the original data and the simulations under the null model:\nnuc_freq_df %\u0026gt;% left_join(sim_summary, by = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate_at(c(\u0026quot;mean_freq\u0026quot;, \u0026quot;min_freq\u0026quot;, \u0026quot;max_freq\u0026quot;, \u0026quot;n\u0026quot;), prettyNum, big.mark = \u0026quot;,\u0026quot;, digits = 0) %\u0026gt;% mutate(simulations = paste0(mean_freq, \u0026quot; (\u0026quot;, min_freq, \u0026quot;, \u0026quot;, max_freq, \u0026quot;)\u0026quot;)) %\u0026gt;% select(nucleotide, n, simulations) %\u0026gt;% kable(col.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency in C. elegans genome\u0026quot;, \u0026quot;Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations\u0026quot;), align = \u0026quot;c\u0026quot;)   Nucleotide Frequency in C. elegans genome Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations    A 4,335 3,449 (3,281, 3,637)  C 1,225 3,450 (3,307, 3,589)  G 2,055 3,447 (3,313, 3,638)  T 6,179 3,449 (3,289, 3,612)    This helps clarify how unusual the observed data would be under the null model—the counts of all four nucleotides in the C. elegans genome are completely outside the range of frequencies in the simulated data.\nAnother way to look at this is with histograms of the distribution of frequencies of each nucleotide under the null model compared to the observed frequencies in the C. elegans nucleotide:\nggplot(sim_nuc_freq_df, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~ nucleotide) + theme_classic() + scale_x_continuous(name = \u0026quot;Frequency of nucleotide in the simulation under the null model\u0026quot;, labels = scales::comma) + scale_y_continuous(name = \u0026quot;# of simulations (out of 1,000)\u0026quot;) + geom_vline(data = nuc_freq_df, aes(xintercept = n), color = \u0026quot;red\u0026quot;) + labs(title = expression(paste(\u0026quot;Nucleotide frequency in \u0026quot;, italic(\u0026quot;C. elegans\u0026quot;), \u0026quot; compared null model simulations\u0026quot;)), caption = \u0026quot;Red line shows the frequency observed for the nucleotide in C. elegans\u0026quot;) Finally, to help in answering this question, it would be interesting to look at a single measure for each simulation (and for the observed data) rather than comparing each nucleotide one at a time. Chapter 1 gives the equation for a statistic to measure variability in multinomial data by calculating the sum of squares for the differences between the observed and expected count of nucleotides for each of the four nucleotides in a sample (p. 12).\nI calculated this statistic for the observed data and then for each of the 1,000 simulations.\n(obs_stat \u0026lt;- nuc_freq_df %\u0026gt;% mutate(expected = mean(n), stat_input = (n - expected) ^ 2 / expected) %\u0026gt;% summarize(variability_stat = sum(stat_input))) ## # A tibble: 1 x 1 ## variability_stat ## \u0026lt;dbl\u0026gt; ## 1 4387. sim_stat \u0026lt;- sim_nuc_freq_df %\u0026gt;% mutate(expected = mean(value), stat_input = (value - expected) ^ 2 / expected) %\u0026gt;% group_by(sample) %\u0026gt;% summarize(variability_stat = sum(stat_input)) sim_stat %\u0026gt;% slice(1:5) ## # A tibble: 5 x 2 ## sample variability_stat ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 3.07 ## 2 2 2.00 ## 3 3 1.83 ## 4 4 8.64 ## 5 5 1.56 Here is a plot of the distribution of this statistic across the 1,000 simulations:\nggplot(sim_stat, aes(x = variability_stat)) + geom_rect(data = sim_stat, aes(xmin = quantile(variability_stat, prob = 0.025), xmax = quantile(variability_stat, prob = 0.975), ymin = 0, ymax = Inf), fill = \u0026quot;beige\u0026quot;, alpha = 0.5) + geom_histogram(bins = 30, fill = \u0026quot;white\u0026quot;, color = \u0026quot;tan\u0026quot;, alpha = 0.5) + theme_classic() + labs(title = \u0026quot;Variability from expected values\u0026quot;, subtitle = \u0026quot;Values from simulations under the null\u0026quot;, x = \u0026quot;Value of variability statistic\u0026quot;, y = \u0026quot;Number of simulations with given value\u0026quot;, caption = \u0026quot;The shaded yellow area shows the region of the central 95% of\\nstatistic values for the 1,000 simulations under the null model.\u0026quot;) The value of this statistic for the observed nucleotide frequencies for C. elegans is 4387, which is much larger (indicating greater variability from expected values under the null model) than the value observed under most of the simulations. It is, in fact, far outside the central 95% range of values observed in simulations.\n ","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477727,"objectID":"22f4b300c757465ac6d281137e95056c","permalink":"/post/exercise-solution-for-chapter-1/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-1/","section":"post","summary":"This exercise asks us to explore the frequency of each of the four nucleotides (A, C, G, and T) in the genome of C. elegans, a type of worm used frequently in scientific research.\nThis solution requires that several R extension packages be loaded in your R session. If you do not have these packages installed to your computer yet, you should follow instructions we’ve posted separately describing the required set-up for this exercise.","tags":["exercises","Chapter 1"],"title":"Exercise solution for Chapter 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 2","vocabulary"],"content":"  The first portion of Chapter 2 (2.1-2.7) is focused on statistical modeling of data. It introduces a number of distributions commonly used in statistics, as well as model fitting estimation procedures (e.g. maximum likelihood estimation).\nThe vocabulary words for Chapter 2, part 1, are:         statistical inference / up / statistical approach  An upward-reasoning approach that start with data and works towards defining a model that might possibly explain the data.    deduction  Starting from a mathematical/statistical model with known parameters and computing the probability of observing an event.    null model  The model associated with the null hypothesis, which formulates an “uninteresting” baseline.    goodness-of-fit  Evaluation of whether a theorectical distribution/model is appropriate for a data set.    rootogram  Diagram to assess model goodness-of-fit for a data set. Bar chart where the bars “hang” from their theorectical values and will approximately line up with horizontal axis if the model is a good fit to the data.    maximum likelihood estimator (MLE)  A rule, or mathematical formula, that outputs an estimate of a parameter for a model, where that estimate maximizes the probability of the observed data.    conservative (approach)  An analysis approach that errs on the side of caution to avoid concluding an alternative hypothesis (e.g. detecting a signal) when it is not true.    vectorization  In regard to function evaluation, if a vector is supplied to a function that expects a scalar, R will apply the function to each element of the vector.    likelihood function  The probability of the data under a model expressed as a function of the model parameter(s).    estimation  Process of using data to perform inference on population parameters.    statistical testing  Formal decision process to determine if a null model is appropriate for the observed data.    regression  Relating how an outcome measure depends on one or more covariates.    residual  Deviation between the observed data and the expected value of the data point according to a model.    generalized linear model  A class of models for non-continuous or non-negative data that allows regression of an outcome on observed covariates. An extension of linear regression.    chi-squared distribution  A distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).    quantile-quantile (QQ) plot  Used to compare two distributions (or samples). Deviations in the plot from the y=x line suggest differences between the two distributions.    quantile  Value corresponding to a percentile of a distribution.    empirical cumulative distribution function (ECDF)  Function with input value x gives as output the probability that a random variable from the distribution is less than or equal to x. Function is defined using a sample and assigning probability 1/n to each data point.    chi-squared statistic  A summary statistic of a data set that has a theorectical chi-squared distribution.    base pairing  The pattern that adenine (A) and thymine (T) are paired (appear with equal frequency) in the DNA of an organism, and similarly cytosine (C) and guianine (G) are paired.    contingency table  Table of counts summarizing the number of times combinations of factor levels were observed in the data set.    Hardy-Weinberg equilibrium (HWE)  Assuming random mating, this principle characterizes the distribution of genotype frequencies as a function of the relative frequencies of each allele.    position weight matrix (PWM) / position-specific scoring matrix (PSSM)  Table giving the probability of each nucleotide at each position    sequence logo  A graphical summary of the position weight matrix or position-specific scoring matrix.     Practice   ","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581281287,"objectID":"2788b54b4b00409516daea264740162d","permalink":"/post/vocabularly-for-chapter-2-part-1/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/post/vocabularly-for-chapter-2-part-1/","section":"post","summary":"Vocabulary for the first part of Chapter 2","tags":["Chapter 2","vocabulary"],"title":"Vocabularly for Chapter 2, Part 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 1","exercises"],"content":" The code instructions in the exercise statement appear to be outdated. The code below worked on my machine. Note that when asked whether I would like to update packages from the binary version, I said no. (When I said yes, R gave an error.)\nif (!requireNamespace(\u0026quot;BiocManager\u0026quot;, quietly = TRUE)) install.packages(\u0026quot;BiocManager\u0026quot;) BiocManager::install(c(\u0026quot;Biostrings\u0026quot;, \u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;,\u0026quot;BSgenome\u0026quot;)) You can see the various data genome data sets available by loading the BSgenome library and typing available.genomes().\nOnce you have the needed packages installed, you can access the sequence data for this exercise via the following commands.\nsuppressMessages(library(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;)) Celegans ## Worm genome: ## # organism: Caenorhabditis elegans (Worm) ## # provider: UCSC ## # provider version: ce2 ## # release date: Mar. 2004 ## # release name: WormBase v. WS120 ## # 7 sequences: ## # chrI chrII chrIII chrIV chrV chrX chrM ## # (use \u0026#39;seqnames()\u0026#39; to see all the sequence names, use the \u0026#39;$\u0026#39; or \u0026#39;[[\u0026#39; operator ## # to access a given sequence) seqnames(Celegans) ## [1] \u0026quot;chrI\u0026quot; \u0026quot;chrII\u0026quot; \u0026quot;chrIII\u0026quot; \u0026quot;chrIV\u0026quot; \u0026quot;chrV\u0026quot; \u0026quot;chrX\u0026quot; \u0026quot;chrM\u0026quot; Celegans$chrM ## 13794-letter \u0026quot;DNAString\u0026quot; instance ## seq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA class(Celegans$chrM) ## [1] \u0026quot;DNAString\u0026quot; ## attr(,\u0026quot;package\u0026quot;) ## [1] \u0026quot;Biostrings\u0026quot; length(Celegans$chrM) ## [1] 13794 The Biostrings packages provides functions to summarize the sequence. For example:\nlibrary(\u0026quot;Biostrings\u0026quot;) lfM = letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) lfM ## A C G T ## 4335 1225 2055 6179 sum(lfM) ## [1] 13794 ","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581008738,"objectID":"8d16e489e5ef46020884190c7e09cad2","permalink":"/post/chapter-1-exercise-setup/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-exercise-setup/","section":"post","summary":"Instructions on how to get started on Chapter 1, exercise 1.8.","tags":["Chapter 1"],"title":"Chapter 1 exercise setup","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 1"],"content":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580933164,"objectID":"3bb9cd5326655191aa8e191c887845d2","permalink":"/post/chapter-1-vocabulary-quiz/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-vocabulary-quiz/","section":"post","summary":" The vocabulary quiz will be live here during the start of the course.\nLoading…  ","tags":["quiz","Chapter 1"],"title":"Chapter 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","vocabulary","blogdown","github"],"content":" As one of your assignments for this class, you are responsible for creating a blog post with all the vocabulary and definitions for one week of the course. This blog post will explain how you can create and publish that blog post on our course website.\nCreate the blog post Update your fork of the website You should have already forked our website to add your details for the “Person” section. You can use this same fork to add your blog post, but you should make sure you get your fork up-to-date with the current version of the website before you do.\nA fork of a repository does not stay up-to-date with the original repository it copied by itself. Instead, unless you update it, it will continue to be a snapshot of the original repository (plus any changes you’ve made to your copy) as of the time when you forked it. If the original has made a lot of changes since you made your fork, it might be very hard to make a clean pull request as there will be (potentially) lots of conflicts because of changes made to the original. It’s considered polite to make sure that you’re working with an up-to-date fork of a repository if you want to make a pull request back to the original.\nTo update your fork of the original repository, open your “csu_msmb.Rproj” file to open our website’s R Project on your computer. This should open RStudio with the website’s project open (check the top right corner of your RStudio window to confirm—it should say “csu_msmb”).\nThere is a little blue gear symbol in the “Git” pane in RStudio. Click on the down arrow to the right of it and select “Shell…”. This should open a bash shell on your computer. (If your computer uses Windows, there’s a chance that it might open something other than a bash shell. In that case, you can change your preferences in RStudio to reconfigure to always use a bash shell terminal when you ask for a shell from RStudio.)\nIn this shell, you need to run two git commands. First, you’ll add a remote branch to your repository. You already have one remote branch called “origin”—that’s the GitHub repository that you have in your account, which you forked from the original. Now you’ll add the original (the GitHub repository in my account) as another remote branch. Each branch has its own name, and you can use that name to refer to it in later git commands. The convention, if you add an original repository that you forked from as a remote, is to name that remote branch “upstream”. Run the following code in your bash shell to add the original GitHub repository as a remote branch with the name “upstream”:\ngit remote add upstream git@github.com:geanders/csu_msmb.git Now that you have added the original as a remote, you can pull in any commits that were made to it since you originally forked it. There are a few ways you can do that, but one way to do it in one step is with git’s pull command. This fetches the changes and merges them into your local version of the repository, all in one step. Run the following code in your bash shell to do that:\ngit pull upstream master Ideally, all this will have worked seamlessly (if not, check with the faculty and we can help you troubleshoot). Close your bash shell and check your version of the “csu_msmb” project to see if it looks like it’s up-to-date with the original. You can go to the “Commit” button in the “Git” tab, and there is a “History” selection in the window that pops up. Look through that and make sure that you see recent commits to confirm that your version is now up-to-date.\nFinally, this has updated your local version, but not your GitHub remote. Go ahead and use the green up arrow in RStudio’s “Git” pane to push your updated local version up to GitHub. Now both your local (“master”) and remote (“origin”) branches should be up-to-date with our original version, so it will make it much easier to merge in your changes.\nIf you’d like to learn more about this process, there’s a really nice blog post here.\n Making a new blog post In blogdown, each blog post is an RMarkdown document. The stuff at the very top of the file (the YAML with details like the title and author) will look a bit different than plain RMarkdown files, but once you get into the body of the post, you should find that the rules are very similar to RMarkdown.\nYou will be creating a blog post that will include a table with the vocabulary list as well as a few other elements. There are a few ways you can add a new blog post file in blogdown. You’re welcome to use any method you’d like, but if you’re not sure where to start, this is one way.\nMake sure that you have RStudio open to the project for our course’s website. If you do, you should see csu_msmb in the upper right hand corner of your RStudio session. (If not, go to File -\u0026gt; Open Project... and navigate through your file directory to your local version of our project directory and open the csu_msmb.RProj file there.)\nNext, you can use an RStudio “Addin” to make a new blog post using a nice user interface. These Addins are all alternatives to things you could do with a function call in R, but the Addin often provides a more immediately user-friendly interface for you to enter options. For example, the Addin for creating a blog post does all the actions of a blogdown function called new_post, but instead of needing to remember the parameter name to use for the author listing and the title and all that, you can just fill the information into a nice form and go from there.\nTo find the new post Addin, look at the top of your RStudio session window. You should see “Addins” with a down arrow beside it. Click on the down arrow. When you do, you should see a “New Post” option. Select this option. A form should pop up with spaces for you to fill in the title, author, and some other details.\nFill this form out in the following way:\n Title: This should be “Vocabulary for Chapter [x]”, but with “[x]” replaced with your chapter number. Author: Make sure you put your name exactly as listed in the “People” section of the website. This will help the website generator connect this post with your user profile, so when someone reads it they’ll get your picture and a link to find out more about you at the end of the post. Date: This is where you put the publication date of your blog post, and it has a pretty cool feature. Even if you write your blog post earlier, the post will not be published on the blog until the date listed in this section. That means that you can start writing your blog on one day but know that it won’t show up online until later. It also means you can start work on your blog, but a half-finished draft won’t show up online until you get to the publication date. For right now, set the current date in this section, so that your blog post will show up locally as you work on it, but later you’ll actually change this date so that, when you submit your pull request, your post won’t show up until the faculty have had the chance to suggest some changes and for you to make any needed fixes. Categories and tags: For both the “Categories” and the “Tags”, be sure to include vocabulary and Chapter [x] (with [x] replaced with your chapter’s number). These tags will let everyone on our website quickly find all the blog posts on your chapter or all the vocabulary lists. Format: You have several choices for the type of file to use to write your blog post. Since we’re going to be using some R code to make the table look pretty, you’ll need to pick one of the options that allows for R code chunks, so that rules out plain Markdown. I recommend that you use “.Rmd”.  Once you make these entries, click the button labeled “Done”. This creates an RMarkdown file for your blog post and opens it for you. Here’s an example of me doing this process if I were writing the vocabulary list for Chapter 16:\nYou can see, in the RMarkdown file that’s created and opened, that all these details end up getting inserted into the YAML at the top of the RMarkdown file. If you ever need to change anything (like the date or the title), you can change it here in the RMarkdown file. Do so carefully, though—YAML can be pretty picky about things like spacing and special characters (hyphens, for example).\nIf you ever need to find this file later, all of the blog posts are saved in a special place in our project’s directory: in the content subdirectory, there’s a subdirectory called post that contains both the RMarkdown files used to write the posts and the output (an HTML file) that is created by the RMarkdown each time you save the file. You might notice that they all have long file names—the file name for a blog post is a combinataion of its publication date and its “slug”, which is some abbreviation of the original title. If you really want, you can change what the slug will be when you first create the blog post, but I don’t think you really need to.\n Writing in the blog post   via GIPHY Within the body of the blog post RMarkdown file (in other words, below the --- that marks the end of the YAML section), you can write the blog post just as you would any RMarkdown document. This means that you can use things like ** to mark bold text, * for italics, and #, ##, etc., for section headings.\nIt also means that you can insert chunks of R code that will run and add their output within the post. Unlike in regular RMarkdown, you usually won’t have to press the Knit button to knit the document. Instead, the blog post should re-knit every time you save the file. You can check to see by looking at the Viewer pane to look at the current version of the site (if it doesn’t show the site automatically, load the blogdown package and then run serve_site).\nIf you have not worked much with RMarkdown before, you might want to check out some references on how it works. There are several great articles on the RMarkdown website that can help.\nIn your blog post, go ahead and draft a first paragraph that describes the key concepts covered in the chapter. Also, create third-level section headings (i.e., use ### to mark the section heading) for “Sources consulted or cited” and “Practice”. Save your blog post file and check to see if these changes have been made in the version of the website in your Viewer pane!\n  Create the vocabulary list Now, for the content of your post. You’ll be creating a vocabulary list, as well as embedding a Quizlet practice app, so that your classmates can learn the vocabulary for the chapter. This list will be what everyone is responsible for in the weekly vocabulary quiz.\nYou can see an example of a vocabulary blog post for Chapter 1. You can use this as a template for your own post.\nIdentify the vocabulary terms you need to define First, you will need to decide which words from the chapter to define. We expect that you will include all the bolded terms for your chapter. Here are some guidelines for deciding on the vocabulary terms to define for your chapter:\n You should include all words in the chapter that are given in bold. Be sure to look for bolded terms in the sidenotes and end-of-chapter exercises, too! Occassionally, the authors use bold for subheadings (see the “Why R and Bioconductor?” section in the Introduction or the “Summary of this chapter” section of Chapter 1). These subheadings do not need to be included in the vocabulary list for the chapter. If you find one or more common synonyms for a term, you can include that with the term in the list (e.g., “variability / spread / dispersion”). Feel free to change a term from singular to plural or vice versa if it helps you in writing the term’s definition. Similarly, if the bolded term does not include all the words that would be helpful (e.g., the bolded term is “sufficient”, but the term of interest is “sufficient statistic”), you can add a word or two to the bolded term. The bolded terms in the book tend to favor statistical terms over biological ones. If there are some biological terms you needed to look up when you read the chapter, or that you think some people in the class might not know, feel free to add them to your vocabulary list.   Create a .tsv file with terms and definitions While you could directly add the vocabulary into an RMarkdown table, we are asking you to save it into a plain text .tsv file, which will then be read into the RMarkdown document to form a table. We doing this because it creates a few advantages. First, if we have the vocabulary list in a dataframe (which we get when we read it in from a plain text file), we can use some cool R packages to format the table nicely, without having to learn loads of new Markdown or HTML formatting tricks. Second, we want to also use the vocabulary list as input to a Quizlet list, which will let us embed a practice app with flashcards and quizzes. One of the easiest ways to create a Quizlet list is to copy in vocabulary list directly in the tsv format, so this approach makes that secondary use easy.\nIn our website’s repository, there is a special subdirectory for saving vocabulary list .tsv files, with one for each chapter. In the Project directory, go to content -\u0026gt; post -\u0026gt; vocab_list. This is where you want to save the .tsv file for your chapter.\nTo create the file, in RStudio go to the “File” tab in the menu at the top and select “New File” -\u0026gt; “Text File”. This will open a file in RStudio in plain text format. Save the file as “chapter_[x].tsv” (but replace “[x]” with your chapter number). Make sure you save it in the “vocab_list” subdirectory of the project with the rest of the vocabulary list files.\nNow write your vocabulary terms and definitions in this .tsv. This file extension stands for “tab-separated”, so to format the file correctly, you should:\n Put each term / definition pair on its own line. Because some terms will be long, they may visibly “wrap” in the text file you have open, but as long as you don’t press the “Return” key, they should still be on one line of the file. To doublecheck, you may want to make sure that you have line-numbering on in RStudio and make sure that only one line number is listed for each term on the left hand side of the file. Press the “Tab” key to add a tab between the term and definition on each line. This should be the only place you have tabs in the file. R will look for tabs to figure out where to split between vocabulary terms and there definitions (as will Quizlet when you copy the terms into the list there). Sometimes it won’t look like the tab’s added a lot of space, but that’s no problem—the computer can see it even if you can’t! Don’t put any header information at the start of the file. Just start directly with your first vocabulary term.  If you’d like to see an example, check out the “chapter_1.tsv” file in the “vocab_lists” subdirectory. This is the file that serves as input for the Chapter 1 vocabulary list blog post.\nHere are some guidelines for writing your definitions:\n It is fine to use wording from the chapter text or to use wording directly from other websites or sources. However, you must include a list of any of the sources that you used to write your definitions at the bottom of the vocabulary blog post. Further, if you are using sources besides the course textbook, make sure that the definition is appropriate in the context of our course. Often, words will have a number of different definitions across different disciplines. Try to use more formal sources (e.g., textbooks, other published books) rather than less formal websites to find definitions whenever possible. See the Chapter 1 vocabulary list for an example of what we expect for using and listing references. If a vocabulary term was defined in a previous chapter’s vocabulary list, feel free to reuse the definition. Our library has excellent resources that you can use to help write your definitions, including textbooks and dictionaries specific to biology and statistics.   Adding R code to show the list in the post I’ve written up some R code that will read in the vocabulary list and make it into a nicely formatted table in the HTML version of the blog post. You can re-use this R code in your post, you’ll just need to change the name of the input file to the one for your chapter’s file.\nThis R code uses a few R packages beyond the base R code. If you haven’t installed these packages yet, you’ll need to before the code will run. You’ll need to install:\n knitr dplyr readr kableExtra  Once you have these, below your paragraph summarizing the chapter’s theme, write:\n “The vocabulary words for Chapter [x] are:”\n (but with your chapter’s number) and then paste in the following code and change chapter_1.tsv in the code to the correct file name for the .tsv file you created for your chapter’s vocabulary.\n```{r echo = FALSE, message = FALSE, warning = FALSE} # Load packages library(dplyr) library(readr) library(knitr) library(kableExtra) # Read in vocabulary from tsv into a dataframe # This is where you'll need to replace the file name with your own vocab % kable(align = c(\"rl\"), col.names = c(\"\", \"\")) %% kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %% column_spec(1, bold = T, border_right = T) %% column_spec(2, width = \"30em\") ``` This code reads in the data from your .tsv file and then formats it in a nice way. If you’d like to understand it better, try commenting out some lines and see how it changes the output. One of my favorite piece of this code, one that I think might come in useful for you later, is column_spec(2, width = \"30em\"). This sets the width of one of the columns to be 30 ems (the width of the letter “m” in whatever font you’re using). By setting the width, the table won’t automatically expand to fit the text you put in the column onto one row. Instead, it will allow the text to “wrap”, going onto separate lines if the definition entry is long enough.\nIf you want to find out more about creating really fancy tables from RMarkdown, check out the documentation on the kableExtra package. What you can do (and how) is different, depending on whether you’re outputting to a pdf or a HTML file, so there’s separate documentation for each.\nOnce you add this code in, I’ve found that you actually do need to press the Knit button sometimes. If you don’t see your list when you save your file, or if it doesn’t update properly as you make changes to your file, try knitting with the Knit button and that should help.\n Creating and embedding a Quizlet app The last piece of the blog post is the practice section. For this, you’ll create a vocabulary list on Quizlet, which you can then embed in the blog post, so the other students can practice right on our site.\nYou’ll need to sign up for a Quizlet account first. The free account is fine.\nNext, create a new vocabulary list. There’s a “Create” button for making new lists on the main page. While you can add vocabulary by hand, you can also post in a whole list if it’s in a tab-separated or comma-separated format. Copy in the contents of your vocabulary list .tsv file. You can preview the terms lower on the page once you do, to make sure that all the terms and definitions came in correctly. If everything looks good, click on the buttons for “Import” and then “Create”.\nThis will create your list and take you to a page where you can try out your flashcards. On this page, there’s also a button with three dots. If you click on this, there’s a choice of “Embed”. When you embed HTML content, you are inserting an application from one website within another one. Embedding is a really fun trick for enriching blog posts and other RMarkdown documents that are rendered to HTML. For example, you can also embed Shiny apps, YouTube videos, and Google maps in your RMarkdown using the same process we’ll use here.\nWhen you select “Embed”, a pop-up window will open with some HTML code. Copy this and then paste it in the “Practice” section of your vocabulary blog post. Be sure to leave a blank line above and below the text you paste. When you look at your blog post in a web browser now, you should see the practice flashcards embedded in the “Practice” section.\n  Submit the post   via GIPHY So far, you’ve made these changes to your local copy of our website’s repository. To submit the changes to us, you’ll need to push your changes to your remote version of the repository (the one in your GitHub account) and then submit a pull request to us for us to pull those changes into the original website repository (the one in my GitHub account). This process should feel pretty familiar—it’s pretty much what you did to submit your changes to your profile information for the website on the first day of class.\nAs with other steps, there are several ways you can do this, and if you have an idea of how to get it done, any way is fine. If you don’t know where to start, though, you can follow along in this section for one way to do it.\nPushing your changes to your remote repo First, you’ll need to get any changes you’ve made from your local repository up to your remote version on GitHub.\nFirst, commit any changes that you’ve made through the Git window in your RStudio session. This will record the changes you’ve made in the git record for your local repository.\nNext, you’ll need to push these commits to the remote repository, to send these changes to GitHub. In the Git window in RStudio, there’s a green up button. Push that. It should send all your changes up to your GitHub version of the repository. To check, go online to your GitHub account and look through your repositories for your fork of “csu_msmb”. Click on “Commits” to see a history of the commits to the repository—your latest ones should be at the top of the list.\n Submitting a pull request to the original repo At this point, you’ve made changes, checked them, and pushed them to your GitHub version of the repository. Remember, though, that you forked the repository from our original one, and so you’ve been working with a copy of the repository this whole time, rather than changing our original version.\nTo get your changes incorporated into our original version, you’ll need to request that we pull your changes into the original repository. To do this, you can submit a pull request through GitHub. Go to the main page for your fork of the GitHub repository and look for a button that says “New pull request”. When you click this, it will walk you through making a pull request. You’ll have a space to write a message describing the changes you’re recommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation on pull requests.\n  Edit and re-submit the post based on faculty feedback The other faculty members and I will get a notice when you submit your pull request. We’ll take a look and will probably have some suggestions for the wording of some of the vocabulary terms. We’ll give you some feedback through the pull request page, and then we’ll work together to get the list finalized before it’s published for the rest of the class.\n ","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580751472,"objectID":"b6a63bb5b3e96b091185e33290708428","permalink":"/post/creating-a-vocabulary-list-blog-post/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/creating-a-vocabulary-list-blog-post/","section":"post","summary":"As one of your assignments for this class, you are responsible for creating a blog post with all the vocabulary and definitions for one week of the course. This blog post will explain how you can create and publish that blog post on our course website.\nCreate the blog post Update your fork of the website You should have already forked our website to add your details for the “Person” section.","tags":["instructions","vocabulary","blogdown","github"],"title":"How to create a vocabulary list blog post","type":"post"},{"authors":["Brooke Anderson"],"categories":["blogdown","github","instructions"],"content":" One goal of this course is to continue developing your data science programming skills. This will include plenty of work on R programming, but also more to help you learn tools for reproducible research, like RMarkdown and git.\nWe will be using our course website as a collaboration tool during this course. This website was created using blogdown, which allows you to create and update a blogging website with R and RStudio. We are using a GitHub repository to share all the code for this website and serving the site using Netlify.\nDuring this course, you will have two graded products that you will need to submit as blog posts to our site. One will be a glossary of vocabulary terms for one chapter of the book, listing key words and their definitions for the chapter. The second will be the “official” version of one week’s in-course exercise.\nTo help get you up to speed with using blogdown, GitHub, and RMarkdown with our site, we’ll start by having you update your profile details for our website. We’ll also use this to give you all a chance to introduce yourselves to each other and to us. This post covers the details for how to do that.\nRequired set-up This exercise, and this course as a whole, requires a certain set-up on your computer:\nR installed on your laptop RStudio installed on your laptop git installed on your laptop Your own GitHub account  If you already have all this set-up, you can skip to the next section. Otherwise, this section has details on completing this set up.\nInstall R on your laptop You can install R from the Comprehensive R Archive Network (CRAN). Search for the version appropriate for your computer’s operating system.\nIf you already have R installed, check your version number. If it’s older than six months or so, you should probably update your version for the class. You can use the sessionInfo() function to find out details about your current R session, including the version of R you’re currently running:\nsessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 tools_3.6.3 ## [5] htmltools_0.4.0 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 blogdown_0.18 knitr_1.28 stringr_1.4.0 ## [13] digest_0.6.25 xfun_0.13 rlang_0.4.6 evaluate_0.14 Based on the return from this call, for example, I can tell that I have R version 3.6.3 (2020-02-29).\n Install RStudio on your laptop You can download RStudio directly from their website. The free Desktop version will work great for this course. If your version of RStudio is more than a year old, you should probably update it for this course. To check your version of R Studio, open R Studio, go to the “RStudio” tab at the top, and click on “About RStudio”.\n Install git on your laptop The software git is version control software, which will help you record and track changes that you’ve made to code and other plain text documents.\nIt’s free to download. Go to https://git-scm.com/downloads and select the version for your operating system. For this software, you’re probably okay if you downloaded it a little while ago (although if more than two years or so, you might want to update).\n Get a GitHub account You will need a (free) GitHub account for this course. You can sign up for one (if you don’t already have one) at https://github.com/. While there are some fancier paid plans available, the free account will work great for this class.\nWhen you sign up, you’ll get to choose a GitHub handle. You might want to make this something that will be easy for people to remember. For example, if your name is still available, that would be a great option. This handle will form part of the address to all of your GitHub repositories, so it is convenient if it is easy for people you work with to remember (mine, unfortunately, is not!).\n  About blogdown The blogdown package is an R package created by Yihui Xie that allows you to use R and RStudio to create and update your own webpage with a blog. The appeal of being able to do this with R is that you can write blog posts using RMarkdown, so you can include executable R code in each post.\nblogdown creates your site using the Hugo framework. Hugo is software that can build static websites (i.e., ones that can be served to viewers without needing database backends or other fancy things). People have created different templates for Hugo-generated websites, and these templates provide the structure and framework, while you can adapt the content.\nThis means that our website (which is, essentially, a collection of files in a directory written in a form that a web browser can convert to a pretty website) includes a lot of files and code that come straight from a template that someone else wrote, and then places here and there where we can add or change the files to make the website ours.\nOne of the ways that you can change the website is to add posts. You’ll be doing this later in the course by contributing two blog posts of your own, one on the vocabulary for a chapter and one with the “official” version of the exercise for a chapter. We’ll cover more on how to add a blog post later in the course.\nThe other way that you can change the website is to change some of its “front page” data. The website has a section on “People”, with the profiles of everyone in the class. The information shown in this section is all saved in plain text files in our website’s file directory. Today, you’ll change you details in the file dedicated to you and then send those changes back to us so we can update the online version of the site.\nYou will need to install two pieces of software to work on our website. First, you’ll need the R package blogdown. You can install this package in the normal way, using install.packages:\ninstall.packages(\u0026quot;blogdown\u0026quot;) Once you have blogdown, you can install the Hugo software using a function in the blogdown package, install_hugo:\nlibrary(blogdown) install_hugo() For both these installations, your computer will need to be online or you’ll get an error.\n Getting a fork of our repository on your computer All our websites files are posted in a GitHub repository at https://github.com/geanders/csu_msmb. With this (or any) GitHub repository, you can suggest changes by forking the repository, cloning the fork to your computer, making and committing the changes, pushing those commits back up to your fork of the repository on GitHub, and then submitting a pull request.\nForking a GitHub repository   via GIPHY When you fork a GitHub repository, you get a copy of that repository that you can play around with and change yourself, without it affecting the original repository. It’s essentially just copying the whole repository, with all its files, into a repository on your GitHub account.\nThe only thing that makes it different from a plain copy (and what makes it really powerful in some cases) is that, if you decide that your changes might make the original repository better, you can submit a pull request. This requests that the owners of the original repository update it to incorporate the changes you’ve made on your fork of the repository. The original authors can review each of the commits you’ve made, so they can even cherry-pick your changes if they want.\nGitHub also lets the original authors see if there are any merge conflicts created from changes that they’ve made to the original repository since you forked it. This can let the original authors see how hard it will be to incorporate all of your changes in the forked version with their version of the repository.\nTo fork the repository with our course’s website materials, all you’ll need to do is go to our GitHub repository for the course website (while you’re signed in to GitHub) and click on the “Fork” button towards the right of the page. Now go and check in the “Repositories” section of your own GitHub account—you should see that you now have a forked copy of the “csu_msmb” repository.\nIn this exercise, you’ll work with the fork of the repository, and then once you’ve made your changes, you submit a pull request, so that we can get your changes back into the main webpage.\nIf you need more help on how to fork a repository, GitHub has a help page on the topic that might be useful.\n Cloning the fork to your computer Next, you’ll want to get a copy of your forked repository onto your own computer, where you can work with it, make changes, and preview the website with your updates.\nTo do this, you’ll clone your fork of the repository onto your computer. The version of the repository on GitHub is called the remote branch of the repository and the version you get on your computer once you’ve cloned it is the local branch. By cloning (instead of just downloading), you’ll maintain a connection between the remote and local versions through git, which will allow you to push changes that you make and commit on your own computer up to the remote branch on GitHub.\nGo to GitHub, make sure you are logged into your account, and navigate to your forked version of the repository for this class. There should be a button to the right of the page that says “Clone or download” (you may need to scroll down to find it).\nWhen you click on this button, it will give you a choice between “SSH” and “HTTPS” for the protocol to use to connect your local and remote branches of the repository. You’re welcome to try either, but I usually (on a Mac) have better luck with “SSH”. Occasionally, people running Windows in my courses have had better luck with “HTTPS”, although for most folks “SSH” seems to work fine. Once you choose which protocol to use, you can copy the snippet of code that is given in the pop-up.\nNext, you’ll run this code from a bash shell on your own computer to clone the repository. You first will need to open a shell. If you’re on a Mac, you can do that with the “Terminal” application. With Windows, you’ll probably need to use the bash shell that comes with the Windows version of git. Search your programs for “bash” or “git bash” and see if you see something that looks promising.\nOnce you open a shell, you’ll see a command prompt, like this:\nusername$ You can type shell commands here and then press “Return” to run them. You should first move into the directory where you want to clone the repository. Your “Desktop” might be a good place for it for now (unless you have some organization you use for course-related files). The cd shell command lets you “change directory”. If you don’t put anything after cd, it will change to your home directory. Otherwise, it will move to the directory you specify. For example, if the “Desktop” directory is a subdirectory of my home directory, I could move into it by running:\ncd Desktop If you have not use shell commands much before and are having any problems navigating to the directory you’d like, let us know in class, and one of us can help you.\nOnce you are in this directory, you’ll paste git clone followed by the command you copied from the “Clone or download” button on GitHub. It will probably look something like this (but with your GitHub handle in place of “geanders”):\ngit clone git@github.com:geanders/csu_msmb.git When you run this, you may have to put in your GitHub username and password. You may also get some questions about whether you really want to download the repository (you do). If everything’s successful, you should see that there’s a new directory called “csu_msmb” in which directory you decided to put it (“Desktop”, for example).\nThis directory has a special file in it that makes it an R Project—a special version of a file directory with some extra structure and saved preferences. Make sure that you open the project as a whole when you work on it in R Studio, rather than opening just by clicking on one of the files. To do this, you can go in R Studio to \"File\" -\u0026gt; \"Open Project...\" and then navigate through your file directory to the “csu_msmb” directory you just cloned.\nIf you need more help, GitHub has a help page with more on how to clone a repository from GitHub to your own computer.\n Changing and committing in RStudio When you have R Studio open to an R Project that is using git version control, R Studio will include a “Git” pane. You can use this pane to commit changes you make to files in the repository, write messages explaining those commits, and push your changes to your remote branch of the repository on GitHub.\nWhen you commit a change, that change is written into a log of every change made to the files in the repository. You can later look through these commits, so you’ll want the commit messages to make sense when you read them in the future. When you’re collaborating with others, the commit messages will help you see what each other are doing.\nWhen you first commit a change, the commit is only saved in your local branch. To send it up to the remote branch of the repository on GitHub, you’ll need to push those commits. Once you push your local commits, your GitHub repository should exactly mirror your local repository.\nAs soon as you make a change to a file in the repository that’s being track by git, that file will show up in the Git pane, with a little check box beside it. When you’re ready to commit a change, click on the “Commit” button on the top left of the Git pane. This will open a pop-up box.\nIn this box, click the check boxes for all the changed files on the left you’d like to include in the commit. Then write a short commit message, describing the changes you’ve made. You should try to fit it all in the first line of the “commit message” window. If you can’t, write a short description in the first line, skip a line, and then you can write as much as you want.\nOnce you’ve written your commit message, click on the “commit” button. This will record this commit. To check that it has, you can go to the “History” tab and make sure the commit shows up as the last thing in your history.\n  Updating your profile details Rendering blogdown websites in RStudio Once you’ve opened the R Project with our website, you can use the blogdown package to serve the website. This will only update and show the website on your computer (not change our main website online), but it lets you check that everything’s working and preview what the site will look like online.\nRStudio’s “Viewer” pane can work as a web browser. This means that it can show our website. When you have opened the R Project with the cloned repository of our website (“csu_msmb”), try running the following in your R console to render the site:\nlibrary(blogdown) serve_site() If everything worked, you should be able to see a version of the website in your RStudio “Viewer” pane. If you’d like to see it in your usual web browser, click on the “Show in new window” button on the top left of the “Viewer” pane (this looks like a little rectangle with an arrow on it). This will open the website in your default web browser.\nTake a look at the web address when you do—it should start with 127.0.0.1. This is a loopback address—an IP address that refers back to your local computer (localhost), rather than an outside web servers. Anytime you’re building a website and checking it locally, you’ll see this in the web address when you open the site in a web browser. (You can even get T shirts with “There’s no place like 127.0.0.1”).\nAs you work through the next parts of the exercise, the rendered website in the Viewer pane should update every time you save your changes to files in the website. If you have the website open in your default browser, too, you might want to refresh the site with the normal “Refresh” button for your browser. If things ever seem like they’ve gotten out of sink, you can always re-run serve_site().\n Navigating the website’s file directory to find your profile We all have our own author profile in a subdirectory within the website’s files. To find yours, go to the “content” subdirectory of the website files and then the “authors” subdirectory within that. You should see a subdirectory there with your name. Click on that and you’ll see the two files that make up your author profile, \"_index.md\" and “avatar.jpg”.\n Updating your information in \"_index.md\" Your details are all given in the \"_index.md\" file in your author subdirectory. To update your details on the website, you’ll need to change your details in this file.\nThe file is written in a Markup language called YAML. If you’ve used RMarkdown before, you might recognize this syntax from the information that goes at the very top of each RMarkdown file.\nIn your \"_index.md\" file, anywhere there is a placeholder, like “[Year]” or “[Institution]”, replace the placeholder with your own information.\nBe very careful when changing things like spaces and hyphens in the structure, as YAML is based on parsing these elements. As with any Markup language, as you are learning it, it’s best to try to render the final document often as you make changes, so you can make sure the changes make it through like you want and so you can catch any problems quickly.\nMake sure you change the following sections:\n bio: education: email: interests: name: organizations:  Some of the sections in social: are commented out, including the information for buttons for GitHub, GoogleScholar, and Twitter. If you have accounts through any of these services, you can add these buttons with your updated information. Just delete the # at the beginning of all lines in that section and then change the handle or web address information so that it links to your account for that service. In this section, also update your email address, with mailto: at the beginning, for the email icon.\nThe very bottom of the file, under the ---, provides space for you to write a paragraph summarizing who you are and your academic / research interests.\nBe sure to save the file after you’ve made all your changes.\nFor an example of a completed \"_index.md\" file, you can see mine here.\n Updating your avatar picture There’s also a place in your author profile directory to include a photo to represent yourself. To change from the default (the blue guy), replace the “avatar.jpg” file in your author profile directory with the JPG of your choice, and use the same file name (“avatar.jpg”).\nIt would be helpful for you to use a photo of yourself, since that will help us put names with faces, but if you don’t have one or would prefer not to use your own photo, feel free to pick any photo (for which you have appropriate permissions) to use.\nYou might need to crop your photo some to get it to show up in the circle on the website correctly. Try with your uncropped picture once, check the website in the RStudio Viewer pane to see how it looks, and then if it doesn’t work, play around with cropping it until you’re happy.\n  Submitting your updates Pushing the commits back to GitHub When you are ready to push all the changes you’ve committed to your local branch, you can do this from the Git pane in R Studio. In this pane, there are two arrows: a green up arrow and a blue down arrow. Click on the green up arrow to push the commits from your computer (the local branch) to GitHub (the remote branch). Visit your GitHub page for the repository (or refresh it if you already had it open) and check if your changes have successfully been pushed to the remote branch.\nIf you haven’t created an SSH key and shared it with GitHub, you may be asked for your GitHub password every time you try to push. This will get to be a pain, so you’ll probably want to set up an SSH key. For more on how to do this (as well as other help with using RStudio with version control), check out RStudio’s help documentation on the topic.\n Requesting that we pull your changes   via GIPHY At this point, you’ve made changes, checked them, and pushed them to your GitHub version of the repository. Remember, though, that you forked the repository from our original one, and so you’ve been working with a copy of the repository this whole time, rather than changing our original version.\nTo get your changes incorporated into our original version, you’ll need to request that we pull your changes into the original repository. To do this, you can submit a pull request through GitHub. Go to the main page for your fork of the GitHub repository and look for a button that says “New pull request”. When you click this, it will walk you through making a pull request. You’ll have a space to write a message describing the changes you’re recommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation on pull requests.\n  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578952088,"objectID":"aaf734a217c669a7993ece1eb560bb61","permalink":"/post/add-profile-details/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/add-profile-details/","section":"post","summary":"One goal of this course is to continue developing your data science programming skills. This will include plenty of work on R programming, but also more to help you learn tools for reproducible research, like RMarkdown and git.\nWe will be using our course website as a collaboration tool during this course. This website was created using blogdown, which allows you to create and update a blogging website with R and RStudio.","tags":["blogdown","github","instructions"],"title":"How to add your profile details to our course website","type":"post"},{"authors":["Brooke Anderson"],"categories":["vocabulary","Chapter 1"],"content":"  Chapter 1 covers generative modeling for discrete data. It introduces a number of terms covering probablity and statistical modeling, as well as a few biological terms. The vocabulary words for Chapter 1 are:\n        probability model  A mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.    vector  In programming, a one-dimensional array of data, all with the same data type.    discrete event  In statistics, an event that can take a finite or countable number of values (e.g., number of deaths in a community by day).    categorical variable  A variable that can belong to one of a finite set of levels.    levels  In the context of a categorical variable, the set of values to which the variable can be assigned.    factor  In the context of statistical programming, a data type that can take one of a limited number of possible values (e.g., sex, nationality).    exchangeable  A property of a vector of random variables that implies the order in which the variables appear in the vector doesn’t matter.    sufficient statistic  A (summary) statistic that contains all the information about the model parameters that is in the original, uncondensed form of the data.    Bernoulli distribution  A probability distribution describing a random variable that can take on two possible outcomes (e.g., win / loss).    parameter  A numerical value that describes a population.    complementary  A description of two events who are mutually exclusive and whose probabilities sum to one (i.e., either one event or the other is guaranteed to happen, but not both).    binomial random variable  A variable whose values occur according to a binomial probability distribution.    probability mass distribution  A function giving the probability that a discrete random variable is equal to a given value.    Poisson distribution  A probability distribution for count data that has support on the non-negative integers. This distribution is also used to approximate a binomial distribution when the probability of success is small and the number of trials is large.    epitope / antigen determinent  Site on a macromolecular antigen to which an antibody binds. This is the part of an antigen that is recognized by the immune system.    Enzyme-linked immunosorbent assay (ELISA)  An assay that is used to detect specific epitopes at different positions along a protein.    conditional on  Given    cumulative distribution function  A function giving the probability that a random variable is less than any specified value.    extreme value analysis  Analysis focused on the behavior of the very large or the very small outcomes of a random distribution, allowing an exploration of the probability of rare events.    rare event  Something that occurs with a very low probability.    rank statistic  A data vector sorted least to greatest.    Monte Carlo method  A method that uses computer simulation from a generative model to determine probabilities of events.    probability or generative modeling  A method of modeling where all the parameters are known and the mathematical theory allows us to work by deduction.    deduction  A top-down method of reasoning, starting from a theory or principle rather than from data.    statistical modeling  A method of modeling where the distribution of the data is not known.    fit  In the context of statistical modeling, estimating the parameters of a model based on observed data.    multinomial  A generalization of the binomial distribution to cases where there are a finite set of possible outcomes (e.g., a roll of a die).    power / true positive rate  The probability of detecting something if it is there.    null hypothesis  Often, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.    matrix  In programming, a two-dimensional array of data, all with the same data type.    expected value  The average (mean) value of a random variable.    variability / spread / dispersion  In statistics, the amount by which a set of observations deviate from their mean.    statistic  A numerical characteristic of a sample and known constants (i.e. no unknown parameters).    null distribution  The probability distribution under the null hypothesis.    alternative  In the context of a generating process and hypothesis testing, the generating process that is considered in comparison to the generating process under the null hypothesis.    chi-squared distribution  A distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).    p-value  The probability of seeing the observed data or something more extreme under the generative model associated with the null hypothesis.    probability density function  A function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.    default  In the context of arguments to an R function, the value that is used if no custom value is specified.    C. elegans genome nucleotide frequency  How often adenine, cytosine, guanine, and thymine occur in the DNA of a roundwork often used in scientific research.    Bioconductor  Open-source software that provides contributed programs for bioinformatic data analysis.    codon  A three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis).    DNA read  An inferred sequence of base pairs for a single DNA fragment, based on sequencing.    nucleotide  In the context of DNA, one of four compounds (adenine (A); cytosince (C); guanine (G); and tymine (T)) that make up the basic information unit.    genome  An organism’s complete set of DNA, including all of its genes.    replication cycle  In biology, the process that begins with the infection of a host cell by a virus and ends with the release of mature progeny virus particles.    point mutation  A change, addition, or deletion of a single nucleotide in a gene sequence.    genotype  The genetic make-up of an individual’s cells, including how the individual’s genetic make-up differs from others’.    diploid  Having genetic material in two complete sets of chromosomes, from two parents.    protein  A compound made up of amino acids; one of the four types of macromolecules that make up living organisms.    antibody  A type of protein made by certain white blood cells in response to an antigen.    antigen  A foreign substance in the body to which the immune system reacts.     Sources consulted or cited Some of the definitions above are based in part or whole on listed definitions in the following sources.\n Holmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press, Cambridge, United Kingdom. Everitt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom. Bioconductor: Open Source Software for Bioinformatics. https://www.bioconductor.org/ Wikipedia: The Free Encyclopedia. https://en.wikipedia.org/wiki/Main_Page NIH Genetics Home Reference. https://ghr.nlm.nih.gov/ NCI Dictionary of Cancer Terms. https://www.cancer.gov/publications/dictionaries/cancer-terms   Practice   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578957646,"objectID":"689ae0d83c0f37774ff128d4fd9d9d42","permalink":"/post/vocabulary-for-chapter-1/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-1/","section":"post","summary":"Chapter 1 covers generative modeling for discrete data. It introduces a number of terms covering probablity and statistical modeling, as well as a few biological terms. The vocabulary words for Chapter 1 are:\n        probability model  A mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.    vector  In programming, a one-dimensional array of data, all with the same data type.","tags":["vocabulary","Chapter 1"],"title":"Vocabulary for Chapter 1","type":"post"}]